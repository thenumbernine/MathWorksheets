<!doctype html>
<html>
	<head>
		<meta charset='utf-8'>
		<title>perpendicular spaces</title>
		<script type="text/javascript" src='../tryToFindMathJax.js'></script>
	</head>
	<body>
<a href='index.html'>Back</a><br>
<br>

Let $\mathcal{M}$ be our manifold of dimension n<br>
Lets separate it into a hypersurface/submanifold $\Sigma$ of dimension m.<br>
Let $e_i, i \in \{1, ..., m\}$ be our basis elements of the submanifold.<br>
Let $e_i, i \in \{m+1, ..., n\}$ be our basis elements of the orthogonal space.<br>
Lets use a coordinate basis so $e_i = \partial_i$ and $[e_i, e_j] = 0$.<br>
<br>

Let $\eta(x) = \delta_{ij} \cdot \sigma_i = diag(\sigma_1 ... \sigma_n)$ be the diagonalization of the metric of the manifold at some coordinate $x = \{ x^1 ... x^n \}$, for $\sigma_i \in \pm1$.<br>
So $\eta = \eta_{ij} e^i \otimes e^j = \sigma_i \cdot \delta_{ij} e^i \otimes e^j$<br>

$\eta = i \downarrow \overset{j \rightarrow}{
\left[ \begin{matrix} 
	\sigma_1 	& 			& 0 		\\
				& \ddots 	& 			\\
	0 			& 			& \sigma_n
\end{matrix} \right]
}$<br>
<br>

Let $\eta^\perp = diag(\sigma_1 ... \sigma_m, \underset{\times (n-m)}{\underbrace{0 ... 0}})$ be the diagonalization of the metric of the submanifold at some point $x^a$.<br>
So $\eta^\perp = (\eta^\perp)_{ij} e^i \otimes e^j$ and $(\eta^\perp)_{ij} = 0$ for i or j $\in \{ m+1 ... n\}$, or otherwise $= \eta_{ij}$.<br>

$\eta^\perp = i \downarrow \overset{j \rightarrow}{
\left[ \begin{array}{ccc|cc}
	\sigma_1 	& 			& 0 		& 0 &			& 0 	\\
				& \ddots 	& 			& 	& \ddots 	&		\\
	0 			& 			& \sigma_m	& 0 & 	 	 	& 0 	\\
	\hline                                         
	0 			& 			& 0 		& 0 & 	 	 	& 0 	\\
				& \ddots	& 			& 	& \ddots 	&		\\
	0 			& 			& 0 		& 0 & 			& 0
\end{array} \right]
}$<br>
<br>

Let $\eta^\top = diag(\underset{\times m}{\underbrace{0 ... 0}}, \sigma_{m+1} ... \sigma_n)$ be the diagonalization of the metric of the space perpendicular to the submanifold at point $x^a$.<br>
So $\eta^\top = (\eta^\top)_{ij} e^i \otimes e^j$ and $(\eta^\top)_{ij} = 0$ for i or j $\in \{1 ... m\}$ or otherwise $= \eta_{ij}$.<br>

$\eta^\top = i \downarrow \overset{j \rightarrow}{
\left[ \begin{array}{ccc|cc}
	0 			& 			& 0 		& 0 &			& 0 	\\
				& \ddots 	& 			& 	& \ddots 	&		\\
	0 			& 			& 0			& 0 & 	 	 	& 0 	\\
	\hline                                         
	0 			& 			& 0 		& \sigma_{m+1} & 	 	 	& 0 	\\
				& \ddots	& 			& 	& \ddots 	&		\\
	0 			& 			& 0 		& 0 & 			& \sigma_n
\end{array} \right]
}$<br>
<br>

So $\eta = \eta^\perp + \eta^\top$<br>
<br>



Let 
$
	\Sigma 
	= e^1 \wedge ... \wedge e^m 
	= m! e^{[1} \otimes ... \otimes e^{m]}
	= \delta^{1 ... m}_{i_1 ... i_m} e^{i_1} \otimes ... \otimes e^{i_m}
$ be the basis of the submanifold.<br>
So that in component form, $\Sigma = \Sigma_{i_1 ... i_m} e^{i_1} \otimes ... \otimes e^{i_m}$
for $\Sigma_{i_1 ... i_m} = \delta^{1 ... m}_{i_1 ... i_m} = m! \delta^1_{[i_1} \cdot ... \cdot \delta^m_{i_m]}$<br>
<br>

So $
	\Sigma(e_j) 
	= \delta^{1 ... m}_{i_1 ... i_m} (e^{i_1} \otimes ... \otimes e^{i_m}) (e_j)
	= \delta^{1 ... m}_{i_1 ... i_m} e^{i_1}(e_j) e^{i_2} \otimes ... \otimes e^{i_m}
	= \delta^{1 ... m}_{i_1 ... i_m} \delta^{i_1}_j e^{i_2} \otimes ... \otimes e^{i_m}
	= \delta^{1 2 ... m}_{j i_2 ... i_m} e^{i_2} \otimes ... \otimes e^{i_m}
$<br>
So $\Sigma(e_j) = 0$ for $j \in \{ m+1 ... n \}$, 
and $\Sigma(e_j) = \pm 1$ for $j \in  \{ 1 ... m \}$.<br>
TODO better explanation.<br>
<br>

Let $
	N = \star \Sigma 
	= *(e^1 \wedge ... \wedge e^m) 
	= e^{m+1} \wedge ... \wedge e^n 
	= m! e^{[m+1} \otimes ... \otimes e^{n]}
$ be the basis of the perpendicular space.<br>
So $
	N_{i_{m+1} ... i_n} 
	= \frac{1}{m!} \Sigma_{i_1 ... i_m} {\epsilon^{i_1 ... i_m}}_{i_{m+1} ... i_n}
	= \frac{1}{m!} \delta^{1 ... m}_{i_1 ... i_m} {\epsilon^{i_1 ... i_m}}_{i_{m+1} ... i_n}
	= {\epsilon^{1 ... m}}_{i_{m+1} ... i_n}
	= g^{1 i_1} \cdot ... \cdot g^{m i_m} \cdot \epsilon_{i_1 ... i_m i_{m+1} ... i_n}
	= g^{1 i_1} \cdot ... \cdot g^{m i_m} \cdot \epsilon_{i_1 ... i_n}
$<br>
TODO define N in terms of $\eta^{i_1 ... i_m}$?<br>
<br>

<!-- 
Let $N^\sharp = g^{1 i_1} \cdot ... g^{m i_m} \epsilon_{i_1 ... i_n}$<Br>
So that $(N^\flat)^{i_{m+1} ... i_n} = \epsilon_{1_1 ... i_n}$<br>
<br>
-->

$N(e_j)$ for $j \in \{1...n\}$<br>
$= \epsilon_{1 ... m i_{m+1} ... i_n} (e^{i_{m+1}} \otimes ... \otimes e^{i_n}) (e_j)$<br>
$= \epsilon_{1 ... m i_{m+1} ... i_n} e^{i_{m+2}} \otimes ... \otimes e^{i_n} \delta^{i_{m+1}}_j$<br>
$= \epsilon_{1 ... m j i_{m+2} ... i_n} e^{i_{m+2}} \otimes ... \otimes e^{i_n}$<br>
$= 0$ if $j \in \{ 1 ... m \}$ since our permutation tensor $\epsilon_I$ will get repeated-index.<br>
$= \pm \sqrt{|g|}$ if $j \in \{ m+1 ... n \}$ since we don't have repeated roots.<Br>
<br>

Let $n^i$ be the the i'th normal form.<br>
Define the i'th normal $n^i$ as the inverse of the non-coordinate transform to diagonalize the metric but only for the $i \in \{m+1 ... n\}$ indexes:<br>
$n^i = (n^i)_u e^u = {e^i}_u e^u$,<br>

Notice that, for diagonalization:<br>
$g_{uv}$<br>
$= e_u \cdot e_v$<br>
$= {e^i}_u {e^j}_v \eta_{ij}$<br>
$= {e^i}_u {e^j}_v (\eta^\perp_{ij} + \eta^\top_{ij})$<br>
$= 
	{e^i}_u {e^j}_v \eta^\perp_{ij}
	+ \frac{1}{\alpha_i} (n^i)_u \frac{1}{\alpha_j} (n^j)_v \eta^\top_{ij} 
$<br>
$= \underset{{i,j \in [m+1,n]}}{\Sigma} \frac{1}{\alpha_i} (n^i)_u \frac{1}{\alpha_j} (n^j)_v \eta^\top_{ij} 
	+ \underset{{i,j \in [1,m]}}{\Sigma} {e^i}_u {e^j}_v \eta^\perp_{ij}
$<br>
So you still need influence from the submanifold basis.<br>

<br>
In contrast:<br>
$(\eta^\top)^{ij}$<br>
$= {e^i}_u {e^j}_v g^{uv}$<br>
$= \frac{1}{\alpha_i} \frac{1}{\alpha_j} (n^i)_u (n^j)_v g^{uv}$<br>
TODO is this true?:<br>
for linear diagonalization of the metric ${e^i}_u$ such that ${e^i}_u {e^j}_v (\eta^\top)_{ij} = g_{uv}$<br>
so $n \eta^\top n = {n^i}_u (\eta^\top)_{ij} {n^j}_v e^u \otimes e^v = g_{uv} e^u \otimes e^v$<br>
<br>

There are multiple ways to define an inverse diagonalization ${e^i}_u = (n^i)_u$.<br>
Choose the specific diagonalization: 
$	n^i 
	= -(\alpha_i) \nabla (x^i) 
	= -(\alpha_i) e^u \nabla_u (x^i) 
	= -(\alpha_i) e^u \delta_u^i 
	= -(\alpha_i) e^i
$<br>
Normalize it so $\sigma_i = (\alpha_i)^2 n^i \cdot n^i = (\alpha_i)^2 g^{ii}$<br>
Notice that $\alpha_i$ is a scalar, not tensoral, and the i index isn't paired with a basis element.<br>
<br>

The vector form of normal $n^i$ is: 
$
	n^i
	= (n^i)_u e^u
	= (n^i)^u e_u
	= g^{uv} (n^i)_v e_u 
	= -g^{uv} (\alpha_i) \delta^i_v e_u 
	= -g^{ui} (\alpha_i) e_u
$<br>
So $(n^i)^u = -g^{ui} (\alpha_i)$<br>
Let $(\beta^i)^u = g^{ui} (\alpha_i)$ be the i'th shift vector.<br>
<br>

I think this definition of $n^i = (n^i)_u e^u = {e^i}_u e^u$ should work with the definition of $N = \star \Sigma$ as well.<br>
Then $\gamma \ne g + N \otimes N$ can be used as well..<br>
<br>

Let $
	\gamma 
	= \gamma_{uv} e^u \otimes e^v 
	= g - n \eta^\top n
	= g - (\eta^\top)_{ij} n^i \otimes n^j
	= (g_{uv} - (\eta^\top)_{ij} {n^i}_u {n^j}_v)(e^u \otimes e^v)
$ be the projection operator<br>
<br>

So for $k \in \{1 ... m\}$:<br>
$\gamma(e_k)$<br>
$= (g_{uv} - (\eta^\top)_{ij} {n^i}_u {n^j}_v)(e^u \otimes e^v) (e_k)$<br>
$= (g_{uv} - (\eta^\top)_{ij} {e^i}_u {e^j}_v) \delta^u_k e^v$<br>
$= (g_{uv} - (\eta^\top)_{ij} (\alpha_i) (\alpha_j) \delta^i_u \delta^j_v) \delta^u_k e^v$<br>
$= (g_{kv} - (\eta^\top)_{kv} (\alpha_k) (\alpha_v)) e^v$<br>
Since $k \in \{1 ... m\}, (\eta^\top)_{kv} = 0$<br>
$= g_{kv} e^v$<br>
$= e_k$<br>
<br>

And for $i \in \{m+1 ... n \}$:<br>
$\gamma(e_k)$<br>
$= (g - n \eta^\top n) (e_k)$<br>
$= (g_{kv} - (\eta^\top)_{kv} (\alpha_k) (\alpha_v)) e^v$<br>
Since $k \in \{1 ... m\}, (\eta^\top)_{kv} = \eta_{kv} = \delta_{kv} \sigma_k$<br>
$= (g_{kv} - \delta_{kv} \sigma_k (\alpha_k) (\alpha_v)) e^v$<br>
TODO prove this is equal ... seems like I'll have to use raised $(n^i)^u = -g^{iu} (\alpha_i)$ somehow.<br>
$= 0$<br>
<br>

<br>
<a href='index.html'>Back</a><br>
	</body>
</html>
