<!doctype html>
<html>
	<head>
        <meta charset='utf-8'>
		<script type='text/javascript' src='../tryToFindMathJax.js'></script>
		<title>Parallel Propagators</title>
	</head>
	<body>

<br>

Start with a manifold $\mathcal{M}$ with dimension n.<br>
Define a point on the manifold: $p \in \mathcal{M}$<br>
Define the tangent space at a point on the manifold: $T_p(\mathcal{M})$<br>
Let u be a coordinate chart of the manifold, and let $x = \{x^\mu\}$ be a tuple of coordinates in the chart's domain, with $\mu$ spanning 1 to n (or 0 to n-1 if you would like), so $u(x) = p$.<br>
Define a basis $\{e_\mu(x)\}$ that spans $T_p(\mathcal{M})$.<br>
<br>

Define the basis represented in global Cartesian components to be: $e_\mu(x) = {e_\mu}^I(x) e_I$, for Cartesian basis $e_I$<Br>
$e_I$ has to be constant wrt all x because we're going to be integrating them across a region.  Don't worry, the transform ${e_\mu}^I$ will be used in the formalities but not in the results.<br>
<br>

Let's write that out per-component.  I'll use hats to denote the Cartesian basis indexes:<br>
${e_\mu}^I = \downarrow I \overset{\rightarrow \mu}{\left[\begin{matrix}
	{e_1}^\hat{1} & ... & {e_n}^\hat{1} \\
	\vdots & & \vdots \\
	{e_1}^\hat{n} & ... & {e_n}^\hat{n}
\end{matrix}\right]} = \overset{\rightarrow \mu}{\left[\begin{matrix}
	e_1 & | & ... & | & e_n
\end{matrix}\right]} = \downarrow I \left[\begin{matrix}
	e^\hat{1} \\ --- \\ \vdots \\ --- \\ e^\hat{n}
\end{matrix}\right]
$<br>
<br>

Notice I picked my matrix representation of my ${e_\mu}^I$ tensor components such that contravariant components are distinct per each row (so contravariant indexes are represented as column vectors) 
and covariant components are distinct per each column (so covariant indexes are represented as row vectors).<br>
<br>

While we're here, let's define the dual basis / inverse transform $e^\mu$, such that $e_\mu e^\nu = \delta^\nu_\mu$.<br>
And let's define its Cartesian background components $e^\mu = {e^\mu}_I e^I$ where $e^I = e_I$ since the Cartesian metric is identity.<br>
<br>

${e^\mu}_I = \downarrow \mu \overset{\rightarrow I}{\left[\begin{matrix}
	{e^1}_\hat{1} & ... & {e^1}_\hat{n} \\
	\vdots & & \vdots \\
	{e^n}_\hat{1} & ... & {e^n}_\hat{n}
\end{matrix}\right]}$<br>
<br>

Notice that, in matrix form, $[{e^\mu}_I] = [{e_\mu}^I]^{-1}$<br>
<br>

The discernment between the basis and its background when written in index notation - by convention - is whether the first index, stated in curvilinear coordinates, is covariant or contravariant.  Covariant $e_\mu$ represents the vector basis while contravariant $e^\mu$ represents the dual basis.
This is probably a bad idea, and I should probably use some other symbol to denote the dual basis in index notation, maybe even $(e^{-1})^\mu$<br>
<br>

I'm going to be using $[A_{\mu\nu}]$ to denote a matrix representation of a tensor.
Why?  Because often times omitting indexes implies summation of symbols.  So to distinguish between $\Gamma_\mu = {\Gamma^\alpha}_{\mu\alpha} = \frac{1}{2} e_\mu (ln \sqrt{|g|})$ and $\Gamma_\mu = $ a matrix of ${\Gamma^\alpha}_{\mu\beta}$ I will wrap my matrices in brackets: $[\Gamma_\mu]$.<br>
Sometimes I'll still keep the summation indexes as a reminder of contra/co-variant representation of the tensor component quantities, 
other times I will remove the summation indexes if the contra-/co-variance of the tensor should be deducable.
If the brackets do remain around a tensor, even if it is in index notation, that is just to serve as a reminder (to me) that this is a matrix value.<br>
<br>

If I ever drop all indexes from $[e_\mu]$, I will shorthand denote $e = [{e_\mu}^I]$ and $e^{-1} = [{e^\mu}_I]$.<br>
<br>

Arbitrary (p q) tensor:<br>
$A = {A^{\mu_1 ... \mu_p}}_{\nu_1 ... \nu_q} 
	\underset{k=1}{\overset{p}{\otimes}} e_{\mu_k} \underset{k=1}{\overset{q}{\otimes}} e^{\nu_k}
$<br>
Vector field:<br>
$v = v^\mu e_\mu$<br>
<br>

Connection and covariant derivative:<br>
$\nabla_\mu e_\nu = {\Gamma^\alpha}_{\mu\nu} e_\alpha$<br>
<br>
Notice that, for the rest of this worksheet, I am going to require that the connection is a metric-cancelling Levi-Civita connection.
This means it has no torsion.
I am not assuming it is a coordinate basis though, so the connection may include partial metric terms and may include commutation terms.
<br>
<br>

Written out per-component:<br>
$\nabla_\mu \left[\begin{matrix}
	{e_1}^\hat{1} & ... & {e_n}^\hat{1} \\
	\vdots & & \vdots \\
	{e_1}^\hat{n} & ... & {e_n}^\hat{n}
\end{matrix}\right] = \left[\begin{matrix}
	{e_1}^\hat{1} & ... & {e_n}^\hat{1} \\
	\vdots & & \vdots \\
	{e_1}^\hat{n} & ... & {e_n}^\hat{n}
\end{matrix}\right] \left[\begin{matrix}
	{\Gamma^1}_{\mu 1} & ... & {\Gamma^n}_{\mu 1} \\
	\vdots & & \vdots \\
	{\Gamma^1}_{\mu n} & ... & {\Gamma^n}_{\mu n}
\end{matrix}\right]$<br>
<br>

Notice that, because I picked my contravariant components to be distinct per each row (contravariant indexes are represented as column vectors) and covariant components to be distinct per each column (covariant indexes are represented as row vectors), this means the matrix-multiplication of the connection must be on the right hand.  
If you want to multiply the connection matrix on the left hand side of the basis matrix then you must use distinct contravariant components per each column (contravariant indexes are represented as row vectors) and distinct covariant compoments per each row (covariant indexes are represented as column vectors).<br>
<br>

Since we are differentiating the Cartesian components which are constant at all points, let's exchange the covariant derivative with a partial derivative:<br>
$\frac{\partial}{\partial x^\mu} [{e_\nu}^I](x) = [{e_\alpha}^I](x) \cdot [{\Gamma^\alpha}_{\mu\nu}](x) $<br>
<br>

Abuse notation a bit:<br>
$\int [{e^\alpha}_I](x) \cdot d [{e_\nu}^I](x) = \int_{x'^\mu = x_L^\mu}^{x'^\mu = x_R^\mu} [{\Gamma^\alpha}_{\mu\nu}](x') dx'$<br>
<br>

Integrate the linear dynamic system and hide the Cartesian basis index to find:<br>
$[e_\nu](x^\mu_R) = [e_\alpha](x^\mu_L) \cdot exp\left( \int_{x'^\mu = x_L^\mu}^{x'^\mu = x_R^\mu} [{\Gamma^\alpha}_{\mu\nu}](x'^\mu) dx'^\mu \right)$<br>
<br>

...where I make some liberal exchanges of when I want $x^\mu_L$ to indicate the scalar value of the left-bounds of the $\mu$ index of the x coordinate, and when I want it to mean the x coordinate with the $\mu$ component exchanged with the scalar value $x^\mu_L$.<br>
<br>

So that's how you transport a basis from one point $x^\mu_L$ to another point $x^\mu_R$ along a single changing coordinate $x^\mu$.<br>
How about if you want to transport it along an arbitrary coordinate path on the manifold?<br>
For that, for now, I will only consider separate movement along individual coordinate lines:<br>
Let's assume x and y are points on our manifold.<br>
<br>

$[e_\nu](y) = [e_{\alpha_1}](x) 
	\cdot exp\left( \int_{z^1 = x^1}^{z^1 = y^1} [{\Gamma^{\alpha_1}}_{1 {\alpha_2}}](z^1, x^2, ..., x^n) dz^1 \right)
	\cdot ...
	\cdot exp\left( \int_{z^\mu = x^\mu}^{z^\mu = y^\mu} [{\Gamma^{\alpha_\mu}}_{\mu {\alpha_{\mu+1}}}](y^1, ..., y^{\mu-1}, z^\mu, x^{\mu+1}, ..., x^n) dz^\mu \right)
	\cdot ...
	\cdot exp\left( \int_{z^n = x^n}^{z^n = y^n} [{\Gamma^{\alpha_n}}_{n \nu}](y^1, ..., y^{n-1}, z^n) dz^n \right)
$<br>
<br>
Notice that I am transforming the original basis across each connection's coordinate dimension individually, and after I perform each transform I exchange the source coordinate in the integral with the destination component.<br>
Think of it like traversing the edges of a n-hypercube to get from (0,0,...,0) to (1,1,...,1).<br>
<br>

Maybe with some more notation abuse I could write that as:<br>
$[e_\nu](y) = [e_\alpha](x) 
	\cdot \underset{\mu=1}{\overset{n}{\Pi}} exp\left( \int_{z^\mu = x^\mu}^{z^\mu = y^\mu} [{\Gamma^\alpha}_{\mu \nu}](y^{1..\mu-1} | z^\mu | x^{\mu+1..n}) dz^\mu \right)$<br>
<br>

Let's spare ourselves from writing this term any more than one time.  Let's define the 'parallel propagator', a linear transform for propagating vector components from the tangent space at one component on a manifold to the tangent space at another point:<br>
<br>
${P_\mu(x_L^\mu, x_R^\mu)^\alpha}_\nu = exp\left( -\int_{x'^\mu = x_L^\mu}^{x'^\mu = x_R^\mu} [{\Gamma^\alpha}_{\mu\nu}](x'^\mu) dx'^\mu \right)$<br>
<br>
The subscript $\mu$ here is denoting that we are travelling along a curve along the coordinate line of $x^\mu$.
Notice the minus sign in the exponent.  This means we will perform the inverse of this when we apply it to our ${e_\mu}^I(x)$ basis.  
Keep this in mind, it'll make sense later when we get to vector components.<br>
<br>

Mind you the result of $P(a,b)$ is a matrix, and the $\alpha$ and $\nu$ in this case are the matrix indexes.<br>
Since it is the exponent of the integral of a matrix of the connection coefficients, I'm willing to bet that its transpose is what we use to propagate one-forms in the same way that the connection matrix transpose is used in the covariant derivative of one-forms.<br>
For more on the parallel propagator, check out chapter 3 of Carroll.<br>
<br>

Notice that this depends on the identity: $P^{-1}(x,y) = P(y,x)$<br>
That can be proven quickly with:<br>
$P(y,x)$<br>
$= exp(-\int_y^x [\Gamma_v] d\lambda)$<br>
$= exp(\int_x^y [\Gamma_v] d\lambda)$<br>
$= exp(-\int_x^y [\Gamma_v] d\lambda)^{-1}$<br>
$= P^{-1}(x,y)$<br>
<br>

Now the above transformation from $e_\nu(x_L^\mu)$ to $e_\nu(y^\mu_R)$ would look like:<br>
$e(x_R^\mu) = e(x_L^\mu) \cdot (P^{-1} (x_L^\mu, x_R^\mu)) = e(x_L^\mu) \cdot (P(x_R^\mu, x_L^\mu))$<br>
<br>

Now the above transformation from $e_\nu(x)$ to $e_\nu(y)$, going component-by-component along the corners of a hypercube, would look like:<br>
$e(y) = e(x) 
	\cdot P_1^{-1}((x^1, x^2, ,..., x^n), (y^1, x^2, ,..., x^n))
	\cdot ... 
	\cdot P_k^{-1}((y^1, ..., y^{k-1}, x^k,, x^{k+1}, ..., x^n), (y^1, ..., y^{k-1}, y^k, x^{k+1}, ..., x^n))
	\cdot ... 
	\cdot P_n^{-1}((y^1, ..., y^{n-1}, x^n), (y^1, ..., y^{n-1}, y^n))$<br>
Where we have slowly replaced one component of our coordinate at a time until we arrived at the destination of the curve.<br>
<br>
That can just be written as $e(x) \underset{k=1}{\overset{n}{\Pi}} P_k^{-1}(x^k, y^k)$ if we don't mind hiding the chart coordinates, which seem to be an important detail.<br>
<br>
<br>

Notice that order of application of the exp-integrals-of-connections is important.  If you were to swap orders then you would have to deal with the $[\nabla_\mu, \nabla_\mu] e_\alpha = e_\beta {R^\beta}_{\alpha\mu\nu}$ Riemann curvature tensor.  Think about exactly how later.  I saw in the referenced S.E. that the commutation of the infintesimal propagator in the $e_\mu$ and then $e_\nu$ direction produces the curvature, so that answers that.<br>
<br>
Does this mean that, by the Bianchi identity of the Riemann curvature tensor, that exchanging an index may produce a result whose difference with the original is proportional to the Riemann curvature tensor but cycling all indexes will result in the same integral?  I'll think about that later.  Look into the path ordering stuff of Carroll for more on this.<br>
<br>
<br>

Alright, what if you do want to just cut across the diagonal of the coordinate space hypercube instead of following the edges around the outside?
I'm guessing that would look like:<br>
<br>
$[e_\nu](y) = [e_\alpha](x) \cdot exp\left( -\int_{\lambda = 0}^{\lambda = 1} [{\Gamma^\alpha}_{\mu\nu}](x + \lambda v) v^\mu d\lambda \right)^{-1}$<br>
<br>
...where $v = y - x$.  I'll call this the generalized linear case.
If you want to propagate along one certain coordinate $\mu$ a distance of $l$, just set $v^\nu = l \cdot \delta^\nu_\mu$.<br>
<br>

This definition of the generalized linear parallel propagator would be especially useful in the 3D rotation group, where the parallel propagator along a line corresponds to a rotation about the axis of the vector between line coordinates.<br> 
Propagating along the x coordinate then the y coordinate vs y-then-x will give you the commutation of rotating along the x-axis then y-axis vs y-axis then x-axis -- both represent different rotations, and both are different to rotating directly along the (1,1,0) axis.<br>
<br>

Mind you, changing the chart coordinates in a linear fashion like this does not produce a geodesic transport.
In order to do that, you would (once again) need to make use of the connection.  A double integral of some function of the connection?  
After all, the geodesic equation is a 2nd derivative of the position.  Think on this one more later.
The shorthand representation of our generalized linear parallel transport could be something like: $e(y) = e(x) P_v^{-1}(x,y)$<br>
<br>

The notation for the generalized linear parallel transport looks just like the notation for transport around the edges of a hypercube representaiton above, even though these represent two different transformations.
It looks like I need to specify in the notation a way to discern the path of the transport as well.
Just write $P_C(s,t)$ and specify that C is a curve, and whether the curve is a geodesic, or a line in coordinate chart space, or a piecewise edge traversal from one corner of a hypercube to the other (as I first stated).
I'm not sure whether s and t should be parameters of that curve (which wouldn't convey much information unless the curve was along a coordinate line), or end-points of the curve (which could translate to some degree with our definition of coordinate-line parallel propoagators, however we would be interchanging points with scalars).
So there is your solution to the geodesic case: just calculate the curve, and then substitute that curve into the coordinate evaluating the connection.<br>
<br>
<br>

<hr>
<br>

So now on to vector/tensor components.  I'll start with vector, but you can extrapolate if you want.<br>
$v = v^\mu e_\mu$<br>
<br>

In matrix form:<br>
$v = \left[\begin{matrix}
	e_1 & | & ... & | & e_n
\end{matrix}\right] \left[\begin{matrix}
	v^1 \\ --- \\ \vdots \\ --- \\ v^n
\end{matrix}\right]$<br>
<br>

As a matrix, in our fixed background Cartesian components (since they are independent of our choice of p):<br>
$v = v^\mu {e_\mu}^I e_I = v^I e_I$<br>
<br>

In matrix form:<br>
$v = \left[\begin{matrix}
	e_\hat{1} & | & ... & | & e_\hat{n}
\end{matrix}\right] \left[\begin{matrix}
	v^\hat{1} \\ --- \\ \vdots \\ --- \\ v^\hat{n}
\end{matrix}\right] = \left[\begin{matrix}
	e_\hat{1} & | & ... & | & e_\hat{n}
\end{matrix}\right] \left[\begin{matrix}
	{e_1}^\hat{1} & ... & {e_n}^\hat{1} \\
	\vdots & & \vdots \\
	{e_1}^\hat{n} & ... & {e_n}^\hat{n}
\end{matrix}\right] \left[\begin{matrix}
	v^1 \\ --- \\ \vdots \\ --- \\ v^n
\end{matrix}\right]$<br>
<br>

So our background basis components of our vector field are:<br>
<br>
$v^I = {e_\mu}^I v_\mu$<br>
<br>
$\left[\begin{matrix}
	v^\hat{1} \\ --- \\ \vdots \\ --- \\ v^\hat{n}
\end{matrix}\right] = \left[\begin{matrix}
	{e_1}^\hat{1} & ... & {e_n}^\hat{1} \\
	\vdots & & \vdots \\
	{e_1}^\hat{n} & ... & {e_n}^\hat{n}
\end{matrix}\right] \left[\begin{matrix}
	v^1 \\ --- \\ \vdots \\ --- \\ v^n
\end{matrix}\right]$<br>
<br>

So if we have a vector with components relative to the basis at one point x, how do we find the same vector components relative to a basis at y?<br>
$v(x) = v'(y)$<br>
<br>

I'll take advantage of the fact that our Cartesian components are fixed in order to calculate this:<br>
$v^I(x) = v'^I(y)$<br>
<br>

In matrix form:<br>
<br>

$\left[\begin{matrix}
	v^\hat{1} \\ --- \\ \vdots \\ --- \\ v^\hat{n}
\end{matrix}\right]_{(x)} = \left[\begin{matrix}
	v'^\hat{1} \\ --- \\ \vdots \\ --- \\ v'^\hat{n}
\end{matrix}\right]_{(y)}$<br>
<br>

Notice I've equated my hatted quantities, because these are with respect to our fixed background basis $e_I$.<br>
Now to expand it in terms of the coordinate basis components:<br>
<br>

${e_\mu}^I(x) v^\mu(x) = {e_\mu}^I(y) v'^\mu(y)$<br>
<br>

$\left[\begin{matrix}
	{e_1}^\hat{1} & ... & {e_n}^\hat{1} \\
	\vdots & & \vdots \\
	{e_1}^\hat{n} & ... & {e_n}^\hat{n}
\end{matrix}\right]_{(x)} \left[\begin{matrix}
	v^1 \\ --- \\ \vdots \\ --- \\ v^n
\end{matrix}\right]
 = 
\left[\begin{matrix}
	{e_1}^\hat{1} & ... & {e_n}^\hat{1} \\
	\vdots & & \vdots \\
	{e_1}^\hat{n} & ... & {e_n}^\hat{n}
\end{matrix}\right]_{(y)} \left[\begin{matrix}
	v'^1 \\ --- \\ \vdots \\ --- \\ v'^n
\end{matrix}\right]$
<br>

Solve for $v'^\mu$:<br>
${e^\nu}_I(y) {e_\mu}^I(x) v^\mu(x) = {e^\nu}_I(y) {e_\mu}^I(y) v'^\mu(y)$<br>
${e^\nu}_I(y) {e_\mu}^I(x) v^\mu(x) = \delta^\nu_\mu v'^\mu(y)$<br>
${e^\nu}_I(y) {e_\mu}^I(x) v^\mu(x) = v'^\nu(y)$<br>
<br>

i.e.:<br>
$e^{-1}(y) e(x) v(x) = v'(y)$<br>
$(e(y))^{-1} e(x) v(x) = v'(y)$<br>
<br>

i.e.:<br>
$ \left[\begin{matrix}
	v'^1 \\ --- \\ \vdots \\ --- \\ v'^n
\end{matrix}\right]
= \left( \left[\begin{matrix}
	{e_1}^\hat{1} & ... & {e_n}^\hat{1} \\
	\vdots & & \vdots \\
	{e_1}^\hat{n} & ... & {e_n}^\hat{n}
\end{matrix}\right]_{(y)} \right)^{-1}
\left[\begin{matrix}
	{e_1}^\hat{1} & ... & {e_n}^\hat{1} \\
	\vdots & & \vdots \\
	{e_1}^\hat{n} & ... & {e_n}^\hat{n}
\end{matrix}\right]_{(x)} \left[\begin{matrix}
	v^1 \\ --- \\ \vdots \\ --- \\ v^n
\end{matrix}\right]$<br>
<br>

That looks fine, except we are still using ${e_\mu}^I$, which has components in our Cartesian basis.<br>
How do we represent everything only in our curvilinear chart coordinates?<br>
<br>

Let's represent our $e(y)$ as a parallel transport of the basis from $e(x)$ to $e(y)$.<br>
$e(y) = e(x) \cdot P^{-1}(x,y)$<br>
Substitute to find:<br>
$(e(x) \cdot P^{-1}(x,y))^{-1} e(x) v(x) = v'(y)$<br>
$P(x,y) e^{-1}(x) e(x) v(x) = v'(y)$<br>
$P(x,y) \cdot v(x) = v'(y)$<br>
<br>

Tada! Now we have a representation of the parallel transport that depends only on connections, not on the Cartesian basis.  
I told you that putting that minus sign would make sense.  So our 'forward' definition of the parallel propagator will propagate vector components, while its 'inverse' definition propagates basis vectors.
Just like how the 'forward' definition acts on contravariant indexes and the 'inverse' definition acts on covariant indexes ... and the basis vectors are denoted with covariant indexes.<br>
<br>

The interesting thing is that the resulting components after parallel transport from x to y is $v(x)$ left-multiplied with the transport from y to x.<br>
This is the same as the inverse of the parallel-transport transform from x to y, and that itself will be left-multiplied with the original parallel-transport transform from x to y, and that left-multiplied with $e(x)$, in order to deduce that $v = v'$:<br>
$v = v'$:<br>
$v = e(x) v'(x)$<br> 
$v = e(x) \cdot P^{-1}(x,y) \cdot P(x,y) \cdot v'(x)$<br>
$v = e(x) \cdot P^{-1}(x,y) \cdot v'(y)$<br>
$v = e(y) v'(y)$<br>
$v = v'$<br>
<br>
<br>

<hr>
<br>

How about one-forms?<br>
<br>

Well usually one-form components are represented as row vectors, and the accompanying one-form dual basis can be seen as a right-multiply with a column-'vector':<br>
<br>
$w = w_\mu e^\mu$<br>
<br>

In matrix form:<br>
$w = \left[\begin{matrix}
	w_1 & | & ... & | & w_n
\end{matrix}\right] \left[\begin{matrix}
	e^1 \\ --- \\ \vdots \\ --- \\ e^n
\end{matrix}\right]$<br>
<br>

So what is the propagator of our $e^\mu$ basis?  Well what is the PDE?<br>
<br>
$\nabla_\mu e^\alpha = -{\Gamma^\alpha}_{\mu\nu} e^\nu$<br>
<br>

I could maintain the contravariant-index-is-column-vector, covariant-index-is-row-vector standard and rewrite our linear dynamic system as a left multiplication on the basis (instead of a right) and come up with a parallel propagator matrix which is a left-multiply on the basis and a right-multiply on the one-form components...
Maybe I will later. For now I will just say 'transpose it all', use the previous linear dynamic system we were using except with $e_\mu$'s taking the place of $e^\mu$'s.  You'll see that, to represent the covariant derivative of the one-form dual basis, you need to replace the matrix of connections with its negative, transpose.
Then we integrate and exponent.  What does the negative inside the integral do?  It becomes an inverse on the outside of the exponent.  What does the transpose do?  Well in the orthonormal basis case that our connection is antisymmetric and its exponent is orthogonal, transpose gives us an inverse.
In the orthonormal case those two modifications combine and cancel, and we see that the propagator of a one-form is the same as the propagator of a vector - which makes sense, since the orthonormal metric is the identity matrix, so the one-form component values equal the vector component values.
But take note that, in the non-orthonormal case, you can't use the contravariant propagator to propagate the covariant indexes.  You must use its transpose inverse.<br>
<br>

Let's walk through this, step-by-step.<br>
<br>

Our covariant derivative of our one-form dual-basis:<br>
<br>
$\nabla_\mu e^\nu(x) = -{\Gamma^\nu}_{\mu\alpha}(x) e^\alpha(x)$<br>
<br>

Represented with respect to a fixed global Cartesian basis:<br>
<br>

$\nabla_\mu [{e^\nu}_I](x) = -[{\Gamma^\nu}_{\mu\alpha}](x) [{e^\alpha}_I](x)$<br>
<br>

Written out per-component using our contravariant-is-column, covariant-is-row convention:<br>
<br>
$\nabla_\mu \left[\begin{matrix}
	{e^1}_\hat{1} & ... & {e^1}_\hat{n} \\
	\vdots & & \vdots \\
	{e^n}_\hat{1} & ... & {e^n}_\hat{n}
\end{matrix}\right] = \left[\begin{matrix}
	-{\Gamma^1}_{\mu 1} & ... & -{\Gamma^1}_{\mu n} \\
	\vdots & & \vdots \\
	-{\Gamma^n}_{\mu 1} & ... & -{\Gamma^n}_{\mu n}
\end{matrix}\right] \left[\begin{matrix}
	{e^1}_\hat{1} & ... & {e^1}_\hat{n} \\
	\vdots & & \vdots \\
	{e^n}_\hat{1} & ... & {e^n}_\hat{n}
\end{matrix}\right]$<br>
<br>

Solve with an exponent:<br>
<br>

$\nabla_\mu [{e^\nu}_I](x^\mu_R) = exp( -\int_{x'^\mu=x^\mu_L}^{x'^\mu=x^\mu_R} [{\Gamma^\nu}_{\mu\alpha}](x') dx' ) [{e^\alpha}_I](x^\mu_L)$<br>
<br>

So our matrix is the same matrix as before, except it is negative'd.  Also instead of a right multiply it is a left multiply (so now everyone can stop biting their nails over this).
In fact, overall, this looks a lot more like the definition of a parallel propagator, and the traditional left-multiply definition of a linear dynamic system.<br>
<br>

Notice that if we transpose everything then we match the previous equation for our vector basis, except that now the connection coefficients are negative'd - and therefore the exponent is inverted.<br>
<br>

So the parallel propagator of a one-form is the transpose-inverse of the parallel propagator of a vector.<br>
<br>

Then of course if you apply this to the one-form, like with the vectors, you must apply its inverse.<br>
<br>
$e^{-1}(y) = P(x,y) \cdot e^{-1}(x)$<br>
<br>
This is just the inverse of our definition for the propagation of the vector basis.<br>
<br>
Then we can assert our one-forms between different points $w = w'$ match:<br>
$w = w'$<br>
$w = w'(x) \cdot e^{-1}(x)$<br>
$w = w'(x) \cdot P^{-1}(x,y) \cdot P(x,y) \cdot e^{-1}(x)$<br>
$w = w'(x) \cdot P^{-1}(x,y) \cdot e^{-1}(y)$<br>
$w = w'(y) \cdot e^{-1}(y)$<br>
$w = w'$<br>
<br>

So here we see:<br>
<br>
$w(y) = w(x) \cdot P^{-1}(x,y)$<br>
<br>
which is the same as<br>
<br>
$w(y) = w(x) \cdot P(y,x)$<br>
<br>
So to parallel-propagate one forms, we right-multiply with the inverse of the propagator - just as when we propagated vectors we left-multiplied the forward parallel propagator.
Of course if we want to view this as a left-multiply - so its form matches the parallel propagator of the vector - then we, once again, transpose our equation.<br>
<br>
$w^T(y) = P^{-T}(x,y) \cdot w^T(x)$<br>
<br>


<hr>
<br>

How about permuting a (p,q) tensor?<br>
<br>

$A(y)$<br>
$= {A^{\mu_1 ... \mu_p}}_{\nu_1 ... \nu_q}(y)
	\underset{k=1}{\overset{p}{\otimes}} e_{\mu_k}(y) \underset{k=1}{\overset{q}{\otimes}} e^{\nu_k}(y)
$<br>
$= {P(x,y)^{\mu_1}}_{\alpha_1} \cdot ... \cdot 
{P(x,y)^{\mu_p}}_{\alpha_p} \cdot 
{P(y,x)^{\beta_1}}_{\nu_1} \cdot ... \cdot
{P(y,x)^{\beta_q}}_{\nu_q} \cdot 
{A^{\alpha_1 ... \alpha_p}}_{\beta_1 ... \beta_q}(y)
	\underset{k=1}{\overset{p}{\otimes}} e_{\mu_k}(x) \underset{k=1}{\overset{q}{\otimes}} e^{\nu_k}(x)
$<br>
<br>

Take note that, true to the definition of one-form / covariant-index propagation, the x and y must be reversed since the parallel propagation matrix must be transposed.<br>
<br>

Let's make our notation a bit more concise.  I am going to generalize the propagator indexes:<br>
<br>


${P(x, y)^{\mu_1 ... \mu_m}}_{\alpha_1 ... \alpha_m}
=
P(x, y) 
	{{}^{\mu_1}}_{\alpha_1} 
	...
P(x, y) 
	{{}^{\mu_m}}_{\alpha_m} 
$<br>
<br>
...where each k and k+m of the 2m indexes match up, so you can write out the destination indexes and then the source indexes next to the parallel propagator of the tensor you are propagating, just as on a single parallel propagator the destination index is first and then the source index is second.<br>
<br>
Maybe I could do one better and simply define it as a giant tensor operator, like the projection operator or the covariant derivative operator:<br>
<br>
$A(y)$<br>
$= P(x, y, A(x))$<br>
$= P(x, y, {A^{\alpha_1 ... \alpha_p}}_{\beta_1 ... \beta_q}(x)
	\underset{k=1}{\overset{p}{\otimes}} e_{\alpha_k}(x) \underset{k=1}{\overset{q}{\otimes}} e^{\beta_k}(x)
$<br>
$= {P(x, y, A)^{\mu_1 ... \mu_p}}_{\nu_1 ... \nu_q} 
	\underset{k=1}{\overset{p}{\otimes}} e_{\mu_k}(x) \underset{k=1}{\overset{q}{\otimes}} e^{\nu_k}(x)
$<br>
$= {P(x, y)^{\mu_1}}_{\alpha_1} \cdot ... 
	\cdot {P(x, y)^{\mu_p}}_{\alpha_p}
	\cdot {P(y, x)^{\beta_1}}_{\nu_1} \cdot ... 
	\cdot {P(y, x)^{\beta_q}}_{\nu_q}
	\cdot {A^{\alpha_1 ... \alpha_p}}_{\beta_1 ... \beta_q}(x)
	\underset{k=1}{\overset{p}{\otimes}} e_{\mu_k}(x) \underset{k=1}{\overset{q}{\otimes}} e^{\nu_k}(x)
$<br>
<br>
Once again notice that the parallel propagated contravariant indexes are replaced with a multiply of $P(x,y)$, while the parallel propagated covariant indexes are propagated with $P(y,x)$<br>
<br>
<br>

<hr>
<br>

Now for a specific example: Polar coordinates:<br>
<br>

chart:<br>
$u^I = \left[\begin{matrix}
	r cos\phi \\
	r sin\phi
\end{matrix}\right]$<br>
<br>

${e_r}^I = {u^I}_{,r} = \left[\begin{matrix}
	{e_r}^x \\
	{e_r}^y
\end{matrix}\right] = \left[\begin{matrix}
	cos\phi \\
	sin\phi
\end{matrix}\right]$<br>
<br>

${e_\phi}^I = {u^I}_{,\phi} = \left[\begin{matrix}
	{e_\phi}^x \\
	{e_\phi}^y
\end{matrix}\right] = \left[\begin{matrix}
	-r sin\phi \\
	r cos\phi
\end{matrix}\right]$<br>
<br>

${e_\mu}^I = {u^I}_{,\mu} = \left[\begin{matrix}
	{e_r}^x	& {e_\phi}^x \\
	{e_r}^y	& {e_\phi}^y
\end{matrix}\right] = \left[\begin{matrix}
	cos\phi & -r sin\phi \\
	sin\phi & r cos\phi
\end{matrix}\right]$<br>
<br>

It might be later convenient to think of this as a product of two linear operations:<br>
<br>
$[{e_\mu}^I] = \left[\begin{matrix}
	cos\phi & -sin\phi \\
	sin\phi & cos\phi
\end{matrix}\right] \left[\begin{matrix}
	1 & 0 \\
	0 & r
\end{matrix}\right] = R(\phi) \cdot S(1,r)$<br>
<br>

...where $S(a,b) = \left[\begin{matrix} a & 0 \\ 0 & b \end{matrix}\right]$ is a scale matrix, and $R(\phi)$ is defined as whatever above is left, which happens to look like a rotation matrix.<br>
<br>
<br>

Now for the connections:<br>
<br>

${\Gamma^r}_{\phi\phi} = -r$<br>
${\Gamma^\phi}_{r\phi} = {\Gamma^\phi}_{\phi r} = \frac{1}{r}$<br>
<br>

$[\Gamma_\mu] = \left[\begin{matrix}
	{\Gamma^r}_{\mu r} & {\Gamma^\phi}_{\mu r} \\
	{\Gamma^r}_{\mu\phi} & {\Gamma^\phi}_{\mu\phi}
\end{matrix}\right]$<br>
<br>

$[\Gamma_r] = \left[\begin{matrix}
	{\Gamma^r}_{r r} & {\Gamma^\phi}_{r r} \\
	{\Gamma^r}_{r\phi} & {\Gamma^\phi}_{r\phi}
\end{matrix}\right] = \left[\begin{matrix}
	0 & 0 \\
	0 & \frac{1}{r}
\end{matrix}\right]$<br>
<br>


$[\Gamma_\phi] = \left[\begin{matrix}
	{\Gamma^r}_{\phi r} & {\Gamma^\phi}_{\phi r} \\
	{\Gamma^r}_{\phi\phi} & {\Gamma^\phi}_{\phi\phi}
\end{matrix}\right] = \left[\begin{matrix}
	0 & -r \\
	\frac{1}{r} & 0
\end{matrix}\right]$<br>
<br>

And now for the exponentials of the integrals of the connections:<br>
<br>

$P^{-1}(r_L, r_R) = exp(\int_{r_L}^{r_R} [\Gamma_r] dr)$<br>
$= exp\left( \int_{r_L}^{r_R} \left[\begin{matrix}
	0 & 0 \\
	0 & \frac{1}{r}
\end{matrix}\right]  dr \right)$<br>
$= exp\left( \left[\begin{matrix}
	0 & 0 \\
	0 & ln(r)
\end{matrix}\right]|_{r_L}^{r_R} \right)$<br>
$= exp\left( \left[\begin{matrix}
	0 & 0 \\
	0 & ln(r_R) - ln(r_L)
\end{matrix}\right] \right)$<br>
$= exp\left( \left[\begin{matrix}
	0 & 0 \\
	0 & ln(\frac{r_R}{r_L})
\end{matrix}\right] \right)$<br>
Since we now have a diagonal matrix, we can assert that the exponent of the matrix is a matrix of the exponent of the diagonals:<br>
$= \left[\begin{matrix}
	exp(0) & 0 \\
	0 & exp(ln(\frac{r_R}{r_L}))
\end{matrix}\right]$<br>
$= \left[\begin{matrix}
	1 & 0 \\
	0 & \frac{r_R}{r_L}
\end{matrix}\right]$<br>
$= S(1, \frac{r_R}{r_L})$<br>
<br>

$P^{-1}(\phi_L, \phi_R) = exp(\int_{\phi_L}^{\phi_R} [\Gamma_\phi] d\phi)$<br>
$= exp\left(\int_{\phi_L}^{\phi_R} \left[\begin{matrix}
	0 & -r \\
	\frac{1}{r} & 0
\end{matrix}\right] d\phi \right)$<br>
$= exp\left( \left[\begin{matrix}
	0 & -r (\phi_R - \phi_L) \\
	\frac{1}{r} (\phi_R - \phi_L) & 0
\end{matrix}\right] \right)$<br>
Now we eigen-decompose the matrix before applying the exponential to the eigenvalues:<br> 
$= exp\left( 
\left[\begin{matrix} 
	1 & -i r \\ 
	1 & i r
\end{matrix}\right] 
\left[\begin{matrix} 
	-i (\phi_R - \phi_L) & 0 \\ 
	0 & i (\phi_R - \phi_L)
\end{matrix}\right] 
\left[\begin{matrix} 
	\frac{1}{2} & \frac{1}{2} \\ 
	\frac{i}{2 r} & -\frac{i}{2r}
\end{matrix}\right]
\right)$<br>
$= \left[\begin{matrix} 
	1 & -i r \\ 
	1 & i r
\end{matrix}\right] 
exp\left( \left[\begin{matrix} 
	-i (\phi_R - \phi_L) & 0 \\ 
	0 & i (\phi_R - \phi_L)
\end{matrix}\right] \right)
\left[\begin{matrix} 
	\frac{1}{2} & \frac{1}{2} \\ 
	\frac{i}{2 r} & -\frac{i}{2r}
\end{matrix}\right]
$<br>
Now we use the fact that the exponent of a diagonal matrix is the matrix of the exponent of the individual diagonal elements:<br>
$= \left[\begin{matrix} 
	1 & -i r \\ 
	1 & i r
\end{matrix}\right] 
\left[\begin{matrix} 
	exp(-i (\phi_R - \phi_L)) & 0 \\ 
	0 & exp(i (\phi_R - \phi_L))
\end{matrix}\right]
\left[\begin{matrix} 
	\frac{1}{2} & \frac{1}{2} \\ 
	\frac{i}{2 r} & -\frac{i}{2r}
\end{matrix}\right]
$<br>

$= \left[\begin{matrix}
	cos(\phi_R - \phi_L) & -r sin(\phi_R - \phi_L) \\
	\frac{1}{r} sin(\phi_R - \phi_L) & cos(\phi_R - \phi_L)
\end{matrix}\right]$<br> 
$= \left[\begin{matrix}
	1 & 0 \\
	0 & \frac{1}{r}
\end{matrix}\right] \left[\begin{matrix}
	cos(\phi_R - \phi_L) & -sin(\phi_R - \phi_L) \\
	sin(\phi_R - \phi_L) & cos(\phi_R - \phi_L)
\end{matrix}\right] 
\left[\begin{matrix}
	1 & 0 \\
	0 & r 
\end{matrix}\right]$<br>
$= S(1,\frac{1}{r}) R(\phi_R - \phi_L) S(1, r)$<br>
<br>
<br>

It just so happens that 
$P^{-1}(r_L, r_R) \cdot P^{-1}(\phi_L, \phi_R) = P^{-1}(\phi_L, \phi_R) \cdot P^{-1}(r_L, r_R)$.  
Maybe because ${R^\alpha}_{\beta\mu\nu} = 0$, but that is still just speculation.<br>
And the applying the parallel transport in different orderings gives us:<br> 
<br>

$P^{-1}(r_L, r_R) \cdot P^{-1}(\phi_L, \phi_R)$<br>
$= S(1, \frac{r_R}{r_L}) S(1,\frac{1}{r_R}) R(\phi_R - \phi_L) S(1, r_R)$<br>
$= S(1,\frac{1}{r_L}) R(\phi_R - \phi_L) S(1, r_R)$<br>
<br>
$P^{-1}(\phi_L, \phi_R) \cdot P^{-1}(r_L, r_R)$<br>
$= S(1,\frac{1}{r_L}) R(\phi_R - \phi_L) S(1, r_L) S(1, \frac{r_R}{r_L})$<br>
$= S(1,\frac{1}{r_L}) R(\phi_R - \phi_L) S(1, r_R)$<br>
<br>

So in both cases we end up with the general linear pararallel transport from coordinate chart domain point $x_L$ to $x_R$ as:<br>
<br>
$P^{-1}(x_L, x_R) = S(1, \frac{1}{r_L}) \cdot R(\phi_R - \phi_L) \cdot S(1, r_R)
= \left[\begin{matrix}
	cos(\phi_R - \phi_L) & -r_R sin(\phi_R - \phi_L) \\
	\frac{1}{r_L} sin(\phi_R - \phi_L) & \frac{r_R}{r_L} cos(\phi_R - \phi_L)
\end{matrix}\right]$<br> 
<br>
<br>

What happens when we parallel transport our basis from $x_L$ to $x_R$?<br>
<br>
$e(x_L) \cdot P^{-1}(x_L, x_R) = 
\left(
	R(\phi_L) \cdot S(1, r_L) 
\right) \cdot \left(
	S(1, \frac{1}{r_L}) \cdot R(\phi_R - \phi_L) \cdot S(1, r_R)
\right)$<br>
$= R(\phi_L) \cdot R(\phi_R - \phi_L) \cdot S(1, r_R)$<br>
And since our rotation axis are aligned, we can add our rotation angles together:<br>
$= R(\phi_R) \cdot S(1, r_R)$<br>
$= e(x_R)$<br>
<br>

Tada!  We started with $e(x_L)$, we right-applied our parallel-transport transformation, and we ended up at $e(x_R)$.<br>
<br>

TODO verify that this works with my generalized linear case equation above, and not just per-coordinate separately.  
It probably will but only because polar coordinates have no Riemann curvature.  
Also, try it with the geodesic case.<br> 
<br>

Now do the same thing, except give it some vector components:<br>
$e_\mu(x_R) v^\mu(x_R)$<br>
$= e_\alpha (x_L) \cdot {P^{-1}(x_L, x_R)^\alpha}_\mu v^\mu(x_R)$<br>
Substitute for polar:<br>
$= e (x_L) \cdot ( S(1,\frac{1}{r_L}) R(\phi_R - \phi_L) S(1, r_R) ) v(x_R)$<br>
$= e (x_L) v'(x_L)$<br>
So $v'(x_L) = S(1,\frac{1}{r_L}) R(\phi_R - \phi_L) S(1, r_R) v(x_R)$<br>
It seems a bit counter-intuitive that we start with $v^\mu(x_R)$ and we scale its $v^\phi(x_R)$ component up by $r_R$, but don't forget $v^\phi$ is paired with $e_\phi$, and $|e_\phi| = r_R$, so $|v^\phi| = \frac{1}{r_R}$, so what we are doing is removing that influence from the basis at point $x_R$ before we go on to move things to $x_L$.<br>
<br>

Speaking of vector field, let's get a better description of what the curvilinear vector components are in terms of the Cartesian vector components:<br>
$e_I v^I = e_\mu v^\mu$<br>
$\left[\begin{matrix} e_x & e_y \end{matrix}\right] \left[\begin{matrix} v^x \\ v^y \end{matrix}\right]
= \left[\begin{matrix} e_r & e_\phi \end{matrix}\right] \left[\begin{matrix} v^r \\ v^\phi \end{matrix}\right]$<br>
$\left[\begin{matrix} e_x & e_y \end{matrix}\right] \left[\begin{matrix} v^x \\ v^y \end{matrix}\right]
= \left[\begin{matrix} e_x & e_y \end{matrix}\right] \left[\begin{matrix} cos(\phi) & -r sin(\phi) \\ sin(\phi) & r cos(\phi) \end{matrix}\right] \left[\begin{matrix} v^r \\ v^\phi \end{matrix}\right]$<br>
$\left[\begin{matrix} v^x \\ v^y \end{matrix}\right] = \left[\begin{matrix} cos(\phi) & -r sin(\phi) \\ sin(\phi) & r cos(\phi) \end{matrix}\right] \left[\begin{matrix} v^r \\ v^\phi \end{matrix}\right]$<br>
$\left[\begin{matrix} cos(\phi) & sin(\phi) \\ -\frac{1}{r}sin(\phi) & \frac{1}{r} cos(\phi) \end{matrix}\right] \left[\begin{matrix} v^x \\ v^y \end{matrix}\right] = \left[\begin{matrix} v^r \\ v^\phi \end{matrix}\right]$<br>
<br>
So there we can see that the $|v^\phi|$ component has a scalar of $\frac{1}{r}$.<br>
<br>

I just did this section in terms of propagating the basis, but what about parallel-propagating the vector components?<br>
<br>

$P(x_L,x_R) = S(1, \frac{1}{r_R}) \cdot R(\phi_L - \phi_R) \cdot S(1, r_L) 
= \left[\begin{matrix}
	cos(\phi_L - \phi_R) & -r_L sin(\phi_L - \phi_R) \\
	\frac{1}{r_R} sin(\phi_L - \phi_R) & \frac{r_L}{r_R} cos(\phi_L - \phi_R)
\end{matrix}\right]
= \left[\begin{matrix}
	cos(\phi_R - \phi_L) & r_L sin(\phi_R - \phi_L) \\
	-\frac{1}{r_R} sin(\phi_R - \phi_L) & \frac{r_L}{r_R} cos(\phi_R - \phi_L)
\end{matrix}\right]
$<br>
<br>

$v(x_R) = P(x_L, x_R) v(x_L)$<br>
<br>

$\left[\begin{matrix}
	v^r (x_R) \\
	v^\phi (x_R)
\end{matrix}\right] = \left[\begin{matrix}
	cos(\phi_R - \phi_L) & r_L sin(\phi_R - \phi_L) \\
	-\frac{1}{r_R} sin(\phi_R - \phi_L) & \frac{r_L}{r_R} cos(\phi_R - \phi_L)
\end{matrix}\right] \left[\begin{matrix}
	v^r (x_L) \\
	v^\phi (x_L)
\end{matrix}\right]$<br>
$\left[\begin{matrix}
	v^r (x_R) \\
	v^\phi (x_R)
\end{matrix}\right]
= \left[\begin{matrix}
	cos(\phi_R - \phi_L) v^r(x_L) + r_L sin(\phi_R - \phi_L) v^\phi(x_L) \\
	-\frac{1}{r_R} sin(\phi_R - \phi_L) v^r(x_L) + \frac{r_L}{r_R} cos(\phi_R - \phi_L) v^\phi(x_L)
\end{matrix}\right]$<br>
<br>
So what is going on here?  The $|v^\phi(r_L)| = \frac{1}{r_L}$ factor is being removed, and a new $\frac{1}{r_R}$ factor is being introduced.  
Also an inverse rotation is being applied to the vector components, because a forward-rotation of the vector basis is going to coincide with an inverse-rotation of the components.<br>
<br>

How about one-forms?  Here we use the inverse propagator.  If you represent your one-form components as a column vector with the propagator as a left-multiply then you must transpose it as well - but I'll ignore that and stick with my contravariant-column, covariant-rw standard.<br>
<br>

$w(x_R) = w(x_L) P^{-1}(x_L, x_R)$<br>
<br>

$\left[\begin{matrix}
	w^r (x_R) & 
	w^\phi (x_R)
\end{matrix}\right] 
= \left[\begin{matrix}
	w^r (x_L) & 
	w^\phi (x_L)
\end{matrix}\right]
\left[\begin{matrix}
	cos(\phi_R - \phi_L) & -r_R sin(\phi_R - \phi_L) \\
	\frac{1}{r_L} sin(\phi_R - \phi_L) & \frac{r_R}{r_L} cos(\phi_R - \phi_L)
\end{matrix}\right]
$<br>
$\left[\begin{matrix}
	w^r (x_R) & 
	w^\phi (x_R)
\end{matrix}\right] 
= \left[\begin{matrix}
	\left(
		cos(\phi_R - \phi_L) w^r (x_L)
		+ \frac{1}{r_L} sin(\phi_R - \phi_L) w^\phi (x_L) 
	\right) &

	\left(
		-r_R sin(\phi_R - \phi_L) w^r (x_L)
		+ \frac{r_R}{r_L} cos(\phi_R - \phi_L) w^\phi (x_L)
	\right)
\end{matrix}\right]
$<br>
<br>

Looks similar to the vector transform.  We see that $|w_\phi| \propto r$ just as $|d\phi| \propto \frac{1}{r}$, so we are scaling out the old $r_L$ value and scaling in the new $r_R$ value.
Otherwise the rotation of the components $\{w_r, w_\phi\}$ are equivalent to the rotation of the components $\{v^r, v^\phi\}$, courtesy of the one-form parallel propagator being the inverse-transpose of the vector parallel propagator.<br>
<br>

<hr>
<br>

Example: 3D Rotation Group<br>
<br>

Look at <a href='https://thenumbernine.github.io/symmath/tests/output/rotation%20group.html'>https://thenumbernine.github.io/symmath/tests/output/rotation%20group.html</a> for some derivations using symmath.<br>
<br>

Start with your generators:<br>
${[K_j]^i}_k = \epsilon_{ijk}$<br>
<br>

Define your rotation matrix as the exponential map of the generators:<br>
${[R_j(t)]^i}_k = exp(t {[K_j]^i}_k)$<br>
$\frac{\partial}{\partial t} R_j(t) = K_j \cdot R_j(t)$<br>
Solve for the generator now:<br>
$K_j = \frac{\partial}{\partial t} R_j(t) \cdot R_j^{-1}(t)$<br>
then I should show how this becomes $K_j = \frac{\partial}{\partial t} R_j(0)$<br>
<br>

Now define our Euler angles rotation matrix:<br>
$P = R_z(\psi) R_x(\theta) R_z(\phi)$<br>
Define our basis:<br>
$e_j(P) = \frac{\partial}{\partial x^j} P = \frac{\partial x^\hat{k}}{\partial x^j} \frac{\partial}{\partial x^\hat{k}} P = K_j \cdot P$<br>
Where the $x^j$ corresponds to the Cartesian basis while the $\hat{x}^j$ corresponds to the Euler angle rotation.<br>
<br>

Commutation coefficients: ${c_{ij}}^k e_k = [e_i, e_j] = -\epsilon_{ijk} e_k$.<br>
<br>

Connection coefficients: ${\Gamma^\alpha}_{\beta\gamma} = \frac{1}{2} \epsilon_{\alpha\beta\gamma}$ is purely based on the commutation since the metric is constant.<br>
<br>

${\Gamma^i}_{xj} = [\frac{1}{2} \epsilon_{i x j}] = \downarrow i \overset{\rightarrow j}{\left[\begin{matrix}
	0 & 0 & 0 \\
	0 & 0 & -\frac{1}{2} \\
	0 & \frac{1}{2} & 0
\end{matrix}\right]}$<br>
<br>

${\Gamma^i}_{yj} = [\frac{1}{2} \epsilon_{i y j}] = \left[\begin{matrix}
	0 & 0 & \frac{1}{2} \\
	0 & 0 & 0 \\
	-\frac{1}{2} & 0 & 0
\end{matrix}\right]$<br>
<br>

${\Gamma^i}_{z j} = [\frac{1}{2} \epsilon_{i z j}] = \left[\begin{matrix}
	0 & -\frac{1}{2} & 0 \\
	\frac{1}{2} & 0 & 0 \\
	0 & 0 & 0
\end{matrix}\right]$<br>
<br>

Linear combination of the connection:<br>
<br>

$v^i \Gamma_i = \frac{1}{2} \left[\begin{matrix}
	0 & -v^z & v^y \\
	v^z & 0 & -v^x \\
	-v^y & v^x & 0
\end{matrix}\right]$<br>
<br>

Integral of connection:<br>
<br>

$\int_0^1 v^i \Gamma_i d\lambda = v^i \Gamma_i = \frac{1}{2} \left[\begin{matrix}
	0 & -v^z & v^y \\
	v^z & 0 & -v^x \\
	-v^y & v^x & 0
\end{matrix}\right]$<br>
<br>

Parallel propagator, based on the 'general linear' form / linear interpolation in chart coordinates:<br>
<br>

$P(x,y) = exp(-\int_{\lambda = 0}^{\lambda = 1} \Gamma_i v^i d\lambda)$<br>

$P(x,y) = exp\left( \frac{1}{2} \left[\begin{matrix}
	0 & v^z & -v^y \\
	-v^z & 0 & v^x \\
	v^y & -v^x & 0
\end{matrix}\right] \right)$<br>
<br>

Let $|v| = \sqrt{(v^x)^2 + (v^y)^2 + (v^z)^2}$ and $\hat{v}^i = \frac{1}{|v|} v^i$<br>
<br>

$P(x,y) = exp\left( \frac{|v|}{2} \left[\begin{matrix}
	0 & \hat{v}^z & -\hat{v}^y \\
	-\hat{v}^z & 0 & \hat{v}^x \\
	\hat{v}^y & -\hat{v}^x & 0
\end{matrix}\right] \right)$<br>
<br>

$P(x,y) = R_\hat{v}(-\frac{|v|}{2})$<br>
<br>

Applied:<br>
$v = e(x) v(x)$<br>
$= e(x) P(y,x) P(x,y) v(x)$<br>
$= e(x) R_\hat{v}(\frac{|v|}{2}) R_\hat{v}(-\frac{|v|}{2}) v(x)$<br>
$= e(y) v'(y)$<br>
<br>

So $e(y) = e(x) R_\hat{v}(\frac{|v|}{2})$<br>
And $v'(y) = R_\hat{v}(-\frac{|v|}{2}) v(x)$<br>
<br>

Conceptually, left-multiplying a transformation is applying the transformation relative to the global basis, and right-multiplying it is apply it relative to the local basis.<br>
<br>

Riemann curvature = ${R^i}_{jkl} = \frac{1}{2} \delta^{ij}_{kl}$<br>
<br>

Notice that the dual of the matrix is a vector whose magnitude serves as the rotation angle and whose axis serves as the rotation axis.<br>
Parallel propagator = exponent of integral of connection = rotation matrix along axis by angle as stated above.<br>
Commutation of parallel propagator = Levi-Civita permutation tensor again.<br>
<br>
<br>

	</body>
</html>
