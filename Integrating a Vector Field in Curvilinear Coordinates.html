<!doctype html>
<html>
	<head>
        <meta charset='utf-8'>
		<script type='text/javascript' src='tryToFindMathJax.js'></script>
		<title>Integrating a Vector Field in Curvilinear Coordinates</title>
	</head>
	<body>

by Christopher Moore<br>
<br>

Start with a manifold $\mathcal{M}$ with dimension n.<br>
Define a point on the manifold: $p \in \mathcal{M}$<br>
Define the tangent space at a point on the manifold: $T_p(\mathcal{M})$<br>
Let u be a coordinate chart of the manifold, and let $x = \{x^\mu\}$ be a tuple of coordinates in the chart's domain, with $\mu$ spanning 1 to n (or 0 to n-1 if you would like), so $u(x) = p$.<br>
Define a basis $\{e_\mu(x)\}$ that spans $T_p(\mathcal{M})$.<br>
<br>

Define the basis represented in global Cartesian components to be: $e_\mu(x) = {e_\mu}^I(x) e_I$, for Cartesian basis $e_I$<Br>
$e_I$ has to be constant wrt all x because we're going to be integrating them across a region.  Don't worry, the transform ${e_\mu}^I$ will be used in the formalities but not in the results.<br>
<br>

Let's write that out per-component.  I'll use hats to denote the Cartesian basis indexes:<br>
${e_\mu}^I = \downarrow I \overset{\rightarrow \mu}{\left[\begin{matrix}
	{e_1}^\hat{1} & ... & {e_n}^\hat{1} \\
	\vdots & & \vdots \\
	{e_1}^\hat{n} & ... & {e_n}^\hat{n}
\end{matrix}\right]} = \overset{\rightarrow \mu}{\left[\begin{matrix}
	e_1 & | & ... &  | & e_n
\end{matrix}\right]} = \downarrow I \left[\begin{matrix}
	e^\hat{1} \\ --- \\ \vdots \\ --- \\ e^\hat{n}
\end{matrix}\right]
$<br>
<br>

Notice I picked my matrix representation of my ${e_\mu}^I$ tensor components such that contravariant components are distinct per each row (so contravariant indexes are represented as column vectors) 
and covariant components are distinct per each column (so covariant indexes are represented as row vectors).<br>
<br>

While we're here, let's define the dual basis / inverse transform $e^\mu$, such that $e_\mu e^\nu = \delta^\nu_\mu$.<br>
And lets define its Cartesian background components $e^\mu = {e^\mu}_I e^I$ where $e^I = e_I$ since the Cartesian metric is identity.<br>
<br>

${e^\mu}_I = \downarrow \mu \overset{\rightarrow I}{\left[\begin{matrix}
	{e^1}_\hat{1} & ... & {e^1}_\hat{n} \\
	\vdots & & \vdots \\
	{e^n}_\hat{1} & ... & {e^n}_\hat{n}
\end{matrix}\right]}$<br>
<br>

Notice that, in matrix form, $[{e^\mu}_I] = [{e_\mu}^I]^{-1}$<br>
<br>

The discernment between the basis and its background when written in index notation - by convention - is whether the first index, stated in curvilinear coordinates, is covariant or contravariant.  Covariant $e_\mu$ represents the vector basis while contravariant $e^\mu$ represents the dual basis.
This is probably a bad idea, and I should probably use some other symbol to denote the dual basis in index notation, maybe even $(e^{-1})^\mu$<br>
<br>

I'm going to be using $[A_{\mu\nu}]$ to denote a matrix representation of a tensor.
Sometimes I'll still keep the summation indexes as a reminder of contra/co-variant representation of the tensor component quantities, 
other times I will remove the summation indexes if the contra-/co-variance of the tensor should be deducable.
If the brackets do remain around a tensor, even if it is in index notation, that is just to serve as a reminder (to me) that this is a matrix value.<br>
<br>

If I ever drop all indexes from $[e_\mu]$, I will shorthand denote $e = [{e_\mu}^I]$ and $e^{-1} = [{e^\mu}_I]$.<br>
<br>

Arbitrary (p q) tensor:<br>
$A = {A^{\mu_1 ... \mu_p}}_{\nu_1 ... \nu_q} \underset{k=1}{\overset{p}{\otimes}} e_{\mu_k} \underset{k=1}{\overset{q}{\otimes}} e^{\nu_k} $<br>
Vector field:<br>
$v = v^\mu e_\mu$<br>
<br>

Connection and covariant derivative:<br>
$\nabla_\mu e_\nu = {\Gamma^\alpha}_{\mu\nu} e_\alpha$<br>
<br>

Written out per-component:<br>
$\nabla_\mu \left[\begin{matrix}
	{e_1}^\hat{1} & ... & {e_n}^\hat{1} \\
	\vdots & & \vdots \\
	{e_1}^\hat{n} & ... & {e_n}^\hat{n}
\end{matrix}\right] = \left[\begin{matrix}
	{e_1}^\hat{1} & ... & {e_n}^\hat{1} \\
	\vdots & & \vdots \\
	{e_1}^\hat{n} & ... & {e_n}^\hat{n}
\end{matrix}\right] \left[\begin{matrix}
	{\Gamma^1}_{\mu 1} & ... & {\Gamma^n}_{\mu 1} \\
	\vdots & & \vdots \\
	{\Gamma^1}_{\mu n} & ... & {\Gamma^n}_{\mu n}
\end{matrix}\right]$<br>
<br>

Notice that, because I picked my contravariant components to be distinct per each row (contravariant indexes are represented as column vectors) and covariant components to be distinct per each column (covariant indexes are represented as row vectors), this means the matrix-multiplication of the connection must be on the right hand.  
If you want to multiply the connection matrix on the left hand side of the basis matrix then you must use distinct contravariant components per each column (contravariant indexes are represented as row vectors) and distinct covariant compoments per each row (covariant indexes are represented as column vectors).<br>
<br>

Since we are differentiating the Cartesian components which are constant at all points, lets exchange the covariant derivative with a partial derivative:<br>
$\frac{\partial}{\partial x^\mu} [{e_\nu}^I](x) = [{e_\alpha}^I](x) \cdot [{\Gamma^\alpha}_{\mu\nu}](x) $<br>
<br>

Abuse notation a bit:<br>
$\int [{e^\alpha}_I](x) \cdot d [{e_\nu}^I](x) = \int_{x'^\mu = x_L^\mu}^{x'^\mu = x_R^\mu} [{\Gamma^\alpha}_{\mu\nu}](x') dx'$<br>
<br>

Integrate the linear dynamic system and hide the Cartesian basis index to find:<br>
$[e_\nu](x^\mu_R) = [e_\alpha](x^\mu_L) \cdot exp\left( \int_{x'^\mu = x_L^\mu}^{x'^\mu = x_R^\mu} [{\Gamma^\alpha}_{\mu\nu}](x'^\mu) dx'^\mu \right)$<br>
<br>

...where I make some liberal exchanges of when I want $x^\mu_L$ to indicate the scalar value of the left-bounds of the $\mu$ index of the x coordinate, and when I want it to mean the x coordinate with the $\mu$ component exchanged with the scalar value $x^\mu_L$.<br>
<br>

So that's how you transport a basis from one point $x^\mu_L$ to another point $x^\mu_R$ along a single changing coordinate $x^\mu$.<br>
How about if you want to transport it along an arbitrary coordinate path on the manifold?<br>
For that, for now, I will only consider separate movement along individual coordinate lines:<br>
Let's assume x and y are points on our manifold.<br>
<br>

$[e_\nu](y) = [e_{\alpha_1}](x) 
	\cdot exp\left( \int_{z^1 = x^1}^{z^1 = y^1} [{\Gamma^{\alpha_1}}_{1 {\alpha_2}}](z^1, x^2, ..., x^n) dz^1 \right)
	\cdot ...
	\cdot exp\left( \int_{z^\mu = x^\mu}^{z^\mu = y^\mu} [{\Gamma^{\alpha_\mu}}_{\mu {\alpha_{\mu+1}}}](y^1, ..., y^{\mu-1}, z^\mu, x^{\mu+1}, ..., x^n) dz^\mu \right)
	\cdot ...
	\cdot exp\left( \int_{z^n = x^n}^{z^n = y^n} [{\Gamma^{\alpha_n}}_{n \nu}](y^1, ..., y^{n-1}, z^n) dz^n \right)
$<br>
<br>
Notice that I am transforming the original basis across each connection's coordinate dimension individually, and after I perform each transform I exchange the source coordinate in the integral with the destination component.<br>
Think of it like traversing the edges of a n-hypercube to get from (0,0,...,0) to (1,1,...,1).<br>
<br>

Maybe with some more notation abuse I could write that as:<br>
$[e_\nu](y) = [e_\alpha](x) 
	\cdot \underset{\mu=1}{\overset{n}{\Pi}} exp\left( \int_{z^\mu = x^\mu}^{z^\mu = y^\mu} [{\Gamma^\alpha}_{\mu \nu}](y^{1..\mu-1} | z^\mu | x^{\mu+1..n}) dz^\mu \right)$<br>
<br>
<br>

I should come up with some notation to denote the transform applied for dragging the basis from point x to point y.  How about:<br>
${[\parallel_{x_L^\mu}^{x_R^\mu}]^\alpha}_\nu
= exp\left( \int_{x'^\mu = x_L^\mu}^{x'^\mu = x_R^\mu} [{\Gamma^\alpha}_{\mu\nu}](x'^\mu) dx'^\mu \right)$<br>
<br>

Mind you the result of $\parallel_a^b$ is a matrix, and the $\alpha$ and $\nu$ in this case are the matrix indexes.<br>
<br>

The above transformation from $e_\nu(x_L^\mu)$ to $e_\nu(y^\mu_R)$ would look like:<br>
$e(x_R^\mu) = e(x_L^\mu) \cdot \parallel_{x_L^\mu}^{x_R^\mu}$<br>
<br>

And the above transformation from $e_\nu(x)$ to $e_\nu(y)$, going component-by-component, would look like:<br>
$e(y) = e(x) 
	\cdot \parallel_{x^1}^{y^1} (x^1 ,..., x^n) 
	\cdot ... 
	\cdot \parallel_{x^k}^{y^k} (y^1, ..., y^{k-1}, x^k, ..., x^n)
	\cdot ... 
	\cdot \parallel_{x^n}^{y^n} (y^1, ..., y^{n-1}, x^n)$<br>
<br>
That can just be written as $e(x) \underset{k=1}{\overset{n}{\Pi}} \parallel_{x^k}^{y^k}$ if we don't mind hiding the chart coordinates, which seem to be an important detail.<br>
<br>
<br>

Notice that order of application of the exp-integrals-of-connections is important.  If you were to swap orders then you would have to deal with the $[\nabla_\mu, \nabla_\mu] e_\alpha = e_\beta {R^\beta}_{\alpha\mu\nu}$ Riemann curvature tensor.  Think about exactly how later.<br>
<br>
Does this mean that, by the Bianchi identity of the Riemann curvature tensor, that exchanging an index may produce a result whose difference with the original is proportional to the Riemann curvature tensor but cycling all indexes will result in the same integral?  I'll think about that later.<br>
<br>
<br>

Alright, what if you do want to just cut across the diagonal of the coordinate space hypercube instead of following the edges around the outside?
I'm guessing that would look like:<br>
<br>
$[e_\nu](y) = [e_\alpha](x) \cdot exp\left( \int_{\lambda = 0}^{\lambda = 1} [{\Gamma^\alpha}_{\mu\nu}](x + \lambda v) v^\mu d\lambda \right)$<br>
<br>
...where $v = y - x$.  I'll call this the generalized linear case.
This would be especially useful in manifolds that do have curvature (commutation?), like quaternions, 
where the exponential map of $e_1$ produces $R_x$ a rotation around the x axis and the exponential of $e_2$ produces $R_y$ a rotation around the y axis, 
and $exp(\theta n^i e_i)$ produces $R_n$ a rotation around axis $n^i$ by angle $\theta$.  
The $R_x$ and $R_y$ transforms do not commute, and both are distinctly different from $R_n$.<br>
<br>

Mind you, changing the chart coordinates in a linear fashion like this does not produce a geodesic transport.
In order to do that, you would (once again) need to make use of the connection.  A double integral of some function of the connection?  
After all, the geodesic equation is a 2nd derivative of the position.  Think on this one more later.
The shorthand representation of our generalized linear parallel transport could be something like: $e(y) = e(x) \cdot \parallel_x^y$<br>
<br>

The notation for the generalized linear parallel transport looks just like the notation for transport around the edges of a hypercube representaiton above, even though these represent two different transformations.
It looks like I need to specify in the notation a way to discern the path of the transport as well.
Maybe just write $\parallel_C$ and specify that C is a curve, and whether the curve is a geodesic, or a line in coordinate chart space, or a piecewise edge traversal from one corner of a hypercube to the other (as I first stated).
So there is your solution to the geodesic case: just calculate the curve, and then substitute that curve into the coordinate evaluating the connection.<br>
<br>
<br>

<hr>
<br>

So now on to vector/tensor components.  I'll start with vector, but you can extrapolate if you want.<br>
$v = v^\mu e_\mu$<br>
<br>

In matrix form:<br>
$v = \left[\begin{matrix}
	e_1 & | & ... & | & e_n
\end{matrix}\right] \left[\begin{matrix}
	v^1 \\ --- \\ \vdots \\ --- \\ v^n
\end{matrix}\right]$<br>
<br>

As a matrix, in our fixed background Cartesian components (since they are independent of our choice of p):<br>
$v = v^\mu {e_\mu}^I e_I = v^I e_I$<br>
<br>

In matrix form:<br>
$v = \left[\begin{matrix}
	e_\hat{1} & | & ... &  | & e_\hat{n}
\end{matrix}\right] \left[\begin{matrix}
	v^\hat{1} \\ --- \\ \vdots \\ --- \\ v^\hat{n}
\end{matrix}\right] = \left[\begin{matrix}
	e_\hat{1} & | & ... &  | & e_\hat{n}
\end{matrix}\right] \left[\begin{matrix}
	{e_1}^\hat{1} & ... & {e_n}^\hat{1} \\
	\vdots & & \vdots \\
	{e_1}^\hat{n} & ... & {e_n}^\hat{n}
\end{matrix}\right] \left[\begin{matrix}
	v^1 \\ --- \\ \vdots \\ --- \\ v^n
\end{matrix}\right]$<br>
<br>

So our background basis components of our vector field are:<br>
<br>
$v^I = {e_\mu}^I v_\mu$<br>
<br>
$\left[\begin{matrix}
	v^\hat{1} \\ --- \\ \vdots \\ --- \\ v^\hat{n}
\end{matrix}\right] = \left[\begin{matrix}
	{e_1}^\hat{1} & ... & {e_n}^\hat{1} \\
	\vdots & & \vdots \\
	{e_1}^\hat{n} & ... & {e_n}^\hat{n}
\end{matrix}\right] \left[\begin{matrix}
	v^1 \\ --- \\ \vdots \\ --- \\ v^n
\end{matrix}\right]$<br>
<br>

So if we have a vector with components relative to the basis at one point x, how do we find the same vector components relative to a basis at y?<br>
$v(x) = v'(y)$<br>
<br>

I'll take advantage of the fact that our Cartesian components are fixed in order to calculate this:<br>
$v^I(x) = v'^I(y)$<br>
<br>

In matrix form:<br>
<br>

$\left[\begin{matrix}
	v^\hat{1} \\ --- \\ \vdots \\ --- \\ v^\hat{n}
\end{matrix}\right]_{(x)} = \left[\begin{matrix}
	v'^\hat{1} \\ --- \\ \vdots \\ --- \\ v'^\hat{n}
\end{matrix}\right]_{(y)}$<br>
<br>

Notice I've equated my hatted quantities, because these are with respect to our fixed background basis $e_I$.<br>
Now to expand it in terms of the coordinate basis components:<br>
<br>

${e_\mu}^I(x) v^\mu(x) = {e_\mu}^I(y) v'^\mu(y)$<br>
<br>

$\left[\begin{matrix}
	{e_1}^\hat{1} & ... & {e_n}^\hat{1} \\
	\vdots & & \vdots \\
	{e_1}^\hat{n} & ... & {e_n}^\hat{n}
\end{matrix}\right]_{(x)} \left[\begin{matrix}
	v^1 \\ --- \\ \vdots \\ --- \\ v^n
\end{matrix}\right]
 = 
\left[\begin{matrix}
	{e_1}^\hat{1} & ... & {e_n}^\hat{1} \\
	\vdots & & \vdots \\
	{e_1}^\hat{n} & ... & {e_n}^\hat{n}
\end{matrix}\right]_{(y)} \left[\begin{matrix}
	v'^1 \\ --- \\ \vdots \\ --- \\ v'^n
\end{matrix}\right]$
<br>

Solve for $v'^\mu$:<br>
${e^\nu}_I(y) {e_\mu}^I(x) v^\mu(x) = {e^\nu}_I(y) {e_\mu}^I(y) v'^\mu(y)$<br>
${e^\nu}_I(y) {e_\mu}^I(x) v^\mu(x) = \delta^\nu_\mu v'^\mu(y)$<br>
${e^\nu}_I(y) {e_\mu}^I(x) v^\mu(x) = v'^\nu(y)$<br>
<br>

i.e.:<br>
$e^{-1}(y) e(x) v(x) = v'(y)$<br>
$(e(y))^{-1} e(x) v(x) = v'(y)$<br>
<br>

i.e.:<br>
$ \left[\begin{matrix}
	v'^1 \\ --- \\ \vdots \\ --- \\ v'^n
\end{matrix}\right]
= \left( \left[\begin{matrix}
	{e_1}^\hat{1} & ... & {e_n}^\hat{1} \\
	\vdots & & \vdots \\
	{e_1}^\hat{n} & ... & {e_n}^\hat{n}
\end{matrix}\right]_{(y)} \right)^{-1}
\left[\begin{matrix}
	{e_1}^\hat{1} & ... & {e_n}^\hat{1} \\
	\vdots & & \vdots \\
	{e_1}^\hat{n} & ... & {e_n}^\hat{n}
\end{matrix}\right]_{(x)} \left[\begin{matrix}
	v^1 \\ --- \\ \vdots \\ --- \\ v^n
\end{matrix}\right]$<br>
<br>

That looks fine, except we are still using ${e_\mu}^I$, which has components in our Cartesian basis.<br>
How do we represent everything only in our curvilinear chart coordinates?<br>
<br>

Let's represent our $e(y)$ as a parallel transport of the basis from $e(x)$ to $e(y)$.<br>
$e(y) = e(x) \cdot \parallel_x^y$<br>
Substitute to find:<br>
$(e(x) \cdot \parallel_x^y)^{-1} e(x) v(x) = v'(y)$<br>
$(\parallel_x^y)^{-1} e^{-1}(x) e(x) v(x) = v'(y)$<br>
$\parallel_y^x \cdot v(x) = v'(y)$<br>
<br>

Tada! Now we have a representation of the parallel transport that depends only on connections, not on the Cartesian basis.<br>
<br>

Notice that this depends on the identity: $(\parallel_x^y)^{-1} = \parallel_y^x$<br>
That can be proven quickly with:<br>
$\parallel_y^x$<br>
$= exp(\int_y^x \Gamma_v d\lambda)$<br>
$= exp(-\int_x^y \Gamma_v d\lambda)$<br>
$= exp(\int_x^y \Gamma_v d\lambda)^{-1}$<br>
$= (\parallel_x^y)^{-1}$<br>
<br>

The interesting thing is that the resulting components after parallel transport from x to y is $v(x)$ left-multiplied with the transport from y to x.<br>
This is the same as the inverse of the parallel-transport transform from x to y, and that itself will be left-multiplied with the original parallel-transport transform from x to y, and that left-multiplied with $e(x)$, in order to deduce that $v' = v$:<br>
$e(x) \cdot \parallel_x^y \cdot (\parallel_x^y)^{-1} \cdot v(x) = e(x) \cdot \parallel_x^y \cdot v'(y)$<br>
$e(x) v(x) = e(y) v'(y)$<br>
$v = v'$<br>
<br>
<br>

<hr>
<br>

Integrating a vector field in Cartesian components, vs integrating a vector field wrt a fixed frame:<br>
<br>

I hope you can already see that integrating a vector with curvilinear components will not produce the same as integrating a vector field with Cartesian components.
Just do any of many quick examples to prove this.  Integrate the $e_r$ vector through a curve in polar coordinates and compare it with some Cartesian integral results.<br>
<br>

What do we do when we integrate a vector field?  We start with $\int v(x) dV$.  
Then we break it down into Cartesian components $\int e_I(x) v^I(x) dV$.  
Then we tell ourselves that the Cartesian basis is constant throughout the integral - and not dependent on any integrated variables - and therefore we can factor it out: 
$\int e_I(x) v^I(x) dV = \int e_I v^I(x) dV = e_I \int v^I(x) dV$.
And that is your typical vector field integral recipe.<br>
<br>

But what if we're dealing with a non-Cartesian coordinate basis?  We break down our vector field: $\int e_\mu(x) v^\mu(x) dV$.  
But here the typical error performed is that the basis is factored out, as it is done in the Cartesian basis case.  
But the basis is dependent on the coordinate, so you can't do that: $\int e_\mu(x) v^\mu(x) dV \ne e_\mu(x) \int v^\mu(x) dV$.
This statement doesn't even make sense.  
You just factored out one of your integrating variables in the parameter of the basis.  
You can only do this if the basis doesn't depend on the integrating parameter, and while this is true for Cartesian, it isn't for polar or just about any other choice.<br>
<br>

So what's the remedy?
For some reason everyone thinks the remedy is to multiply by the Jacobian.  Go ahead and try to do that and see how far it gets you.  No, what's the real remedy?
Pick a fixed tangent space to evalute the integral in, and transport each vector inside the integral onto that tangent space.  Then the basis isn't dependent on the integral.
(Will the integral result be skewed if the fixed basis is not an orthonormal basis, such as in polar picking a basis where $r \ne 1$?)
<br>

So using our equation above on transporting vector components we get:<br>
<br>

$\int e_\mu(x) v^\mu(x) dV$<br>
$= \int e_\mu(x) {(\parallel_x^{x_0})^\mu}_\alpha {(\parallel_{x_0}^x)^\alpha}_\nu v^\nu(x) dV$<br>
$= \int e_\alpha(x_0) {(\parallel_{x_0}^x)^\alpha}_\mu v^\mu(x) dV$<br>
...and now the basis is independent of the integral, so we can factor it out.<br>
$= e_\alpha(x_0) \int {(\parallel_{x_0}^x)^\alpha}_\mu v^\mu(x) dV$<br>
<br>
Tada! Now we have an equation for our integral that uses no Cartesian basis components whatsoever.<br>
<br>
So from there if we want we can convert the equation into our Cartesian basis and viola:<br>
$= e_I \cdot {e_\alpha}^I(x_0) \int {(\parallel_{x_0}^x)^\alpha}_\mu v^\mu(x) dV$<br>
Hopefully the example cases below agree with this, especially in the event that our basis isn't normalized, where the integrated fixed-basis vector components might be some factor of the integrated Cartesian-basis vector components.
I guess in that case the ratio would still be constant (since it is dependent on $x_0$, which is independent of the integral), and since integration is linear then we should be fine.<br>
<br>

<br>


<hr>
<br>

Now for a specific example: Polar coordinates:<br>
<br>

chart:<br>
$u^I = \left[\begin{matrix}
	r cos\phi \\
	r sin\phi
\end{matrix}\right]$<br>
<br>

${e_r}^I = {u^I}_{,r} = \left[\begin{matrix}
	{e_r}^x \\
	{e_r}^y
\end{matrix}\right] = \left[\begin{matrix}
	cos\phi \\
	sin\phi
\end{matrix}\right]$<br>
<br>

${e_\phi}^I = {u^I}_{,\phi} = \left[\begin{matrix}
	{e_\phi}^x \\
	{e_\phi}^y
\end{matrix}\right] = \left[\begin{matrix}
	-r sin\phi \\
	r cos\phi
\end{matrix}\right]$<br>
<br>

${e_\mu}^I = {u^I}_{,\mu} = \left[\begin{matrix}
	{e_r}^x	& {e_\phi}^x \\
	{e_r}^y	& {e_\phi}^y
\end{matrix}\right] = \left[\begin{matrix}
	cos\phi & -r sin\phi \\
	sin\phi & r cos\phi
\end{matrix}\right]$<br>
<br>

It might be later convenient to think of this as a product of two linear operations:<br>
<br>
$[{e_\mu}^I] = \left[\begin{matrix}
	cos\phi & -sin\phi \\
	sin\phi & cos\phi
\end{matrix}\right] \left[\begin{matrix}
	1 & 0 \\
	0 & r
\end{matrix}\right] = R(\phi) \cdot S(1,r)$<br>
<br>

...where $S(a,b) = \left[\begin{matrix} a & 0 \\ 0 & b \end{matrix}\right]$ is a scale matrix, and $R(\phi)$ is defined as whatever above is left, which happens to look like a rotation matrix.<br>
<br>
<br>

Now for the connections:<br>
<br>

${\Gamma^r}_{\phi\phi} = -r$<br>
${\Gamma^\phi}_{r\phi} = {\Gamma^\phi}_{\phi r} = \frac{1}{r}$<br>
<br>

$[\Gamma_\mu] = \left[\begin{matrix}
	{\Gamma^r}_{\mu r} & {\Gamma^\phi}_{\mu r} \\
	{\Gamma^r}_{\mu\phi} & {\Gamma^\phi}_{\mu\phi}
\end{matrix}\right]$<br>
<br>

$[\Gamma_r] = \left[\begin{matrix}
	{\Gamma^r}_{r r} & {\Gamma^\phi}_{r r} \\
	{\Gamma^r}_{r\phi} & {\Gamma^\phi}_{r\phi}
\end{matrix}\right] = \left[\begin{matrix}
	0 & 0 \\
	0 & \frac{1}{r}
\end{matrix}\right]$<br>
<br>


$[\Gamma_\phi] = \left[\begin{matrix}
	{\Gamma^r}_{\phi r} & {\Gamma^\phi}_{\phi r} \\
	{\Gamma^r}_{\phi\phi} & {\Gamma^\phi}_{\phi\phi}
\end{matrix}\right] = \left[\begin{matrix}
	0 & -r \\
	\frac{1}{r} & 0
\end{matrix}\right]$<br>
<br>

And now for the exponentials of the integrals of the connections:<br>
<br>

$\parallel_{r_L}^{r_R} = exp(\int_{r_L}^{r_R} [\Gamma_r] dr)$<br>
$= exp\left( \int_{r_L}^{r_R} \left[\begin{matrix}
	0 & 0 \\
	0 & \frac{1}{r}
\end{matrix}\right]  dr \right)$<br>
$= exp\left( \left[\begin{matrix}
	0 & 0 \\
	0 & ln(r)
\end{matrix}\right]|_{r_L}^{r_R} \right)$<br>
$= exp\left( \left[\begin{matrix}
	0 & 0 \\
	0 & ln(r_R) - ln(r_L)
\end{matrix}\right] \right)$<br>
$= exp\left( \left[\begin{matrix}
	0 & 0 \\
	0 & ln(\frac{r_R}{r_L})
\end{matrix}\right] \right)$<br>
Since we now have a diagonal matrix, we can assert that the exponent of the matrix is a matrix of the exponent of the diagonals:<br>
$= \left[\begin{matrix}
	exp(0) & 0 \\
	0 & exp(ln(\frac{r_R}{r_L}))
\end{matrix}\right]$<br>
$= \left[\begin{matrix}
	1 & 0 \\
	0 & \frac{r_R}{r_L}
\end{matrix}\right]$<br>
$= S(1, \frac{r_R}{r_L})$<br>
<br>

$\parallel_{\phi_L}^{\phi_R} = exp(\int_{\phi_L}^{\phi_R} [\Gamma_\phi] d\phi)$<br>
$= exp\left(\int_{\phi_L}^{\phi_R} \left[\begin{matrix}
	0 & -r \\
	\frac{1}{r} & 0
\end{matrix}\right] d\phi \right)$<br>
$= exp\left( \left[\begin{matrix}
	0 & -r (\phi_R - \phi_L) \\
	\frac{1}{r} (\phi_R - \phi_L) & 0
\end{matrix}\right] \right)$<br>
Now we eigen-decompose the matrix before applying the exponential to the eigenvalues:<br> 
$= exp\left( 
\left[\begin{matrix} 
	1 & -i r \\ 
	1 & i r
\end{matrix}\right] 
\left[\begin{matrix} 
	-i (\phi_R - \phi_L) & 0 \\ 
	0 & i (\phi_R - \phi_L)
\end{matrix}\right] 
\left[\begin{matrix} 
	\frac{1}{2} & \frac{1}{2} \\ 
	\frac{i}{2 r} & -\frac{i}{2r}
\end{matrix}\right]
\right)$<br>
$= \left[\begin{matrix} 
	1 & -i r \\ 
	1 & i r
\end{matrix}\right] 
exp\left( \left[\begin{matrix} 
	-i (\phi_R - \phi_L) & 0 \\ 
	0 & i (\phi_R - \phi_L)
\end{matrix}\right] \right)
\left[\begin{matrix} 
	\frac{1}{2} & \frac{1}{2} \\ 
	\frac{i}{2 r} & -\frac{i}{2r}
\end{matrix}\right]
$<br>
Now we use the fact that the exponent of a diagonal matrix is the matrix of the exponent of the individual diagonal elements:<br>
$= \left[\begin{matrix} 
	1 & -i r \\ 
	1 & i r
\end{matrix}\right] 
\left[\begin{matrix} 
	exp(-i (\phi_R - \phi_L)) & 0 \\ 
	0 & exp(i (\phi_R - \phi_L))
\end{matrix}\right]
\left[\begin{matrix} 
	\frac{1}{2} & \frac{1}{2} \\ 
	\frac{i}{2 r} & -\frac{i}{2r}
\end{matrix}\right]
$<br>

$= \left[\begin{matrix}
	cos(\phi_R - \phi_L) & -r sin(\phi_R - \phi_L) \\
	\frac{1}{r} sin(\phi_R - \phi_L) & cos(\phi_R - \phi_L)
\end{matrix}\right]$<br> 
$= \left[\begin{matrix}
	1 & 0 \\
	0 & \frac{1}{r}
\end{matrix}\right] \left[\begin{matrix}
	cos(\phi_R - \phi_L) & -sin(\phi_R - \phi_L) \\
	sin(\phi_R - \phi_L) & cos(\phi_R - \phi_L)
\end{matrix}\right] 
\left[\begin{matrix}
	1 & 0 \\
	0 & r 
\end{matrix}\right]$<br>
$= S(1,\frac{1}{r}) R(\phi_R - \phi_L) S(1, r)$<br>
<br>
<br>

It just so happens that 
$\parallel_{r_L}^{r_R} \cdot \parallel_{\phi_L}^{\phi_R} = \parallel_{\phi_L}^{\phi_R} \cdot \parallel_{r_L}^{r_R}$.  
Maybe because ${R^\alpha}_{\beta\mu\nu} = 0$, but that is still just speculation.<br>
And the applying the parallel transport in different orderings gives us:<br> 
<br>

$\parallel_{r_L}^{r_R} \cdot \parallel_{\phi_L}^{\phi_R}$<br>
$= S(1, \frac{r_R}{r_L}) S(1,\frac{1}{r_R}) R(\phi_R - \phi_L) S(1, r_R)$<br>
$= S(1,\frac{1}{r_L}) R(\phi_R - \phi_L) S(1, r_R)$<br>
<br>
$\parallel_{\phi_L}^{\phi_R} \cdot \parallel_{r_L}^{r_R}$<br>
$= S(1,\frac{1}{r_L}) R(\phi_R - \phi_L) S(1, r_L) S(1, \frac{r_R}{r_L})$<br>
$= S(1,\frac{1}{r_L}) R(\phi_R - \phi_L) S(1, r_R)$<br>
<br>

So in both cases we end up with the general linear pararallel transport from coordinate chart domain point $x_L$ to $x_R$ as:<br>
<br>
$\parallel_{x_L}^{x_R} = S(1, \frac{1}{r_L}) \cdot R(\phi_R - \phi_L) \cdot S(1, r_R)
= \left[\begin{matrix}
	cos(\phi_R - \phi_L) & -r_R sin(\phi_R - \phi_L) \\
	\frac{1}{r_L} sin(\phi_R - \phi_L) & \frac{r_R}{r_L} cos(\phi_R - \phi_L)
\end{matrix}\right]$<br> 
<br>
<br>

What happens when we parallel transport our basis from $x_L$ to $x_R$?<br>
<br>
$e(x_L) \cdot \parallel_{x_L}^{x^R} = 
\left(
	R(\phi_L) \cdot S(1, r_L) 
\right) \cdot \left(
	S(1, \frac{1}{r_L}) \cdot R(\phi_R - \phi_L) \cdot S(1, r_R)
\right)$<br>
$= R(\phi_L) \cdot R(\phi_R - \phi_L) \cdot S(1, r_R)$<br>
And since our rotation axis are aligned, we can add our rotation angles together:<br>
$= R(\phi_R) \cdot S(1, r_R)$<br>
<br>

Tada!  We started with $e(x_L)$, we right-applied our parallel-transport transformation, and we ended up at $e(x_R)$.<br>
<br>

TODO verify that this works with my generalized linear case equation above, and not just per-coordinate separately.  
It probably will but only because polar coordinates have no Riemann curvature.  
Also, try it with the geodesic case.<br> 
<br>

Now do the same thing, except give it some vector components:<br>
$e_\mu(x_R) v^\mu(x_R)$<br>
$= e_\alpha (x_L) \cdot {(\parallel_{x_L}^{x_R})^\alpha}_\mu v^\mu(x_R)$<br>
Substitute for polar:<br>
$= e (x_L) \cdot ( S(1,\frac{1}{r_L}) R(\phi_R - \phi_L) S(1, r_R) ) v(x_R)$<br>
$= e (x_L) v'(x_L)$<br>
So $v'(x_L) = S(1,\frac{1}{r_L}) R(\phi_R - \phi_L) S(1, r_R) v(x_R)$<br>
It seems a bit counter-intuitive that we start with $v^\mu(x_R)$ and we scale its $v^\phi(x_R)$ component up by $r_R$, but don't forget $v^\phi$ is paired with $e_\phi$, and $|e_\phi| = r_R$, so $|v^\phi| = \frac{1}{r_R}$, so what we are doing is removing that influence from the basis at point $x_R$ before we go on to move things to $x_L$.<br>
<br>

Speaking of vector field, let's get a better description of what the curvilinear vector components are in terms of the Cartesian vector components:<br>
$e_I v^I = e_\mu v^\mu$<br>
$\left[\begin{matrix} e_x & e_y \end{matrix}\right] \left[\begin{matrix} v^x \\ v^y \end{matrix}\right]
= \left[\begin{matrix} e_r & e_\phi \end{matrix}\right] \left[\begin{matrix} v^r \\ v^\phi \end{matrix}\right]$<br>
$\left[\begin{matrix} e_x & e_y \end{matrix}\right] \left[\begin{matrix} v^x \\ v^y \end{matrix}\right]
= \left[\begin{matrix} e_x & e_y \end{matrix}\right] \left[\begin{matrix} cos(\phi) & -r sin(\phi) \\ sin(\phi) & r cos(\phi) \end{matrix}\right] \left[\begin{matrix} v^r \\ v^\phi \end{matrix}\right]$<br>
$\left[\begin{matrix} v^x \\ v^y \end{matrix}\right] = \left[\begin{matrix} cos(\phi) & -r sin(\phi) \\ sin(\phi) & r cos(\phi) \end{matrix}\right] \left[\begin{matrix} v^r \\ v^\phi \end{matrix}\right]$<br>
$\left[\begin{matrix} cos(\phi) & sin(\phi) \\ -\frac{1}{r}sin(\phi) & \frac{1}{r} cos(\phi) \end{matrix}\right] \left[\begin{matrix} v^x \\ v^y \end{matrix}\right] = \left[\begin{matrix} v^r \\ v^\phi \end{matrix}\right]$<br>
<br>

Now for our vector field integral:<br>
First in Cartesian:<br>
$\int v(x) dV$<br>
$= \int e_I v^I(x) dV$<br>
$= \left[\begin{matrix} e_x & e_y \end{matrix}\right] 
	\int \left[\begin{matrix} v^x(x) \\ v^y(x) \end{matrix}\right] dV$<br>
$= \left[\begin{matrix} e_x & e_y \end{matrix}\right] 
	\left[\begin{matrix} \int v^x(x) dV \\ \int v^y(x) dV \end{matrix}\right]$<br>
<br>

Next in a fixed reference basis:<br>
$\int v(x) dV$<br>
$= e_I {e_\alpha}^I(x_0) \int {(\parallel_{x_0}^x)^\alpha}_\mu v^\mu(x) dV$<br>
$= \left[\begin{matrix}
	e_x & e_y
\end{matrix}\right]
\left[\begin{matrix}
	cos(\phi_0) & -r_0 sin(\phi_0) \\
	sin(\phi_0) & r_0 cos(\phi_0)
\end{matrix}\right]
\int 
\left[\begin{matrix}
	cos(\phi - \phi_0) & -r sin(\phi - \phi_0) \\
	\frac{1}{r_0} sin(\phi - \phi_0) & \frac{r}{r_0} cos(\phi - \phi_0)
\end{matrix}\right]
\left[\begin{matrix}
	v^r(x) \\
	v^\phi(x)
\end{matrix}\right]
dV$<br>
$= \left[\begin{matrix}
	e_x & e_y
\end{matrix}\right]
\left[\begin{matrix}
	cos(\phi_0) & -r_0 sin(\phi_0) \\
	sin(\phi_0) & r_0 cos(\phi_0)
\end{matrix}\right]
\int 
\left[\begin{matrix}
	v^r(x) cos(\phi - \phi_0) - v^\phi(x) r sin(\phi - \phi_0) \\
	v^r(x) \frac{1}{r_0} sin(\phi - \phi_0) + v^\phi(x) \frac{r}{r_0} cos(\phi - \phi_0)
\end{matrix}\right]
r dr d\phi$<br>
$= \left[\begin{matrix}
	e_r & e_\phi
\end{matrix}\right]|_{(x_0)}
\int 
\left[\begin{matrix}
	r v^r(x) (cos(\phi) cos(\phi_0) - sin(\phi) sin(\phi_0))
	- r^2 v^\phi(x) (sin(\phi) cos(\phi_0) - cos(\phi) sin(\phi_0)) \\
	
	\frac{r}{r_0} (
		v^r(x) (sin(\phi) cos(\phi_0) - cos(\phi) sin(\phi_0)) 
		+ r v^\phi(x) (cos(\phi) cos(\phi_0) - sin(\phi) sin(\phi_0))
	)
\end{matrix}\right]
dr d\phi$<br>
$= \left[\begin{matrix}
	e_r & e_\phi
\end{matrix}\right]|_{(x_0)}
\left[\begin{matrix}
	cos(\phi_0) \left(
		 \int_\phi \left( cos(\phi) \int_r \left( r v^r(x) \right) dr \right) d\phi
		- \int_\phi \left( sin(\phi) \int_r \left( r^2 v^\phi(x) \right) dr \right) d\phi
	\right)
	
	+ sin(\phi_0) \left(
		\int_\phi \left( cos(\phi) \int_r \left( r^2 v^\phi(x) \right) dr \right) d\phi
		- \int_\phi \left( sin(\phi) \int_r \left( r v^r(x) \right) dr \right) d\phi
	\right)
	\\

	\frac{1}{r_0} cos(\phi_0) \left(
		\int_\phi \left( sin(\phi) \int_r \left( r v^r(x) \right) dr \right) d\phi
		+ \int_\phi \left( cos(\phi) \int_r \left( r^2 v^\phi(x) \right) dr \right) d\phi
	\right)
	
	- \frac{1}{r_0} sin(\phi_0) \left(
		\int_\phi \left( cos(\phi) \int_r \left( r v^r(x) \right) dr \right) d\phi
		+ \int_\phi \left( sin(\phi) \int_r \left( r^2 v^\phi(x) \right) dr \right) d\phi
	\right)
\end{matrix}\right]$<br>
...and keep going...<br>
<br>
<br>


	</body>
</html>
