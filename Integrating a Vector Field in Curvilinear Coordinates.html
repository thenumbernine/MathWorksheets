<!doctype html>
<html>
	<head>
        <meta charset='utf-8'>
		<script type='text/javascript' src='tryToFindMathJax.js'></script>
		<title>Integrating a Vector Field in Curvilinear Coordinates</title>
	</head>
	<body>

by Christopher Moore<br>
<br>

Start with a manifold $\mathcal{M}$ with dimension n.<br>
Define a point on the manifold: $p \in \mathcal{M}$<br>
Define the tangent space at a point on the manifold: $T_p(\mathcal{M})$<br>
Let u be a coordinate chart of the manifold, and let $x = \{x^\mu\}$ be a tuple of coordinates in the chart's domain, with $\mu$ spanning 1 to n (or 0 to n-1 if you would like), so $u(x) = p$.<br>
Define a basis $\{e_\mu(x)\}$ that spans $T_p(\mathcal{M})$.<br>
<br>

Define the basis represented in global Cartesian components to be: $e_\mu(x) = {e_\mu}^I(x) e_I$, for Cartesian basis $e_I$<Br>
$e_I$ has to be constant wrt all x because we're going to be integrating them across a region.  Don't worry, the transform ${e_\mu}^I$ will be used in the formalities but not in the results.<br>
<br>

Let's write that out per-component.  I'll use hats to denote the Cartesian basis indexes:<br>
${e_\mu}^I = \downarrow I \overset{\rightarrow \mu}{\left[\begin{matrix}
	{e_1}^\hat{1} & ... & {e_n}^\hat{1} \\
	\vdots & & \vdots \\
	{e_1}^\hat{n} & ... & {e_n}^\hat{n}
\end{matrix}\right]} = \overset{\rightarrow \mu}{\left[\begin{matrix}
	e_1 & | & ... &  | & e_n
\end{matrix}\right]} = \downarrow I \left[\begin{matrix}
	e^\hat{1} \\ --- \\ \vdots \\ --- \\ e^\hat{n}
\end{matrix}\right]
$<br>
<br>

Notice I picked my matrix representation of my ${e_\mu}^I$ tensor components such that contravariant elements are in separate rows (so vectors are columns) and covariant elements are in separate columns (so one-forms are row).<br>
<br>

While we're here, let's define the dual basis / inverse transform $e^\mu$, such that $e_\mu e^\nu = \delta^\nu_\mu$.<br>
And lets define its Cartesian background components $e^\mu = {e^\mu}_I e^I$ where $e^I = e_I$ since the Cartesian metric is identity.<br>
<br>

${e^\mu}_I = \downarrow \mu \overset{\rightarrow I}{\left[\begin{matrix}
	{e^1}_\hat{1} & ... & {e^1}_\hat{n} \\
	\vdots & & \vdots \\
	{e^n}_\hat{1} & ... & {e^n}_\hat{n}
\end{matrix}\right]}$<br>
<br>

Notice that, in matrix form, $[{e^\mu}_I] = [{e_\mu}^I]^{-1}$<br>
<br>

The discernment between the basis and its background when written in index notation - by convention - is whether the first index, stated in curvilinear coordinates, is covariant or contravariant.  Covariant $e_\mu$ represents the vector basis while contravariant $e^\mu$ represents the dual basis.
This is probably a bad idea, and I should probably use some other symbol to denote the dual basis in index notation, maybe even $(e^{-1})^\mu$<br>
<br>

I'm going to be using $[A_{\mu\nu}]$ to denote a matrix representation of a tensor.
Sometimes I'll still keep the summation indexes as a reminder of contra/co-variant representation of the tensor component quantities, 
other times I will remove the summation indexes if the contra-/co-variance of the tensor should be deducable.
If the brackets do remain around a tensor, even if it is in index notation, that is just to serve as a reminder (to me) that this is a matrix value.<br>
<br>

If I ever drop all indexes from $[e_\mu]$, I will shorthand denote $e = [{e_\mu}^I]$ and $e^{-1} = [{e^\mu}_I]$.<br>
<br>

Arbitrary (p q) tensor:<br>
$A = {A^{\mu_1 ... \mu_p}}_{\nu_1 ... \nu_q} \underset{k=1}{\overset{p}{\otimes}} e_{\mu_k} \underset{k=1}{\overset{q}{\otimes}} e^{\nu_k} $<br>
Vector field:<br>
$v = v^\mu e_\mu$<br>
<br>

Connection and covariant derivative:<br>
$\nabla_\mu e_\nu = {\Gamma^\alpha}_{\mu\nu} e_\alpha$<br>
<br>

Written out per-component:<br>
$\nabla_\mu \left[\begin{matrix}
	{e_1}^\hat{1} & ... & {e_n}^\hat{1} \\
	\vdots & & \vdots \\
	{e_1}^\hat{n} & ... & {e_n}^\hat{n}
\end{matrix}\right] = \left[\begin{matrix}
	{e_1}^\hat{1} & ... & {e_n}^\hat{1} \\
	\vdots & & \vdots \\
	{e_1}^\hat{n} & ... & {e_n}^\hat{n}
\end{matrix}\right] \left[\begin{matrix}
	{\Gamma^1}_{\mu 1} & ... & {\Gamma^n}_{\mu 1} \\
	\vdots & & \vdots \\
	{\Gamma^1}_{\mu n} & ... & {\Gamma^n}_{\mu n}
\end{matrix}\right]$<br>
<br>

Notice that, because I picked my contravariant components to be per-row and covariant to be per-column, this means the matrix-multiplication of the connection must be on the right hand.  If you want to multilpy the connection matrix on the left hand then you must use distinct contravariant per-column (row vectors) and distinct covariant elements per-row (column vectors).<br>
<br>

Since we are differentiating the cartesian components which are constant at all points, lets exchange the covariant derivative with a partial derivative:<br>
$\frac{\partial}{\partial x^\mu} [{e_\nu}^I](x) = [{e_\alpha}^I](x) \cdot [{\Gamma^\alpha}_{\mu\nu}](x) $<br>
<br>

Abuse notation a bit:<br>
$\int [{e^\alpha}_I](x) \cdot d [{e_\nu}^I](x) = \int_{x'^\mu = x_L^\mu}^{x'^\mu = x_R^\mu} [{\Gamma^\alpha}_{\mu\nu}](x') dx'$<br>
<br>

Integrate the linear dynamic system and hide the Cartesian basis index to find:<br>
$[e_\nu](x^\mu_R) = [e_\alpha](x^\mu_L) \cdot exp\left( \int_{x'^\mu = x_L^\mu}^{x'^\mu = x_R^\mu} [{\Gamma^\alpha}_{\mu\nu}](x'^\mu) dx'^\mu \right)$<br>
<br>

...where I make some liberal exchanges of when I want $x^\mu_L$ to indicate the scalar value of the left-bounds of the $\mu$ index of the x coordinate, and when I want it to mean the x coordinate with the $\mu$ component exchanged with the scalar value $x^\mu_L$.<br>
<br>

So that's how you transport a basis from one point $x^\mu_L$ to another point $x^\mu_R$ along a single changing coordinate $x^\mu$.<br>
How about if you want to transport it along an arbitrary coordinate path on the manifold?<br>
For that, for now, I will only consider separate movement along individual coordinate lines:<br>
Let's assume x and y are points on our manifold.<br>
<br>

$[e_\nu](y) = [e_{\alpha_1}](x) 
	\cdot exp\left( \int_{z^1 = x^1}^{z^1 = y^1} [{\Gamma^{\alpha_1}}_{1 {\alpha_2}}](z^1, x^2, ..., x^n) dz^1 \right)
	\cdot ...
	\cdot exp\left( \int_{z^\mu = x^\mu}^{z^\mu = y^\mu} [{\Gamma^{\alpha_\mu}}_{\mu {\alpha_{\mu+1}}}](y^1, ..., y^{\mu-1}, z^\mu, x^{\mu+1}, ..., x^n) dz^\mu \right)
	\cdot ...
	\cdot exp\left( \int_{z^n = x^n}^{z^n = y^n} [{\Gamma^{\alpha_n}}_{n \nu}](y^1, ..., y^{n-1}, z^n) dz^n \right)
$<br>
<br>
Notice that I am transforming the original basis across each connection's coordinate dimension individually, and after I perform each transform I exchange the source coordinate in the integral with the destination component.<br>
Think of it like traversing the edges of a n-hypercube to get from (0,0,...,0) to (1,1,...,1).<br>
<br>

Maybe with some more notation abuse I could write that as:<br>
$[e_\nu](y) = [e_\alpha](x) 
	\cdot \underset{\mu=1}{\overset{n}{\Pi}} exp\left( \int_{z^\mu = x^\mu}^{z^\mu = y^\mu} [{\Gamma^\alpha}_{\mu \nu}](y^{1..\mu-1} | z^\mu | x^{\mu+1..n}) dz^\mu \right)$<br>
<br>

I should come up with some notation to denote the transform applied for dragging the basis from point x to point y.  How about:<br>
<br>
${[\parallel_{x_L^\mu}^{x_R^\mu}]^\alpha}_\nu
= exp\left( \int_{x'^\mu = x_L^\mu}^{x'^\mu = x_R^\mu} [{\Gamma^\alpha}_{\mu\nu}](x'^\mu) dx'^\mu \right)$<br>
<br>

Mind you the result of $\parallel_a^b$ is a matrix, and the $\alpha$ and $\nu$ in this case are the matrix indexes.<br>
<br>

The above transformation from $e_\nu(x_L^\mu)$ to $e_\nu(y^\mu_R)$ would look like:<br>
$e(x_R^\mu) = e(x_L^\mu) \cdot \parallel_{x_L^\mu}^{x_R^\mu}$<br>
<br>

And the above transformation from $e_\nu(x)$ to $e_\nu(y)$, going component-by-component, would look like:<br>
$e(y) = e(x) 
	\cdot \parallel_{x^1 }^{ y^1} (x^1 ,..., x^n) 
	\cdot ... 
	\cdot \parallel_{x^k }^{ y^k} (y^1, ..., y^{k-1}, x^k, ..., x^n)
	\cdot ... 
	\cdot \parallel_{x^n }^{ y^n} (y^1, ..., y^{n-1}, x^n)$<br>
<br>
That can just be written as $e(x) \underset{k=1}{\overset{n}{\Pi}} \parallel_{x^k }^{ y^k}$ if we don't mind hiding the chart coordinates, which seem to be an important detail.<br>
<br>

Notice that order of application of the exp-integrals-of-connections is important.  If you were to swap orders then you would have to deal with the $[\nabla_\mu, \nabla_\mu] e_\alpha = e_\beta {R^\beta}_{\alpha\mu\nu}$ Riemann curvature tensor.  Think about exactly how later.<br>
<br>
Does this mean that, by the Bianchi identity of the Riemann curvature tensor, that exchanging an index may produce a result whose difference with the original is proportional to the Riemann curvature tensor but cycling all indexes will result in the same integral?  I'll think about that later.<br>
<br>

Alright, what if you do want to just cut across the diagonal of the cube instead of following the edges around the outside?<br>
I'm guessing that would look like:<br>
$[e_\nu](y) = [e_\alpha](x) \cdot exp\left( \int_{\lambda = 0}^{\lambda = 1} [{\Gamma^\alpha}_{\mu\nu}](x + \lambda v) v^\mu d\lambda \right)$<br>
...where $v = y - x$, but I still need to prove this.
This would be especially useful in manifolds that do have curvature, like quaternions, where the exponential map of $e_1$ and $e_2$ are not commutative, and $exp(\theta n^i e_i)$ produces a rotation around axis $n^i$ by angle $\theta$.<br>
Then again, thinking further, changing the chart coordinates in a linear fashion does not guarantee a geodesic.  In order to do that, you would (once again) need to make use of the connection.  A double integral of some function of the connection?  After all, the geodesic equation is a 2nd derivative of the position.  Think on this one more later.<br>
The shorthand representation of our geodesic parallel transport could be something like: $e(y) = e(x) \cdot \parallel_x^y$<br>
<br>

The geodesic shorthand looks just like the shorthand around the edges of a hypercube representaiton above.  It looks like I need to specify a way to discern the path of the transport as well.<br>
Maybe just write $\parallel_C$ and specify that C is a curve, and whether the curve is a geodesic or whether the curve is a piecewise edge traversal from one corner of a hypercube to the other (as I first stated).<br>
<br>

<hr>
<br>

So now on to vector/tensor components.  I'll start with vector, but you can extrapolate if you want.<br>
$v = v^\mu e_\mu$<br>
<br>

In matrix form:<br>
$v = \left[\begin{matrix}
	e_1 & | & ... & | & e_n
\end{matrix}\right] \left[\begin{matrix}
	v^1 \\ --- \\ \vdots \\ --- \\ v^n
\end{matrix}\right]$<br>
<br>

As a matrix, in our fixed background Cartesian components (since they are independent of our choice of p):<br>
$v = v^\mu {e_\mu}^I e_I = v^I e_I$<br>
<br>

In matrix form:<br>
$v = \left[\begin{matrix}
	e_\hat{1} & | & ... &  | & e_\hat{n}
\end{matrix}\right] \left[\begin{matrix}
	v^\hat{1} \\ --- \\ \vdots \\ --- \\ v^\hat{n}
\end{matrix}\right] = \left[\begin{matrix}
	e_\hat{1} & | & ... &  | & e_\hat{n}
\end{matrix}\right] \left[\begin{matrix}
	{e_1}^\hat{1} & ... & {e_n}^\hat{1} \\
	\vdots & & \vdots \\
	{e_1}^\hat{n} & ... & {e_n}^\hat{n}
\end{matrix}\right] \left[\begin{matrix}
	v^1 \\ --- \\ \vdots \\ --- \\ v^n
\end{matrix}\right]$<br>
<br>

So our background basis components of our vector field are:<br>
<br>
$v^I = {e_\mu}^I v_\mu$<br>
<br>
$\left[\begin{matrix}
	v^\hat{1} \\ --- \\ \vdots \\ --- \\ v^\hat{n}
\end{matrix}\right] = \left[\begin{matrix}
	{e_1}^\hat{1} & ... & {e_n}^\hat{1} \\
	\vdots & & \vdots \\
	{e_1}^\hat{n} & ... & {e_n}^\hat{n}
\end{matrix}\right] \left[\begin{matrix}
	v^1 \\ --- \\ \vdots \\ --- \\ v^n
\end{matrix}\right]$<br>
<br>

So if we have a vector with components relative to the basis at one point x, how do we find the same vector components relative to a basis at y?<br>
$v(x) = v'(y)$<br>
<br>

I'll take advantage of the fact that our Cartesian components are fixed in order to calculate this:<br>
$v^I(x) = v'^I(y)$<br>
<br>

In matrix form:<br>
<br>

$\left[\begin{matrix}
	v^\hat{1} \\ --- \\ \vdots \\ --- \\ v^\hat{n}
\end{matrix}\right]_{(x)} = \left[\begin{matrix}
	v'^\hat{1} \\ --- \\ \vdots \\ --- \\ v'^\hat{n}
\end{matrix}\right]_{(y)}$<br>
<br>

Notice I've equated my hatted quantities, because these are with respect to our fixed background basis $e_I$.<br>
Now to expand it in terms of the coordinate basis components:<br>
<br>

${e_\mu}^I(x) v^\mu(x) = {e_\mu}^I(y) v'^\mu(y)$<br>
<br>

$\left[\begin{matrix}
	{e_1}^\hat{1} & ... & {e_n}^\hat{1} \\
	\vdots & & \vdots \\
	{e_1}^\hat{n} & ... & {e_n}^\hat{n}
\end{matrix}\right]_{(x)} \left[\begin{matrix}
	v^1 \\ --- \\ \vdots \\ --- \\ v^n
\end{matrix}\right]
 = 
\left[\begin{matrix}
	{e_1}^\hat{1} & ... & {e_n}^\hat{1} \\
	\vdots & & \vdots \\
	{e_1}^\hat{n} & ... & {e_n}^\hat{n}
\end{matrix}\right]_{(y)} \left[\begin{matrix}
	v'^1 \\ --- \\ \vdots \\ --- \\ v'^n
\end{matrix}\right]$
<br>

Solve for $v'^\mu$:<br>
${e^\nu}_I(y) {e_\mu}^I(x) v^\mu(x) = {e^\nu}_I(y) {e_\mu}^I(y) v'^\mu(y)$<br>
${e^\nu}_I(y) {e_\mu}^I(x) v^\mu(x) = \delta^\nu_\mu v'^\mu(y)$<br>
${e^\nu}_I(y) {e_\mu}^I(x) v^\mu(x) = v'^\nu(y)$<br>
<br>

i.e.:<br>
$e^{-1}(y) e(x) v(x) = v'(y)$<br>
$(e(y))^{-1} e(x) v(x) = v'(y)$<br>
<br>

i.e.:<br>
$ \left[\begin{matrix}
	v'^1 \\ --- \\ \vdots \\ --- \\ v'^n
\end{matrix}\right]
= \left( \left[\begin{matrix}
	{e_1}^\hat{1} & ... & {e_n}^\hat{1} \\
	\vdots & & \vdots \\
	{e_1}^\hat{n} & ... & {e_n}^\hat{n}
\end{matrix}\right]_{(y)} \right)^{-1}
\left[\begin{matrix}
	{e_1}^\hat{1} & ... & {e_n}^\hat{1} \\
	\vdots & & \vdots \\
	{e_1}^\hat{n} & ... & {e_n}^\hat{n}
\end{matrix}\right]_{(x)} \left[\begin{matrix}
	v^1 \\ --- \\ \vdots \\ --- \\ v^n
\end{matrix}\right]$<br>
<br>

That looks fine, except we are still using ${e_\mu}^I$, which has components in our Cartesian basis.<br>
How do we represent everything only in our curvilinear chart coordinates?<br>
<br>

Let's represent our $e(y)$ as a parallel transport of the basis from $e(x)$ to $e(y)$.<br>
$e(y) = e(x) \cdot \parallel_{x }^{ y}$<br>
Substitute to find:<br>
$(e(x) \cdot \parallel_{x }^{ y})^{-1} e(x) v(x) = v'(y)$<br>
$(\parallel_x^y)^{-1} e^{-1}(x) e(x) v(x) = v'(y)$<br>
$\parallel_y^x \cdot v(x) = v'(y)$<br>
<br>

Tada! Now we have a representation of the parallel transport that depends only on connections, not on the Cartesian basis.<br>
<br>

Notice that this depends on the identity: $(\parallel_x^y)^{-1} = \parallel_y^x$<br>
That can be proven quickly with:<br>
$\parallel_y^x$<br>
$= exp(\int_y^x \Gamma_v d\lambda)$<br>
$= exp(-\int_x^y \Gamma_v d\lambda)$<br>
$= exp(\int_x^y \Gamma_v d\lambda)^{-1}$<br>
$= (\parallel_x^y)^{-1}$<br>
<br>

The interesting thing is that the resulting components after parallel transport from x to y is $v(x)$ left-multiplied with the transport from y to x.<br>
This is the same as the inverse of the parallel-transport transform from x to y, and that itself will be left-multiplied with the original parallel-transport transform from x to y, and that left-multiplied with $e(x)$, in order to deduce that $v' = v$:<br>
$e(x) \cdot \parallel_x^y \cdot (\parallel_x^y)^{-1} \cdot v(x) = e(x) \cdot \parallel_x^y \cdot v'(y)$<br>
$e(x) v(x) = e(y) v'(y)$<br>
$v = v'$<br>
<br>

<hr>
<br>

Now for a specific example: Polar coordinates:<br>
<br>

chart:<br>
$u^I = \left[\begin{matrix}
	r cos\phi \\
	r sin\phi
\end{matrix}\right]$<br>
<br>

${e_r}^I = {u^I}_{,r} = \left[\begin{matrix}
	{e_r}^x \\
	{e_r}^y
\end{matrix}\right] = \left[\begin{matrix}
	cos\phi \\
	sin\phi
\end{matrix}\right]$<br>
<br>

${e_\phi}^I = {u^I}_{,\phi} = \left[\begin{matrix}
	{e_\phi}^x \\
	{e_\phi}^y
\end{matrix}\right] = \left[\begin{matrix}
	-r sin\phi \\
	r cos\phi
\end{matrix}\right]$<br>
<br>

${e_\mu}^I = {u^I}_{,\mu} = \left[\begin{matrix}
	{e_r}^x	& {e_\phi}^x \\
	{e_r}^y	& {e_\phi}^y
\end{matrix}\right] = \left[\begin{matrix}
	cos\phi & -r sin\phi \\
	sin\phi & r cos\phi
\end{matrix}\right]$<br>
<br>

It might be later convenient to think of this as a product of two linear operations:<br>
<br>
$[{e_\mu}^I] = \left[\begin{matrix}
	cos\phi & -sin\phi \\
	sin\phi & cos\phi
\end{matrix}\right] \left[\begin{matrix}
	1 & 0 \\
	0 & r
\end{matrix}\right] = R(\phi) \cdot S(1,r)$<br>
<br>

...where $S(a,b) = \left[\begin{matrix} a & 0 \\ 0 & b \end{matrix}\right]$ is a scale matrix, and $R(\phi)$ is defined as whatever above is left, which happens to look like a rotation matrix.<br>
<br>

Now for the connections:<br>
<br>

${\Gamma^r}_{\phi\phi} = -r$<br>
${\Gamma^\phi}_{r\phi} = {\Gamma^\phi}_{\phi r} = \frac{1}{r}$<br>
<br>

$[\Gamma_\mu] = \left[\begin{matrix}
	{\Gamma^r}_{\mu r} & {\Gamma^\phi}_{\mu r} \\
	{\Gamma^r}_{\mu\phi} & {\Gamma^\phi}_{\mu\phi}
\end{matrix}\right]$<br>
<br>

$[\Gamma_r] = \left[\begin{matrix}
	{\Gamma^r}_{r r} & {\Gamma^\phi}_{r r} \\
	{\Gamma^r}_{r\phi} & {\Gamma^\phi}_{r\phi}
\end{matrix}\right] = \left[\begin{matrix}
	0 & 0 \\
	0 & \frac{1}{r}
\end{matrix}\right]$<br>
<br>


$[\Gamma_\phi] = \left[\begin{matrix}
	{\Gamma^r}_{\phi r} & {\Gamma^\phi}_{\phi r} \\
	{\Gamma^r}_{\phi\phi} & {\Gamma^\phi}_{\phi\phi}
\end{matrix}\right] = \left[\begin{matrix}
	0 & -r \\
	\frac{1}{r} & 0
\end{matrix}\right]$<br>
<br>

And now for the exponentials of the integrals of the connections:<br>
<br>

$\parallel_{r_L }^{ r_R} = exp(\int_{r_L}^{r_R} [\Gamma_r] dr)$<br>
$= exp\left( \int_{r_L}^{r_R} \left[\begin{matrix}
	0 & 0 \\
	0 & \frac{1}{r}
\end{matrix}\right]  dr \right)$<br>
$= exp\left( \left[\begin{matrix}
	0 & 0 \\
	0 & ln(r)
\end{matrix}\right]|_{r_L}^{r_R} \right)$<br>
$= exp\left( \left[\begin{matrix}
	0 & 0 \\
	0 & ln(r_R) - ln(r_L)
\end{matrix}\right] \right)$<br>
$= exp\left( \left[\begin{matrix}
	0 & 0 \\
	0 & ln(\frac{r_R}{r_L})
\end{matrix}\right] \right)$<br>
Since we now have a diagonal matrix, we can assert that the exponent of the matrix is a matrix of the exponent of the diagonals:<br>
$= \left[\begin{matrix}
	exp(0) & 0 \\
	0 & exp(ln(\frac{r_R}{r_L}))
\end{matrix}\right]$<br>
$= \left[\begin{matrix}
	1 & 0 \\
	0 & \frac{r_R}{r_L}
\end{matrix}\right]$<br>
$= S(1, \frac{r_R}{r_L})$<br>
<br>

$\parallel_{\phi_L}^{\phi_R} = exp(\int_{\phi_L}^{\phi_R} [\Gamma_\phi] d\phi)$<br>
$= exp\left(\int_{\phi_L}^{\phi_R} \left[\begin{matrix}
	0 & -r \\
	\frac{1}{r} & 0
\end{matrix}\right] d\phi \right)$<br>
$= exp\left( \left[\begin{matrix}
	0 & -r (\phi_R - \phi_L) \\
	\frac{1}{r} (\phi_R - \phi_L) & 0
\end{matrix}\right] \right)$<br>
Now we eigen-decompose the matrix before applying the exponential to the eigenvalues:<br> 
$= exp\left( 
\left[\begin{matrix} 
	1 & -i r \\ 
	1 & i r
\end{matrix}\right] 
\left[\begin{matrix} 
	-i (\phi_R - \phi_L) & 0 \\ 
	0 & i (\phi_R - \phi_L)
\end{matrix}\right] 
\left[\begin{matrix} 
	\frac{1}{2} & \frac{1}{2} \\ 
	\frac{i}{2 r} & -\frac{i}{2r}
\end{matrix}\right]
\right)$<br>
$= \left[\begin{matrix} 
	1 & -i r \\ 
	1 & i r
\end{matrix}\right] 
exp\left( \left[\begin{matrix} 
	-i (\phi_R - \phi_L) & 0 \\ 
	0 & i (\phi_R - \phi_L)
\end{matrix}\right] \right)
\left[\begin{matrix} 
	\frac{1}{2} & \frac{1}{2} \\ 
	\frac{i}{2 r} & -\frac{i}{2r}
\end{matrix}\right]
$<br>
Now we use the fact that the exponent of a diagonal matrix is the matrix of the exponent of the individual diagonal elements:<br>
$= \left[\begin{matrix} 
	1 & -i r \\ 
	1 & i r
\end{matrix}\right] 
\left[\begin{matrix} 
	exp(-i (\phi_R - \phi_L)) & 0 \\ 
	0 & exp(i (\phi_R - \phi_L))
\end{matrix}\right]
\left[\begin{matrix} 
	\frac{1}{2} & \frac{1}{2} \\ 
	\frac{i}{2 r} & -\frac{i}{2r}
\end{matrix}\right]
$<br>

$= \left[\begin{matrix}
	cos(\phi_R - \phi_L) & -r sin(\phi_R - \phi_L) \\
	\frac{1}{r} sin(\phi_R - \phi_L) & cos(\phi_R - \phi_L)
\end{matrix}\right]$<br> 
$= \left[\begin{matrix}
	1 & 0 \\
	0 & \frac{1}{r}
\end{matrix}\right] \left[\begin{matrix}
	cos(\phi_R - \phi_L) & -sin(\phi_R - \phi_L) \\
	sin(\phi_R - \phi_L) & cos(\phi_R - \phi_L)
\end{matrix}\right] 
\left[\begin{matrix}
	1 & 0 \\
	0 & r 
\end{matrix}\right]$<br>
$= S(1,\frac{1}{r}) R(\phi_R - \phi_L) S(1, r)$<br>
<br>

It just so happens that 
$\parallel_{r_L}^{r_R} \cdot \parallel_{\phi_L}^{\phi_R} = \parallel_{\phi_L}^{\phi_R} \cdot \parallel_{r_L }^{ r_R}$.  
Maybe because ${R^\alpha}_{\beta\mu\nu} = 0$, but that is still just speculation.<br>
And the applying the parallel transport in different orderings gives us:<br> 
<br>

$\parallel_{r_L}^{r_R} \cdot \parallel_{\phi_L}^{\phi_R}$<br>
$= S(1, \frac{r_R}{r_L}) S(1,\frac{1}{r_R}) R(\phi_R - \phi_L) S(1, r_R)$<br>
$= S(1,\frac{1}{r_L}) R(\phi_R - \phi_L) S(1, r_R)$<br>
<br>
$\parallel_{\phi_L}^{\phi_R} \cdot \parallel_{r_L}^{r_R}$<br>
$= S(1,\frac{1}{r_L}) R(\phi_R - \phi_L) S(1, r_L) S(1, \frac{r_R}{r_L})$<br>
$= S(1,\frac{1}{r_L}) R(\phi_R - \phi_L) S(1, r_R)$<br>
<br>

So in both cases we end up with the general pararallel transport from coordinate chart domain point x to y as:<br>
<br>
$\parallel_{x }^{ y} = S(1, \frac{1}{x^r}) \cdot R(y^\phi - x^\phi) \cdot S(1, y^r)
= \left[\begin{matrix}
	cos(\phi_R - \phi_L) & -r_R sin(\phi_R - \phi_L) \\
	\frac{1}{r_L} sin(\phi_R - \phi_L) & \frac{r_R}{r_L} cos(\phi_R - \phi_L)
\end{matrix}\right]$<br> 
<br>

TODO verify that this works with my general-case equation above, and not just per-coordinate separately.  It probably will but only because polar coordinates have no Riemann curvature.  The general case still isn't a geodesic case.<br>
<br>

TODO verify that the integral of (a vector field transported to a fixed basis), then aligned to Cartesian, is equal to the integral of a (vector field aligned to Cartesian).<br>
<br>

TODO look at what the value of parallel transporting on dx then dy is vs on dy then dx is.<br>

	</body>
</html>
