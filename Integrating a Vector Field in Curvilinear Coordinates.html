<!doctype html>
<html>
	<head>
        <meta charset='utf-8'>
		<script type='text/javascript' src='tryToFindMathJax.js'></script>
		<title>Integrating a Vector Field in Curvilinear Coordinates</title>
	</head>
	<body>

by Christopher Moore<br>
<br>

Start with a manifold $\mathcal{M}$ with dimension n.<br>
Define a point on the manifold: $p \in \mathcal{M}$<br>
Define the tangent space at a point on the manifold: $T_p(\mathcal{M})$<br>
Let u be a coordinate chart of the manifold, and let $x = \{x^\mu\}$ be a tuple of coordinates in the chart's domain, with $\mu$ spanning 1 to n (or 0 to n-1 if you would like), so $u(x) = p$.<br>
Define a basis $\{e_\mu(x)\}$ that spans $T_p(\mathcal{M})$.<br>
<br>

Define the basis represented in global Cartesian components to be: $e_\mu(x) = {e_\mu}^I(x) e_I$, for Cartesian basis $e_I$<Br>
$e_I$ has to be constant wrt all x because we're going to be integrating them across a region.  Don't worry, the transform ${e_\mu}^I$ will be used in the formalities but not in the results.<br>
<br>

Let's write that out per-component.  I'll use hats to denote the Cartesian basis indexes:<br>
${e_\mu}^I = \downarrow I \overset{\rightarrow \mu}{\left[\begin{matrix}
	{e_1}^\hat{1} & ... & {e_n}^\hat{1} \\
	\vdots & & \vdots \\
	{e_1}^\hat{n} & ... & {e_n}^\hat{n}
\end{matrix}\right]} = \overset{\rightarrow \mu}{\left[\begin{matrix}
	e_1 & | & ... &  | & e_n
\end{matrix}\right]} = \downarrow I \left[\begin{matrix}
	e^\hat{1} \\ --- \\ \vdots \\ --- \\ e^\hat{n}
\end{matrix}\right]
$<br>
<br>

Notice I picked my matrix representation of my ${e_\mu}^I$ tensor components such that contravariant components are distinct per each row (so contravariant indexes are represented as column vectors) 
and covariant components are distinct per each column (so covariant indexes are represented as row vectors).<br>
<br>

While we're here, let's define the dual basis / inverse transform $e^\mu$, such that $e_\mu e^\nu = \delta^\nu_\mu$.<br>
And lets define its Cartesian background components $e^\mu = {e^\mu}_I e^I$ where $e^I = e_I$ since the Cartesian metric is identity.<br>
<br>

${e^\mu}_I = \downarrow \mu \overset{\rightarrow I}{\left[\begin{matrix}
	{e^1}_\hat{1} & ... & {e^1}_\hat{n} \\
	\vdots & & \vdots \\
	{e^n}_\hat{1} & ... & {e^n}_\hat{n}
\end{matrix}\right]}$<br>
<br>

Notice that, in matrix form, $[{e^\mu}_I] = [{e_\mu}^I]^{-1}$<br>
<br>

The discernment between the basis and its background when written in index notation - by convention - is whether the first index, stated in curvilinear coordinates, is covariant or contravariant.  Covariant $e_\mu$ represents the vector basis while contravariant $e^\mu$ represents the dual basis.
This is probably a bad idea, and I should probably use some other symbol to denote the dual basis in index notation, maybe even $(e^{-1})^\mu$<br>
<br>

I'm going to be using $[A_{\mu\nu}]$ to denote a matrix representation of a tensor.
Sometimes I'll still keep the summation indexes as a reminder of contra/co-variant representation of the tensor component quantities, 
other times I will remove the summation indexes if the contra-/co-variance of the tensor should be deducable.
If the brackets do remain around a tensor, even if it is in index notation, that is just to serve as a reminder (to me) that this is a matrix value.<br>
<br>

If I ever drop all indexes from $[e_\mu]$, I will shorthand denote $e = [{e_\mu}^I]$ and $e^{-1} = [{e^\mu}_I]$.<br>
<br>

Arbitrary (p q) tensor:<br>
$A = {A^{\mu_1 ... \mu_p}}_{\nu_1 ... \nu_q} \underset{k=1}{\overset{p}{\otimes}} e_{\mu_k} \underset{k=1}{\overset{q}{\otimes}} e^{\nu_k} $<br>
Vector field:<br>
$v = v^\mu e_\mu$<br>
<br>

Connection and covariant derivative:<br>
$\nabla_\mu e_\nu = {\Gamma^\alpha}_{\mu\nu} e_\alpha$<br>
<br>

Written out per-component:<br>
$\nabla_\mu \left[\begin{matrix}
	{e_1}^\hat{1} & ... & {e_n}^\hat{1} \\
	\vdots & & \vdots \\
	{e_1}^\hat{n} & ... & {e_n}^\hat{n}
\end{matrix}\right] = \left[\begin{matrix}
	{e_1}^\hat{1} & ... & {e_n}^\hat{1} \\
	\vdots & & \vdots \\
	{e_1}^\hat{n} & ... & {e_n}^\hat{n}
\end{matrix}\right] \left[\begin{matrix}
	{\Gamma^1}_{\mu 1} & ... & {\Gamma^n}_{\mu 1} \\
	\vdots & & \vdots \\
	{\Gamma^1}_{\mu n} & ... & {\Gamma^n}_{\mu n}
\end{matrix}\right]$<br>
<br>

Notice that, because I picked my contravariant components to be distinct per each row (contravariant indexes are represented as column vectors) and covariant components to be distinct per each column (covariant indexes are represented as row vectors), this means the matrix-multiplication of the connection must be on the right hand.  
If you want to multiply the connection matrix on the left hand side of the basis matrix then you must use distinct contravariant components per each column (contravariant indexes are represented as row vectors) and distinct covariant compoments per each row (covariant indexes are represented as column vectors).<br>
<br>

Since we are differentiating the cartesian components which are constant at all points, lets exchange the covariant derivative with a partial derivative:<br>
$\frac{\partial}{\partial x^\mu} [{e_\nu}^I](x) = [{e_\alpha}^I](x) \cdot [{\Gamma^\alpha}_{\mu\nu}](x) $<br>
<br>

Abuse notation a bit:<br>
$\int [{e^\alpha}_I](x) \cdot d [{e_\nu}^I](x) = \int_{x'^\mu = x_L^\mu}^{x'^\mu = x_R^\mu} [{\Gamma^\alpha}_{\mu\nu}](x') dx'$<br>
<br>

Integrate the linear dynamic system and hide the Cartesian basis index to find:<br>
$[e_\nu](x^\mu_R) = [e_\alpha](x^\mu_L) \cdot exp\left( \int_{x'^\mu = x_L^\mu}^{x'^\mu = x_R^\mu} [{\Gamma^\alpha}_{\mu\nu}](x'^\mu) dx'^\mu \right)$<br>
<br>

...where I make some liberal exchanges of when I want $x^\mu_L$ to indicate the scalar value of the left-bounds of the $\mu$ index of the x coordinate, and when I want it to mean the x coordinate with the $\mu$ component exchanged with the scalar value $x^\mu_L$.<br>
<br>

So that's how you transport a basis from one point $x^\mu_L$ to another point $x^\mu_R$ along a single changing coordinate $x^\mu$.<br>
How about if you want to transport it along an arbitrary coordinate path on the manifold?<br>
For that, for now, I will only consider separate movement along individual coordinate lines:<br>
Let's assume x and y are points on our manifold.<br>
<br>

$[e_\nu](y) = [e_{\alpha_1}](x) 
	\cdot exp\left( \int_{z^1 = x^1}^{z^1 = y^1} [{\Gamma^{\alpha_1}}_{1 {\alpha_2}}](z^1, x^2, ..., x^n) dz^1 \right)
	\cdot ...
	\cdot exp\left( \int_{z^\mu = x^\mu}^{z^\mu = y^\mu} [{\Gamma^{\alpha_\mu}}_{\mu {\alpha_{\mu+1}}}](y^1, ..., y^{\mu-1}, z^\mu, x^{\mu+1}, ..., x^n) dz^\mu \right)
	\cdot ...
	\cdot exp\left( \int_{z^n = x^n}^{z^n = y^n} [{\Gamma^{\alpha_n}}_{n \nu}](y^1, ..., y^{n-1}, z^n) dz^n \right)
$<br>
<br>
Notice that I am transforming the original basis across each connection's coordinate dimension individually, and after I perform each transform I exchange the source coordinate in the integral with the destination component.<br>
Think of it like traversing the edges of a n-hypercube to get from (0,0,...,0) to (1,1,...,1).<br>
<br>

Maybe with some more notation abuse I could write that as:<br>
$[e_\nu](y) = [e_\alpha](x) 
	\cdot \underset{\mu=1}{\overset{n}{\Pi}} exp\left( \int_{z^\mu = x^\mu}^{z^\mu = y^\mu} [{\Gamma^\alpha}_{\mu \nu}](y^{1..\mu-1} | z^\mu | x^{\mu+1..n}) dz^\mu \right)$<br>
<br>
<br>

I should come up with some notation to denote the transform applied for dragging the basis from point x to point y.  How about:<br>
${[\parallel_{x_L^\mu}^{x_R^\mu}]^\alpha}_\nu
= exp\left( \int_{x'^\mu = x_L^\mu}^{x'^\mu = x_R^\mu} [{\Gamma^\alpha}_{\mu\nu}](x'^\mu) dx'^\mu \right)$<br>
<br>

Mind you the result of $\parallel_a^b$ is a matrix, and the $\alpha$ and $\nu$ in this case are the matrix indexes.<br>
<br>

The above transformation from $e_\nu(x_L^\mu)$ to $e_\nu(y^\mu_R)$ would look like:<br>
$e(x_R^\mu) = e(x_L^\mu) \cdot \parallel_{x_L^\mu}^{x_R^\mu}$<br>
<br>

And the above transformation from $e_\nu(x)$ to $e_\nu(y)$, going component-by-component, would look like:<br>
$e(y) = e(x) 
	\cdot \parallel_{x^1}^{y^1} (x^1 ,..., x^n) 
	\cdot ... 
	\cdot \parallel_{x^k}^{y^k} (y^1, ..., y^{k-1}, x^k, ..., x^n)
	\cdot ... 
	\cdot \parallel_{x^n}^{y^n} (y^1, ..., y^{n-1}, x^n)$<br>
<br>
That can just be written as $e(x) \underset{k=1}{\overset{n}{\Pi}} \parallel_{x^k}^{y^k}$ if we don't mind hiding the chart coordinates, which seem to be an important detail.<br>
<br>
<br>

Notice that order of application of the exp-integrals-of-connections is important.  If you were to swap orders then you would have to deal with the $[\nabla_\mu, \nabla_\mu] e_\alpha = e_\beta {R^\beta}_{\alpha\mu\nu}$ Riemann curvature tensor.  Think about exactly how later.<br>
<br>
Does this mean that, by the Bianchi identity of the Riemann curvature tensor, that exchanging an index may produce a result whose difference with the original is proportional to the Riemann curvature tensor but cycling all indexes will result in the same integral?  I'll think about that later.<br>
<br>
<br>

Alright, what if you do want to just cut across the diagonal of the cube instead of following the edges around the outside?
I'm guessing that would look like:<br>
<br>
$[e_\nu](y) = [e_\alpha](x) \cdot exp\left( \int_{\lambda = 0}^{\lambda = 1} [{\Gamma^\alpha}_{\mu\nu}](x + \lambda v) v^\mu d\lambda \right)$<br>
<br>
...where $v = y - x$, but I still need to prove this.
This would be especially useful in manifolds that do have curvature, like quaternions, where the exponential map of $e_1$ and $e_2$ are not commutative, and $exp(\theta n^i e_i)$ produces a rotation around axis $n^i$ by angle $\theta$.<br>
<br>
Then again, thinking further, changing the chart coordinates in a linear fashion does not guarantee a geodesic.  In order to do that, you would (once again) need to make use of the connection.  A double integral of some function of the connection?  After all, the geodesic equation is a 2nd derivative of the position.  Think on this one more later.
The shorthand representation of our geodesic parallel transport could be something like: $e(y) = e(x) \cdot \parallel_x^y$<br>
<br>

The notation for a geodesic looks just like the notation for transport around the edges of a hypercube representaiton above, even though these represent two different transformations.
It looks like I need to specify in the notation a way to discern the path of the transport as well.
Maybe just write $\parallel_C$ and specify that C is a curve, and whether the curve is a geodesic or whether the curve is a piecewise edge traversal from one corner of a hypercube to the other (as I first stated).<br>
<br>

<hr>
<br>

So now on to vector/tensor components.  I'll start with vector, but you can extrapolate if you want.<br>
$v = v^\mu e_\mu$<br>
<br>

In matrix form:<br>
$v = \left[\begin{matrix}
	e_1 & | & ... & | & e_n
\end{matrix}\right] \left[\begin{matrix}
	v^1 \\ --- \\ \vdots \\ --- \\ v^n
\end{matrix}\right]$<br>
<br>

As a matrix, in our fixed background Cartesian components (since they are independent of our choice of p):<br>
$v = v^\mu {e_\mu}^I e_I = v^I e_I$<br>
<br>

In matrix form:<br>
$v = \left[\begin{matrix}
	e_\hat{1} & | & ... &  | & e_\hat{n}
\end{matrix}\right] \left[\begin{matrix}
	v^\hat{1} \\ --- \\ \vdots \\ --- \\ v^\hat{n}
\end{matrix}\right] = \left[\begin{matrix}
	e_\hat{1} & | & ... &  | & e_\hat{n}
\end{matrix}\right] \left[\begin{matrix}
	{e_1}^\hat{1} & ... & {e_n}^\hat{1} \\
	\vdots & & \vdots \\
	{e_1}^\hat{n} & ... & {e_n}^\hat{n}
\end{matrix}\right] \left[\begin{matrix}
	v^1 \\ --- \\ \vdots \\ --- \\ v^n
\end{matrix}\right]$<br>
<br>

So our background basis components of our vector field are:<br>
<br>
$v^I = {e_\mu}^I v_\mu$<br>
<br>
$\left[\begin{matrix}
	v^\hat{1} \\ --- \\ \vdots \\ --- \\ v^\hat{n}
\end{matrix}\right] = \left[\begin{matrix}
	{e_1}^\hat{1} & ... & {e_n}^\hat{1} \\
	\vdots & & \vdots \\
	{e_1}^\hat{n} & ... & {e_n}^\hat{n}
\end{matrix}\right] \left[\begin{matrix}
	v^1 \\ --- \\ \vdots \\ --- \\ v^n
\end{matrix}\right]$<br>
<br>

So if we have a vector with components relative to the basis at one point x, how do we find the same vector components relative to a basis at y?<br>
$v(x) = v'(y)$<br>
<br>

I'll take advantage of the fact that our Cartesian components are fixed in order to calculate this:<br>
$v^I(x) = v'^I(y)$<br>
<br>

In matrix form:<br>
<br>

$\left[\begin{matrix}
	v^\hat{1} \\ --- \\ \vdots \\ --- \\ v^\hat{n}
\end{matrix}\right]_{(x)} = \left[\begin{matrix}
	v'^\hat{1} \\ --- \\ \vdots \\ --- \\ v'^\hat{n}
\end{matrix}\right]_{(y)}$<br>
<br>

Notice I've equated my hatted quantities, because these are with respect to our fixed background basis $e_I$.<br>
Now to expand it in terms of the coordinate basis components:<br>
<br>

${e_\mu}^I(x) v^\mu(x) = {e_\mu}^I(y) v'^\mu(y)$<br>
<br>

$\left[\begin{matrix}
	{e_1}^\hat{1} & ... & {e_n}^\hat{1} \\
	\vdots & & \vdots \\
	{e_1}^\hat{n} & ... & {e_n}^\hat{n}
\end{matrix}\right]_{(x)} \left[\begin{matrix}
	v^1 \\ --- \\ \vdots \\ --- \\ v^n
\end{matrix}\right]
 = 
\left[\begin{matrix}
	{e_1}^\hat{1} & ... & {e_n}^\hat{1} \\
	\vdots & & \vdots \\
	{e_1}^\hat{n} & ... & {e_n}^\hat{n}
\end{matrix}\right]_{(y)} \left[\begin{matrix}
	v'^1 \\ --- \\ \vdots \\ --- \\ v'^n
\end{matrix}\right]$
<br>

Solve for $v'^\mu$:<br>
${e^\nu}_I(y) {e_\mu}^I(x) v^\mu(x) = {e^\nu}_I(y) {e_\mu}^I(y) v'^\mu(y)$<br>
${e^\nu}_I(y) {e_\mu}^I(x) v^\mu(x) = \delta^\nu_\mu v'^\mu(y)$<br>
${e^\nu}_I(y) {e_\mu}^I(x) v^\mu(x) = v'^\nu(y)$<br>
<br>

i.e.:<br>
$e^{-1}(y) e(x) v(x) = v'(y)$<br>
$(e(y))^{-1} e(x) v(x) = v'(y)$<br>
<br>

i.e.:<br>
$ \left[\begin{matrix}
	v'^1 \\ --- \\ \vdots \\ --- \\ v'^n
\end{matrix}\right]
= \left( \left[\begin{matrix}
	{e_1}^\hat{1} & ... & {e_n}^\hat{1} \\
	\vdots & & \vdots \\
	{e_1}^\hat{n} & ... & {e_n}^\hat{n}
\end{matrix}\right]_{(y)} \right)^{-1}
\left[\begin{matrix}
	{e_1}^\hat{1} & ... & {e_n}^\hat{1} \\
	\vdots & & \vdots \\
	{e_1}^\hat{n} & ... & {e_n}^\hat{n}
\end{matrix}\right]_{(x)} \left[\begin{matrix}
	v^1 \\ --- \\ \vdots \\ --- \\ v^n
\end{matrix}\right]$<br>
<br>

That looks fine, except we are still using ${e_\mu}^I$, which has components in our Cartesian basis.<br>
How do we represent everything only in our curvilinear chart coordinates?<br>
<br>

Let's represent our $e(y)$ as a parallel transport of the basis from $e(x)$ to $e(y)$.<br>
$e(y) = e(x) \cdot \parallel_x^y$<br>
Substitute to find:<br>
$(e(x) \cdot \parallel_x^y)^{-1} e(x) v(x) = v'(y)$<br>
$(\parallel_x^y)^{-1} e^{-1}(x) e(x) v(x) = v'(y)$<br>
$\parallel_y^x \cdot v(x) = v'(y)$<br>
<br>

Tada! Now we have a representation of the parallel transport that depends only on connections, not on the Cartesian basis.<br>
<br>

Notice that this depends on the identity: $(\parallel_x^y)^{-1} = \parallel_y^x$<br>
That can be proven quickly with:<br>
$\parallel_y^x$<br>
$= exp(\int_y^x \Gamma_v d\lambda)$<br>
$= exp(-\int_x^y \Gamma_v d\lambda)$<br>
$= exp(\int_x^y \Gamma_v d\lambda)^{-1}$<br>
$= (\parallel_x^y)^{-1}$<br>
<br>

The interesting thing is that the resulting components after parallel transport from x to y is $v(x)$ left-multiplied with the transport from y to x.<br>
This is the same as the inverse of the parallel-transport transform from x to y, and that itself will be left-multiplied with the original parallel-transport transform from x to y, and that left-multiplied with $e(x)$, in order to deduce that $v' = v$:<br>
$e(x) \cdot \parallel_x^y \cdot (\parallel_x^y)^{-1} \cdot v(x) = e(x) \cdot \parallel_x^y \cdot v'(y)$<br>
$e(x) v(x) = e(y) v'(y)$<br>
$v = v'$<br>
<br>

<hr>
<br>

Now for a specific example: Polar coordinates:<br>
<br>

chart:<br>
$u^I = \left[\begin{matrix}
	r cos\phi \\
	r sin\phi
\end{matrix}\right]$<br>
<br>

${e_r}^I = {u^I}_{,r} = \left[\begin{matrix}
	{e_r}^x \\
	{e_r}^y
\end{matrix}\right] = \left[\begin{matrix}
	cos\phi \\
	sin\phi
\end{matrix}\right]$<br>
<br>

${e_\phi}^I = {u^I}_{,\phi} = \left[\begin{matrix}
	{e_\phi}^x \\
	{e_\phi}^y
\end{matrix}\right] = \left[\begin{matrix}
	-r sin\phi \\
	r cos\phi
\end{matrix}\right]$<br>
<br>

${e_\mu}^I = {u^I}_{,\mu} = \left[\begin{matrix}
	{e_r}^x	& {e_\phi}^x \\
	{e_r}^y	& {e_\phi}^y
\end{matrix}\right] = \left[\begin{matrix}
	cos\phi & -r sin\phi \\
	sin\phi & r cos\phi
\end{matrix}\right]$<br>
<br>

It might be later convenient to think of this as a product of two linear operations:<br>
<br>
$[{e_\mu}^I] = \left[\begin{matrix}
	cos\phi & -sin\phi \\
	sin\phi & cos\phi
\end{matrix}\right] \left[\begin{matrix}
	1 & 0 \\
	0 & r
\end{matrix}\right] = R(\phi) \cdot S(1,r)$<br>
<br>

...where $S(a,b) = \left[\begin{matrix} a & 0 \\ 0 & b \end{matrix}\right]$ is a scale matrix, and $R(\phi)$ is defined as whatever above is left, which happens to look like a rotation matrix.<br>
<br>

Now for the connections:<br>
<br>

${\Gamma^r}_{\phi\phi} = -r$<br>
${\Gamma^\phi}_{r\phi} = {\Gamma^\phi}_{\phi r} = \frac{1}{r}$<br>
<br>

$[\Gamma_\mu] = \left[\begin{matrix}
	{\Gamma^r}_{\mu r} & {\Gamma^\phi}_{\mu r} \\
	{\Gamma^r}_{\mu\phi} & {\Gamma^\phi}_{\mu\phi}
\end{matrix}\right]$<br>
<br>

$[\Gamma_r] = \left[\begin{matrix}
	{\Gamma^r}_{r r} & {\Gamma^\phi}_{r r} \\
	{\Gamma^r}_{r\phi} & {\Gamma^\phi}_{r\phi}
\end{matrix}\right] = \left[\begin{matrix}
	0 & 0 \\
	0 & \frac{1}{r}
\end{matrix}\right]$<br>
<br>


$[\Gamma_\phi] = \left[\begin{matrix}
	{\Gamma^r}_{\phi r} & {\Gamma^\phi}_{\phi r} \\
	{\Gamma^r}_{\phi\phi} & {\Gamma^\phi}_{\phi\phi}
\end{matrix}\right] = \left[\begin{matrix}
	0 & -r \\
	\frac{1}{r} & 0
\end{matrix}\right]$<br>
<br>

And now for the exponentials of the integrals of the connections:<br>
<br>

$\parallel_{r_L}^{r_R} = exp(\int_{r_L}^{r_R} [\Gamma_r] dr)$<br>
$= exp\left( \int_{r_L}^{r_R} \left[\begin{matrix}
	0 & 0 \\
	0 & \frac{1}{r}
\end{matrix}\right]  dr \right)$<br>
$= exp\left( \left[\begin{matrix}
	0 & 0 \\
	0 & ln(r)
\end{matrix}\right]|_{r_L}^{r_R} \right)$<br>
$= exp\left( \left[\begin{matrix}
	0 & 0 \\
	0 & ln(r_R) - ln(r_L)
\end{matrix}\right] \right)$<br>
$= exp\left( \left[\begin{matrix}
	0 & 0 \\
	0 & ln(\frac{r_R}{r_L})
\end{matrix}\right] \right)$<br>
Since we now have a diagonal matrix, we can assert that the exponent of the matrix is a matrix of the exponent of the diagonals:<br>
$= \left[\begin{matrix}
	exp(0) & 0 \\
	0 & exp(ln(\frac{r_R}{r_L}))
\end{matrix}\right]$<br>
$= \left[\begin{matrix}
	1 & 0 \\
	0 & \frac{r_R}{r_L}
\end{matrix}\right]$<br>
$= S(1, \frac{r_R}{r_L})$<br>
<br>

$\parallel_{\phi_L}^{\phi_R} = exp(\int_{\phi_L}^{\phi_R} [\Gamma_\phi] d\phi)$<br>
$= exp\left(\int_{\phi_L}^{\phi_R} \left[\begin{matrix}
	0 & -r \\
	\frac{1}{r} & 0
\end{matrix}\right] d\phi \right)$<br>
$= exp\left( \left[\begin{matrix}
	0 & -r (\phi_R - \phi_L) \\
	\frac{1}{r} (\phi_R - \phi_L) & 0
\end{matrix}\right] \right)$<br>
Now we eigen-decompose the matrix before applying the exponential to the eigenvalues:<br> 
$= exp\left( 
\left[\begin{matrix} 
	1 & -i r \\ 
	1 & i r
\end{matrix}\right] 
\left[\begin{matrix} 
	-i (\phi_R - \phi_L) & 0 \\ 
	0 & i (\phi_R - \phi_L)
\end{matrix}\right] 
\left[\begin{matrix} 
	\frac{1}{2} & \frac{1}{2} \\ 
	\frac{i}{2 r} & -\frac{i}{2r}
\end{matrix}\right]
\right)$<br>
$= \left[\begin{matrix} 
	1 & -i r \\ 
	1 & i r
\end{matrix}\right] 
exp\left( \left[\begin{matrix} 
	-i (\phi_R - \phi_L) & 0 \\ 
	0 & i (\phi_R - \phi_L)
\end{matrix}\right] \right)
\left[\begin{matrix} 
	\frac{1}{2} & \frac{1}{2} \\ 
	\frac{i}{2 r} & -\frac{i}{2r}
\end{matrix}\right]
$<br>
Now we use the fact that the exponent of a diagonal matrix is the matrix of the exponent of the individual diagonal elements:<br>
$= \left[\begin{matrix} 
	1 & -i r \\ 
	1 & i r
\end{matrix}\right] 
\left[\begin{matrix} 
	exp(-i (\phi_R - \phi_L)) & 0 \\ 
	0 & exp(i (\phi_R - \phi_L))
\end{matrix}\right]
\left[\begin{matrix} 
	\frac{1}{2} & \frac{1}{2} \\ 
	\frac{i}{2 r} & -\frac{i}{2r}
\end{matrix}\right]
$<br>

$= \left[\begin{matrix}
	cos(\phi_R - \phi_L) & -r sin(\phi_R - \phi_L) \\
	\frac{1}{r} sin(\phi_R - \phi_L) & cos(\phi_R - \phi_L)
\end{matrix}\right]$<br> 
$= \left[\begin{matrix}
	1 & 0 \\
	0 & \frac{1}{r}
\end{matrix}\right] \left[\begin{matrix}
	cos(\phi_R - \phi_L) & -sin(\phi_R - \phi_L) \\
	sin(\phi_R - \phi_L) & cos(\phi_R - \phi_L)
\end{matrix}\right] 
\left[\begin{matrix}
	1 & 0 \\
	0 & r 
\end{matrix}\right]$<br>
$= S(1,\frac{1}{r}) R(\phi_R - \phi_L) S(1, r)$<br>
<br>

It just so happens that 
$\parallel_{r_L}^{r_R} \cdot \parallel_{\phi_L}^{\phi_R} = \parallel_{\phi_L}^{\phi_R} \cdot \parallel_{r_L}^{r_R}$.  
Maybe because ${R^\alpha}_{\beta\mu\nu} = 0$, but that is still just speculation.<br>
And the applying the parallel transport in different orderings gives us:<br> 
<br>

$\parallel_{r_L}^{r_R} \cdot \parallel_{\phi_L}^{\phi_R}$<br>
$= S(1, \frac{r_R}{r_L}) S(1,\frac{1}{r_R}) R(\phi_R - \phi_L) S(1, r_R)$<br>
$= S(1,\frac{1}{r_L}) R(\phi_R - \phi_L) S(1, r_R)$<br>
<br>
$\parallel_{\phi_L}^{\phi_R} \cdot \parallel_{r_L}^{r_R}$<br>
$= S(1,\frac{1}{r_L}) R(\phi_R - \phi_L) S(1, r_L) S(1, \frac{r_R}{r_L})$<br>
$= S(1,\frac{1}{r_L}) R(\phi_R - \phi_L) S(1, r_R)$<br>
<br>

So in both cases we end up with the general pararallel transport from coordinate chart domain point $x_L$ to $x_R$ as:<br>
<br>
$\parallel_{x_L}^{x_R} = S(1, \frac{1}{r_L}) \cdot R(\phi_R - \phi_L) \cdot S(1, r_R)
= \left[\begin{matrix}
	cos(\phi_R - \phi_L) & -r_R sin(\phi_R - \phi_L) \\
	\frac{1}{r_L} sin(\phi_R - \phi_L) & \frac{r_R}{r_L} cos(\phi_R - \phi_L)
\end{matrix}\right]$<br> 
<br>

What happens when we parallel transport our basis from $x_L$ to $x_R$?<br>
<br>
$e(x_L) \cdot \parallel_{x_L}^{x^R} = 
\left(
	R(\phi_L) \cdot S(1, r_L) 
\right) \cdot \left(
	S(1, \frac{1}{r_L}) \cdot R(\phi_R - \phi_L) \cdot S(1, r_R)
\right)$<br>
$= R(\phi_L) \cdot R(\phi_R - \phi_L) \cdot S(1, r_R)$<br>
And since our rotation axis are aligned, we can add our rotation angles together:<br>
$= R(\phi_R) \cdot S(1, r_R)$<br>
<br>

Tada!  We started with $e(x_L)$, we right-applied our parallel-transport transformation, and we ended up at $e(x_R)$.<br>
<br>

TODO verify that this works with my general-case equation above, and not just per-coordinate separately.  It probably will but only because polar coordinates have no Riemann curvature.  The general case still isn't a geodesic case.<br>
<br>

TODO verify that the integral of (a vector field transported to a fixed basis), then aligned to Cartesian, is equal to the integral of a (vector field aligned to Cartesian).<br>
<br>

TODO look at what the value of parallel transporting on dx then dy is vs on dy then dx is.<br>

	</body>
</html>
