<!doctype html>
<html>
	<head>
        <meta charset='utf-8'>
		<script type='text/javascript' src='tryToFindMathJax.js'></script>
		<title>PDE Systems, Parallel Propagators, Finite Volume</title>
	</head>
	<body>

Parallel propagator along a single coordinate:<br>
${P_\mu(x_L^\mu, x_R^\mu)^\alpha}_\nu = exp\left( -\int_{x'^\mu = x_L^\mu}^{x'^\mu = x_R^\mu} \Gamma_\mu(x'^\mu) dx'^\mu \right)$<br>
...where $\Gamma_\mu$ is a matrix with components ${\Gamma^\alpha}_{\mu\nu}$<br>
<br>

Parallel propagator linearly interpolated between coordinates:<br>
$P_\mu(x,y) = exp\left( -\int_{\lambda = 0}^{\lambda = 1} \Gamma_\mu(x + \lambda v) v^\mu d\lambda \right)$<br>
<br>

Matrix exponential:<br>
$exp(A) = exp(R \cdot \Lambda \cdot L) = R \cdot exp(\Lambda) \cdot L$<br>
$R^{-1} \cdot exp(A) \cdot L^{-1} = exp(\Lambda)$<br>
$\Lambda = log(R^{-1} \cdot exp(A) \cdot L^{-1})$<br>
$A = R \cdot \Lambda \cdot L = R \cdot log(R^{-1} \cdot exp(A) \cdot L^{-1}) \cdot L$<br>
$A = log(exp(A)) = log(exp(R \cdot \Lambda \cdot L)) = log(R \cdot exp(\Lambda) \cdot R)$<br>
<br>
TODO for $A = R_A \cdot \Lambda_A \cdot L_A$, prove $log(A) = R_A \cdot log(\Lambda_A) \cdot L_A$<br>
Let $B = log(A)$, so $exp(B) = A$<br>
and $B = R_B \cdot \Lambda_B \cdot L_B$<br>
and (by argument of diagonalization of $exp(A_{ij}) = \delta_{ij} exp(A_{ij})$) we know that $R_B = R_A, L_B = L_A$<br>
so $\Lambda_B = log(\Lambda_A)$<br>
so $log(A) = R_A \cdot log(\Lambda_A) \cdot L_A$<br>
where, for diagonal matrix, $(log(\Lambda_A))_{ij} = 0$ for $i \ne j$, or $log(A_{ij})$ for $i = j$.<br>
<br>

Flux<br> Jacobian Eigendecomposition:<br>
$F^\alpha =$ flux.<br>
${A^\alpha}_\mu = \frac{\partial F^\alpha}{\partial U^\mu} =$ flux Jacobian.<br>
$A = R \cdot \Lambda \cdot L$<br>
<br>

Let $-\int \Gamma_\mu v^\mu d\lambda = -\int \Gamma(\lambda, v) d\lambda = A$ ... notice the first $\Gamma$ parameter is its dependent variable $\lambda$, while the second is the vector $v^\mu$ that the one-form $\Gamma$ is actiong upon.  Maybe it would be more proper math to write $\Gamma(\lambda)(v)$?<br>
So $A = R \cdot \Lambda \cdot L = -\int \Gamma_\mu v^\mu d\lambda$<br>
And $exp(-\int \Gamma_\mu v^\mu d\lambda) = R \cdot \Lambda \cdot L$<br>
And $-\int \Gamma_\mu v^\mu d\lambda = R \cdot log(\Lambda) \cdot L$<br>
<br>

How about if we say $\nabla_{e_\mu} e_\nu = {\Gamma^\alpha}_{\mu\nu} e_\alpha$ is equivalent to $\frac{\partial F^\alpha}{\partial U^\mu}$?<br>
<br>

How about if $-{\Gamma^\alpha}_{\mu\nu} \hat{x}^\mu = {A^\alpha}_\nu$?<br>
$\int -{\Gamma^\alpha}_{\mu\nu} v^\mu d\lambda = \int {A^\alpha}_\nu d\lambda$<br>
<br>

Then there's Dullemond, Wang's Hydrodynamics notes:<br>
Start with $\partial_t U + \partial_{x^i} F^i = 0$<br>
(In arbitrary curvilinear coordiantes $\partial_t U + \nabla_i F^i = 0$)<br>
eqn. 7.36:<br>
$F(U(x_R)) - F(U(x_L)) = \int_0^1 \frac{\partial}{\partial \lambda} F(U(\lambda)) d\lambda$<br>
$= (\int_0^1 \frac{\partial F}{\partial U} d\lambda) \cdot (U(x_R) - U(x_L))$<br>
$= \int_0^1 (R \cdot \Lambda \cdot L) d\lambda \cdot (U(x_R) - U(x_L))$<br>
...<br>
$= \int_0^1 (R \cdot exp(log(\Lambda)) \cdot L) d\lambda \cdot (U(x_R) - U(x_L))$<br>
...<br>
$exp\left( F(U(x_R)) - F(U(x_L)) \right) = exp\left( \left( \int_0^1 \frac{\partial F}{\partial U} d\lambda \right) \cdot (U(x_R) - U(x_L)) \right)$ ... or does that go outside the exp?<br>
$= exp\left( \int_0^1 \frac{\partial F}{\partial U} d\lambda \right) (U(x_R) - U(x_L))$<br>
$= exp\left( \int_0^1 R \cdot \Lambda \cdot L d\lambda \right) (U(x_R) - U(x_L))$<br>
$exp(F(U(x_R))) \cdot exp(-F(U(x_L))) 
	= exp\left( \int_0^1 R \cdot \Lambda \cdot L d\lambda \right) (U(x_R) - U(x_L))$<br>
so the flux $F$ is the log of the connection $\Gamma_x$ that is parallel-propagated along the x-coordiante.<br>
<br>

Parallel propagator:<br>
$e(x_R) = e(x_L) \cdot P^{-1} (x_L, x_R)$<br>
$= e(x_L) \cdot exp(-\int_0^1 \Gamma_v d\lambda)$<br>
$e(x_L)^{-1} e(x_R) = exp(-\int_0^1 \Gamma_v d\lambda)$<br>
$exp(log(e(x_L)^{-1} e(x_R))) = exp(-\int_0^1 \Gamma_v d\lambda)$<br>
$exp(log(e(x_R)) - log(e(x_L))) = exp(-\int_0^1 \Gamma_v d\lambda)$<br>
$log(e(x_R)) - log(e(x_L)) = \int_0^1 -\Gamma_v d\lambda$<br>
...substitute...<br>
$F(U(x_R)) \leftrightarrow log(e(x_R))$<br>
$F(U(x_L)) \leftrightarrow log(e(x_L))$<br>
$\frac{\partial F}{\partial U} \cdot (U(x_R) - U(x_L)) \leftrightarrow -\Gamma_v$<br>
...to get...<br>
$F(U(x_R)) - F(U(x_L)) = \int_0^1 \frac{\partial F}{\partial U} (U(x_R) - U(x_L)) d\lambda$<br>
$F(U(x_R)) - F(U(x_L)) = \int_0^1 \frac{\partial F}{\partial U} \frac{\partial U}{\partial \lambda} d\lambda$<br>
$F(U(x_R)) - F(U(x_L)) = \int_0^1 \frac{\partial F}{\partial \lambda} d\lambda$<br>
<br>


Then there is Misner, Thorn, Wheeler "Gravitation" exercise 16.6 showing how $\partial_t U + \nabla_{x^i} F^i = 0$ is the same as $\nabla_\nu T^{\mu\nu} = 0$<br>
where $\partial_t \rho + \nabla_{x^i} (\rho v^i) = 0$ becomes $\nabla_\mu T^{t\mu} = 0$<br>
and $\partial_t (\rho v^j) + \nabla_{x^i} (\rho v^i v^j + g^{ij} P) = 0$ becomes $\nabla_\mu T^{j\mu} = 0$<br>
<br>

<br>
<br>
<br>
	</body>
</html>
