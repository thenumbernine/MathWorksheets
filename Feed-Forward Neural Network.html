<!doctype html>
<html>
	<head>
		<meta charset='utf-8'>
		<script type='text/javascript' src='tryToFindMathJax.js'></script>
		<title>neural network dynamics</title>
	</head>
	<body>

<h2>Network Description:</h2>

Network topology:<br>
$l = $ number of layers.<br>
$n_k = $ input size of the k'th layer.<br>
$m_k = $ output size of the k'th layer.<br>
More often than not I will be depicting these two as a sequence-of-pairs: $\{(m,n)\}$.  Yes, that's right, the pair is (output dimension, input dimension), as per how matrix spaces are described as Cartesian products of the Real set.<br>
$\sigma_k = $ k'th layer activation function.<br>
<br>

Network state, independent of any signal:<br>
$w_{k, i, j}(t) = $ k'th layer linear relation between i'th output and j'th input, at iteration "t"
	for $1 \le k \le l$, 
	$1 \le i \le m_l$,
	$1 \le j \le n_l$.
	<br>
For networks with using bias, $1 \le j \le n_l+1$.<br>
Yes I am conflating iteration and time.  I don't care about time-dependent input data or state-based recurrent-networks right now.<br>
$w_{k, i, j}(0) =$ initial weight value at iteration t=0. Weights are initialized randomly.<br>
$W_k(t) = $ the k'th layer weights in matrix form. $W_k(t) \in \mathbb{R}^{(m_k \times n_k)}$.<br>
Maybe sometimes I'll separate off the bias weights as a vector $B_k(t)$ to distinguish it from weights multiplied by inputs.<br>
<br>

Network feed-forward live variables:<br>
Inputs are provided from some external environment.<br>
$x_{k, j}(t) =$ k'th layer j'th input, at iteration "t"
	for $1 \le k \le l$ and $1 \le j \le n_l$.<br>
For networks using bias, $1 \le j \le n_l + 1$, with $x_{k, n_l + 1}$ always assigned to 1.<br>
$X_k(t) = $ k'th layer inputs in vector form.<br>
$\xi_{k, i}(t) = \underset{j=1}{\overset{n_l+1}\Sigma} \left( w_{k, i, j}(t) \cdot x_{k, j}(t) \right) = $ k'th layer i'th output.<br>
$\Xi_k(t) = W_k(t) X_k(t) =$ k'th layer accumulation in vector form.<br>
$y_{k, i}(t) = \sigma_k ( \xi_{k, i}(t) ) = \sigma_k ( \Sigma_j w_{k, i, j}(t) \cdot x_{k, j}(t) ) = $ k'th layer output.<br>
$Y_k(t) = \sigma( \Xi_k(t) ) =$ output in vector form.<br>
$x_j(t) = x_{1, j}(t) =$ first layer j'th input = network j'th input.<br>
$x_{k, j}(t) = y_{k-1, i}(t) =$ k'th layer j'th input = (k-1)'th layer j'th output.<br>
$y_j(t) = y_{l, j}(t) = $ network output.<br>
<br>

Network training variables:<br>
$d_j(t) = $ desired output (for trained data) for iteration "t".<br>
$D(t) = $ desired in vector form.<br>
$e_{l,j}(t) = d_j(t) - y_j(t) =$ residual of the output (with gradient towards the desired)<br>
$E_l(t) =$ error of the last layer, in vector form.<br>
$U(t) = \frac{1}{2} \Sigma_j (e_{l,j}(t))^2 = \frac{1}{2} || E_l(t) ||$ network energy function to minimize.<br>
The one-half out front is to cancel the derivative of the square term, so the gradient becomes proportional to the residual vector.<br>
<br>

There's the desired, and residual functions.<br>
I might drop the "t" parameters and declare the dependency to be implicit, just like in physics.<br>
<br>

Back-propagation update:<br>
Update weights to minimize error:<br>
$\frac{\partial w_{k,i,j}}{\partial t} = -\frac{\partial U}{\partial w_{k,i,j}}$<br>
Gradient of the final layer weights:<br>
$\frac{\partial U}{\partial w_{l,i,j}} 
	= \frac{\partial U}{\partial e_{l,a}} 
		\frac{\partial e_{l,a}}{\partial y_{l,b}} 
		\frac{\partial y_{l,b}}{\partial \xi_{l,c}} 
		\frac{\partial \xi_{l,c}}{\partial w_{l,i,j}}
$<br>
$\frac{\partial U}{\partial e_{l,a}} 
	= \frac{\partial}{\partial e_{l,a}} \left( \frac{1}{2} \Sigma_b (e_{l,b})^2 \right)
	= \Sigma_b \left( e_{l,b} \cdot \frac{\partial e_{l,b}}{\partial e_{l,a}} \right)
	= \Sigma_b \left( e_{l,b} \cdot \delta_{ab} \right)
	= e_{l,a}
$<br>
$\frac{\partial e_{l,a}}{\partial y_{l,b}}
	= \frac{\partial}{\partial y_{l,b}} \left( d_a - y_{l,a} \right)
	= -\delta_{ab}
$<br>
$\frac{\partial U}{\partial y_{l,b}} = -e_{l,b}$<br>
Gradient calculations of variables that are valid for each layer:<br>
$\frac{\partial y_{k,a}}{\partial \xi_{k,b}} 
	= \frac{\partial}{\partial \xi_{k,b}} \left( \sigma_k ( \xi_{k,a} ) \right)
	= \sigma_k' ( \xi_{k,a} ) \frac{\partial \xi_{k,a}}{\partial \xi_{k,b}} 
	= \sigma_k' ( \xi_{k,a} ) \cdot \delta_{ab}
$<br>
$\frac{\partial \xi_{k,b}}{\partial w_{k,i,j}}
	= \frac{\partial}{\partial w_{k,i,j}} \left(w_{k, b, c} \cdot x_{k, c} \right)
	= \frac{\partial}{\partial w_{k,i,j}} \left(w_{k, b, c}\right) \cdot x_{k, c} 
	= \delta_{ib} \delta_{jc} x_{k, c} 
	= \delta_{ib} x_{k, j} 
$<br>
$\frac{\partial y_{k,a}}{\partial x_{k,b}}
	= \frac{\partial y_{k,a}}{\partial \xi_{k,c}}
		\cdot \frac{\partial \xi_{k,c}}{\partial x_{k,b}}
	= \sigma'_k(\xi_{k,a}) \delta_{ac}
		\cdot \frac{\partial}{\partial x_{k,b}} \left( w_{k,c,d} x_{k,d} \right)
	= \sigma'_k(\xi_{k,a}) \delta_{ac} w_{k,c,d}
		\cdot \frac{\partial}{\partial x_{k,b}} \left( x_{k,d} \right)
	= \sigma'_k(\xi_{k,a}) \delta_{ac} w_{k,c,d} \delta_{bd}
	= \sigma'_k(\xi_{k,a}) w_{k,a,b}
$<br>
$\frac{\partial U}{\partial x_{k,a}} = \frac{\partial U}{\partial y_{k,b}} \frac{\partial y_{k,b}}{\partial x_{k,a}} = \frac{\partial U}{\partial y_{k,b}} \sigma'_k(\xi_{k,b}) w_{k,b,a}$<br>
Substitute our computed terms:<br>
$\frac{\partial U}{\partial w_{l,i,j}} 
	= e_{l,a}
		\cdot -\delta_{ab}
		\cdot \sigma_l' (\xi_{l,b} ) \delta_{bc}
		\cdot \delta_{ci} x_{l, j}
	= -e_{l,i} \sigma_l' ( \xi_{l,i} ) x_{l, j}
$<br>
$\frac{\partial w_{k,i,j}}{\partial t} = e_{l,i} \sigma_l' ( \xi_{l,i} ) x_{l, j}$<br>
Gradient of previous layers weights:<br>
$\frac{\partial U}{\partial w_{k,i,j}} 
	= \frac{\partial U}{\partial y_{k,a}} 
		\frac{\partial y_{k,a}}{\partial \xi_{k,b}} 
		\frac{\partial \xi_{k,b}}{\partial w_{k,i,j}}
	= \frac{\partial U}{\partial y_{k,a}} \cdot \sigma'_k(\xi_a) \delta_{ab} \cdot \delta_{bi} x_{k,j}
$<br>


<br>

A neural-network weight matrix is just a linear transform.  The output test is typically looking at whether the output is greater than some value.  
So assuming the activation is near-linear (which it tends to be for some domain),
we are really looking at a collection of hyperplanes that are forming a convex hull in our output space around whatever we want to classify.<br>
<br>

<h2>Example: Logical-NOT</h2>

Let's look at a single-neuron with-bias trained to the logical-NOT problem.<br>
We are going to classify <i>true</i> = 1 and <i>false</i> = -1.<br>
This means our desired problem to reproduce is $y(x) = -x$:<br>
<br>

<img src="ffnn_logical_not.svg" width="320"><br>
<br>

<br>
Our output classification will be $y \gt 0$ for <i>true</i>, otherwise <i>false</i>.<br>
Topology: <br>
$l = 1, \{(m,n)\} = \{(1,1)\}$<br>
$\sigma_1(x) = x = $ identity for activation function.<br>
$y = \sigma(w x)$<br>
<br>

State: <br>
$w = w_{1, 1, 1} = $ weight that corresponds with our single input.<br>
No bias is needed and no bias will be used.<br>
Live variables:<br>
$x = x_1 = x_{1, 1} = $ our network input, which will be +1 for <i>true</i> and -1 for <i>false</i>.
Notice that if you were to plot our x vs y, we are trying to reproduce the logical-NOT function $y(x) = -x$.<br>
This means we won't need any extra linear terms.  We don't need a bias term for our weight matrix. 
If I picked <i>false</i> to be 0 then our output function of the input would be $y(x) = 1 - x$, which requires a bias value, and I would need to simulate a bias weight.<br>
$\xi_{1, 1} = w x = $ the linear transform of our inputs, aka "net-accumulated sum of the neuron".<br>
$y = y_1 = y_{1,1} = \sigma_1 ( \xi_{1,1} ) = \xi_{1,1} = w x = $ the network output.<br>
Training variables:<br>
$d = d_1 = $ our desired value, which will be $-x$.  So if our input is 1 for <i>true</i> then our desired output will be -1 for <i>false</i>, and vice-versa.<br>
$e = d - y = (-x) - w x = (-1 - w) x$<br>
$U = \frac{1}{2} e^2 = \frac{1}{2} ((-1 - w) x )^2$<br>
Weight updates:<br>
$\frac{\partial w}{\partial t} = -\frac{\partial U}{\partial w} = -(1 + w) x^2 $<br>
<br>

Fixed points are where $\frac{\partial w}{\partial t} = 0$.  This is true for...<br>
$0 = (1 + w) x^2 $<br>
...which is true for $x = 0$, or for $w = -1$<br>
So those are our dynamic system 2 fixed points.<br>
We don't have to worry about the $x = 0$, since our inputs are either +1 or -1.<br>
So we have fixed points at $w = -1$<br>
Of course $w = -1$ is our desired answer, since we are trying to reproduce the behavior of the equation $y(x) = -x$, i.e. $y(x) = w x$ for $w = -1$.<br>
Let's assume an Euler integration:<br>
$w(t + \Delta t) = w(t) + \Delta t \cdot \frac{\partial w(t)}{\partial t}$<br>
$w(t + \Delta t) = w(t) - \Delta t \cdot \frac{\partial U(t)}{\partial w(t)}$<br>
$w(t + \Delta t) = w(t) - \Delta t \cdot (1 + w(t)) x(t)^2 = -\Delta t \cdot x(t)^2 + (1 - \Delta t \cdot x(t)^2) w(t)$<br>
<br>

Dynamics classification says, for some sequence $x_n$:<br>
$|\frac{\partial x_{n+1}}{\partial x_n}| \gt 1$ is a source, i.e. the system will diverge from this fixed point.<br>
$|\frac{\partial x_{n+1}}{\partial x_n}| = 1$ is indeterminant.  Or does this mean it will neither converge nor diverge? I forget...<br> 
$|\frac{\partial x_{n+1}}{\partial x_n}| \lt 1$ is a sink, i.e. the system will converge to this fixed point.<br>
$|\frac{\partial x_{n+1}}{\partial x_n}| = 0$ is a super-critical sink, i.e. the system is stationary at this point.<br>
<br>

$|\frac{\partial w(t + \Delta t)}{\partial w(t)}| = |1 - \Delta t \cdot x(t)^2|$<br>
<br>

In either case our fixed point convergence is independent of our dynamic variable, the weight, and only dependent on the input magnitude.<br>
If we provide $x(t) = 0$ as an input then our $|\frac{\partial w(t + \Delta t)}{\partial w(t)}| = 1$, which means we have indeterminant behavior neither converging nor diverging.<br>
Now the fixed points are specific to the update, which is specific to our desired training value, which is defined as $\pm x$ for inputs $x \in \{ -1, 1 \}$, so good thing $x = 0$ isn't a valid <i>true</i> or <i>false</i> input.<br>
<br>

For what values is the system converging?<br>
$|\frac{\partial w(t + \Delta t)}{\partial w(t)}| = |1 - \Delta t \cdot x(t)^2| \lt 1$<br>
$-1 \lt \Delta t \cdot x(t)^2 - 1 \lt 1$<br>
$0 \lt \Delta t \cdot x(t)^2 \lt 2$<br>
... assuming $\Delta t \gt 0$:<br>
$0 \lt x(t)^2 \lt \frac{2}{\Delta t}$<br>
This is true for $x \in (-\sqrt{\frac{2}{\Delta t}}, 0) \bigcup (0, \sqrt{\frac{2}{\Delta t}})$<br>
If we use $x \in (-\sqrt{\frac{2}{\Delta t}}, 0) \bigcup (0, \sqrt{\frac{2}{\Delta t}})$ then we will converge.<br>
For $x \gt 0$ values we will diverge.<br>
Since our inputs are $x = \pm 1$, this tells us that we can only use a timestep:<br>
$1 \lt \frac{2}{\Delta t}$<br>
$\Delta t \lt 2$<br>
<br>

<h4>Logical-NOT using {0,1} and missing a bias weight</h4>

Now let's repeat the process with T/F set to {0,1}:<br>
But let's not introduce a bias yet.<br>
Our desired output is going to be $d(x) = 1 - x$ for $x \in \{0,1\}$:<br>
<br>

<img src="ffnn_logical_not_01.svg" width="320"><br>
<br>

Our output classification will be $y \ge \frac{1}{2}$ is true, $y \lt \frac{1}{2}$ is false.<br>
$y = w x$<br>
$e = d - y = 1 - x - w x$<br>
$U = \frac{1}{2} e^2$<br>
$\frac{\partial w}{\partial t} = -\frac{\partial U}{\partial w} = -e \frac{\partial e}{\partial w}$
$ = -e \cdot -x = e x = (d - y) x = (1 - x - w x) x$<br>
We now have fixed points at $x = 0$ and $1 - (1 + w) x = 0$, i.e. $x = \frac{1}{1 + w}$ or $w = \frac{1}{x} - 1$<br>
Let's look at the update to classify those inputs:<br>
$w(t+\Delta t) = w(t) + \Delta t (1 - x - w x) x$<br>
$|\frac{\partial w(t+\Delta t)}{\partial w(t)}| = |1 - x \Delta t|$<br>
This is going to be in $(-1,1)$ for $|x \Delta t| \in (0,2)$<br>
But notice that one of our inputs is $x=0$.  Notice for this input, our fixed-point classification is now $|\frac{\partial w(t+\Delta t)}{\partial w(t)}|= 1$, and our system can no longer guarantee convergence.<br>
Let's assume we are going to skip the $x=0$ inputs and just train this network with $x=1$ and $d(x) = 0$.<br>
Then we still have a fixed point at $w = \frac{1}{x} - 1 = 0$.  The convergence of this is based on $x \Delta t$, so as long as $x=1$ and we keep $\Delta t \in (0,2)$ then we will converge.<br>
Our network converges to the equation $y = 0 \cdot x = 0$, which is an output of <i>false</i> for an input of <i>true</i>, so we did manage to classify one of our two inputs.  
However then network will never be able to classify its <i>false</i> inputs correctly.<br>
<br>

Now what if we add a bias weight to the $\{0,1\}$ classification network?<br>
TODO.<br>
$y = \sigma(w x + b)$<br>
<br>

TODO repeat the process with non-identity $\sigma$, like tanh.<br>
Then using tanh activation with T/F set to $\{-1,1\}$ vs $\{\frac{-9}{10}, \frac{9}{10}\}$.
Spoilers: $tanh(x) = \frac{9}{10}$ exists, so if we set our desired to $\pm \frac{9}{10}$ then our weights have a specific point they will converge to.
In contrast, there exists no x such that $tanh(x) = 1$, except for $\underset{x \rightarrow \infty}{\lim} tanh(x) = 1$, so if we set our desired to $\pm 1$ then our weight values will diverge forever while settling on the desired output.<br>
<br>

Then using a sigmoid activation with T/F set to $\{0,1\}$ vs $\{\frac{1}{10}, \frac{9}{10}\}$ ... and with bias weight (two weight system now) since the classification needs to now fit to $y(x) = 1 - x$ instead of $y(x) = x$<br>
<br>

Using sigmoid activation means $\sigma(0) = \frac{1}{2}$ and $\sigma'(0) = \frac{1}{4}$, 
which means we cannot analyze the system using linear approximation ... unless we are analyzing $4 \sigma(x - \frac{1}{2}) \approx x$.<br>
In contrast, using tanh activation means $\tanh(0) = 0$, $tanh'(0) = cosh^{-2}(0) = 1$, and $tanh(x) \approx x$.<br>
<br>

<h2>Example: Logical-AND</h2>

We are going to classify <i>true</i> = 1 and <i>false</i> = -1, $y \gt 0$ for true, $y \le 0$ for false.<br>
Therefore our desired problem to reproduce is something along the lines of:<br>
$y(x_1, x_2) = \frac{1}{2} x_1 + \frac{1}{2} x_2 - \frac{1}{2}$, such that $y(-1,-1) = -\frac{3}{2} = F, y(-1,1) = -\frac{1}{2} = F, y(1,-1) = -\frac{1}{2} = F, y(1,1) = \frac{1}{2} = T$ .<br>
<br>

<img src="ffnn_logical_and.svg" width="320"><br>
<br>

TODO first try without the bias weight and see what happens / show that it can't fully classify all points ever, only 2 of the 3 <i>false</i>'s at once at most.<br>
<br>

Topology:<br>
$l = 1, \{(m,n)\} = \{(1,2)\}$<br>
$\sigma(x) = \sigma_1(x) = x =$ identity activation.<br>
<br>

State:<br>
$w_1 = w_{1,1,1}, w_2 = w_{1,1,2}, w_3 = w_{1,1,3}$ are our weights, (with $w_3$ being the bias input weight).<br>
Live variables:<br>
$x_1 = x_{1,1}; x_2 = x_{1,2}$ are our inputs.<br>
$\xi_1 = w_1 x_1 + w_2 x_2 + w_3$<br>
$y = \sigma(\xi_1) = w_1 x_1 + w_2 x_2 + w_3$<br>
$y = b + W X$ for $W \in \mathbb{R}^{1 \times 2}$, $X \in \mathbb{R}^2$<br>
<br>

Training variables:<br>
We can do more exact with our description of our desired value as a function of our inputs, by multiplying our inputs, 
however neural networks themselves are planes are linear transformations and cannot directly reproduce multiplication.<br>
$d(x_1, x_2) = 2 \left( \frac{1}{2}(x_1 + 1) \cdot \frac{1}{2} (x_2 + 1) \right) - 1 = \frac{1}{2} (x_1 x_2 + x_1 + x_2 - 1)$.<br>
This is chosen such that $d(-1,-1) = d(-1,1) = d(1,-1) = -1 =$ <i>false</i>, and $d(1,1) =$ <i>true</i>.<br>
<br>

$e = d - y = \frac{1}{2} (x_1 x_2 + x_1 + x_2 - 1) - \left( w_1 x_1 + w_2 x_2 + w_3 \right)$<br>
$e = \frac{1}{2} x_1 x_2 + \frac{1}{2} x_1 + \frac{1}{2} x_2 - \frac{1}{2} - w_1 x_1 - w_2 x_2 - w_3$<br>
$U = \frac{1}{2} e^2 
	= \frac{1}{2} \left(
		\frac{1}{2} x_1 x_2 + \frac{1}{2} x_1 + \frac{1}{2} x_2 - \frac{1}{2} - w_1 x_1 - w_2 x_2 - w_3
	\right)^2$<br>
$\frac{\partial U}{\partial w_i} = e \cdot \frac{\partial e}{\partial w_i}
	= e \cdot \frac{\partial e}{\partial w_i}
	= e \cdot -\frac{\partial y}{\partial w_i}
	= -e x_i$<br>
$\frac{\partial w_i(t)}{\partial t} = -\frac{\partial U}{\partial w_i} = e x_i$<br>
$w_i(t + \Delta t) = w_i(t) + \Delta t (d - y) x_i$<br>
$w_i(t + \Delta t) = w_i(t) + \Delta t (\frac{1}{2} (x_1 x_2 + x_1 + x_2 - 1) - (w_1 x_1 + w_2 x_2 + w_3)) x_i$<br>
<br>

Fixed points:<br>
$\frac{\partial w_i(t)}{\partial t} = 0$ for $x_i e = 0$.
One solution is $x_i = 0$.
Another is $e = 0$, i.e. for $w_1 x_1 + w_2 x_2 + w_3 - \frac{1}{2} (x_1 x_2 + x_1 + x_2 - 1) = 0$<br>
This looks like a plane equation in $\{w_1, w_2, w_3\}$ space.<br>
For our inputs this can be the possible values:<br>
$x_1 = -1, x_2 = -1: -w_1 - w_2 + w_3 + 1 = 0$<br>
$x_1 =  1, x_2 = -1:  w_1 - w_2 + w_3 + 1 = 0$<br>
$x_1 = -1, x_2 =  1: -w_1 + w_2 + w_3 + 1 = 0$<br>
$x_1 =  1, x_2 =  1:  w_1 + w_2 + w_3 - 1 = 0$<br>
So for each possible set of inputs, our neural network state critical point will be a different plane.<br> 
<br>

Dynamics:<br>
$|\frac{\partial w_i(t + \Delta t)}{\partial w_i(t)}| = |1 - \Delta t (x_i)^2|$ for $x_3 = 1$<br>
$|\frac{\partial w_1(t + \Delta t)}{\partial w_1(t)}| = |1 - \Delta t (x_1)^2|$<br>
$|\frac{\partial w_2(t + \Delta t)}{\partial w_2(t)}| = |1 - \Delta t (x_2)^2|$<br>
$|\frac{\partial w_3(t + \Delta t)}{\partial w_3(t)}| = |1 - \Delta t|$<br>
So as long as our inputs are $x_i = \pm 1$, and our timestep $\Delta t = 1$, our fixed point is super-critical.<br>
<br>

So we are going to be always instantly converging on a solution per-input/desired pair, but that solution will vary over all possible pairs.<br>
<br>

What if we look at the critical point of the system after iterating across all possible permutations of inputs/desired? I.e. equivalent to a "batch update" of all 4 possible inputs.<br>
$w_i(t + \Delta t) = w_i(t) + \Delta t \frac{1}{4} ($<br>$
\ 	  (\frac{1}{2} (x_1 x_2 + x_1 + x_2 - 1) - (w_1 x_1 + w_2 x_2 + w_3)) x_i | (x_1 = -1, x_2 = -1)$<br>$
	+ (\frac{1}{2} (x_1 x_2 + x_1 + x_2 - 1) - (w_1 x_1 + w_2 x_2 + w_3)) x_i | (x_1 = 1, x_2 = -1)$<br>$
	+ (\frac{1}{2} (x_1 x_2 + x_1 + x_2 - 1) - (w_1 x_1 + w_2 x_2 + w_3)) x_i | (x_1 = -1, x_2 = 1)$<br>$
	+ (\frac{1}{2} (x_1 x_2 + x_1 + x_2 - 1) - (w_1 x_1 + w_2 x_2 + w_3)) x_i | (x_1 = 1, x_2 = 1)$<br>$
)$<br>
<br>
Individually:<br>
<br>
$\frac{\partial w_1(t)}{\partial t} = \frac{1}{4} \left(
	- (-1 + w_1 + w_2 - w_3)
	+ (-1 - w_1 + w_2 - w_3)
	- (-1 + w_1 - w_2 - w_3)
	+ ( 1 - w_1 - w_2 - w_3)
\right) = -w_1(t) + \frac{1}{2}$<br>
$w_1(t + \Delta t) = w_1(t) \cdot (1 - \Delta t) + \frac{1}{2} \Delta t$<br>

<br>
$\frac{\partial w_2(t)}{\partial t} = \frac{1}{4} \left(
	- (-1 + w_1 + w_2 - w_3)
	- (-1 - w_1 + w_2 - w_3)
	+ (-1 + w_1 - w_2 - w_3)
	+ ( 1 - w_1 - w_2 - w_3)
\right) = -w_2(t) + \frac{1}{2}$<br>
$w_2(t + \Delta t) = w_2(t) \cdot (1 - \Delta t) + \frac{1}{2} \Delta t$<br>
<br>

$\frac{\partial w_3(t)}{\partial t} = \frac{1}{4} \left(
	  (-1 + w_1 + w_2 - w_3)
	+ (-1 - w_1 + w_2 - w_3)
	+ (-1 + w_1 - w_2 - w_3)
	+ ( 1 - w_1 - w_2 - w_3)
\right) = -w_3(t) - \frac{1}{2}$<br>
$w_3(t + \Delta t) = w_3(t) \cdot (1 - \Delta t) - \frac{1}{2} \Delta t$<br>
<br>

Our fixed point of $\frac{\partial w_i(t)}{\partial t} = 0$ shows up for $w_1 = \frac{1}{2}, w_2 = \frac{1}{2}, w_3 = -\frac{1}{2}$.<br>
With these weights, our neural network reproduces the eqution:<br>
$y(x_1, x_2) = w_1 x_1 + w_2 x_2 + w_3 = \frac{1}{2} x_1 + \frac{1}{2} x_2 - \frac{1}{2}$<br>
... just like we were looking for.<br>
<br>

In each case:<br>
$|\frac{\partial w(t+\Delta t)}{\partial w(t)}| = |1 - \Delta t|$<br>
... which will converge for $\Delta t \in (0,2)$, and is super-critical for $\Delta t = 1$.<br>
<br>

However notice that these weights are a fixed point only when we average all four of our input/desired updates together.
Once our network is in the optimal state of $(w_1, w_2, w_3) = (\frac{1}{2}, \frac{1}{2}, -\frac{1}{2})$, if we then go on to perform another update with a single input/desired pair, we will find that $\frac{\partial w(t+\Delta t)}{\partial t} \ne 0$ and we are not at a fixed point.
This mean our network will continue to oscillate forever near the optimal solution but will never settle completely.<br>
<br>

TODO next do classification of OR problem with 1-layer.<br>
Spoilers, it is the same solution except with a different $w_3$ bias term (probably $\frac{1}{2}$) such that (1,1), (1,-1), and (-1,1) produce a result &gt; 0 and (-1,-1) produces a result &lt; 0.<br>
You can repeat the previous problem except with a new desired function
$d(x_1, x_2) = 1 - 2 \left( \frac{1}{2}(1 - x_1) \cdot \frac{1}{2} (1 - x_2) \right) = \frac{1}{2} (-x_1 x_2 + x_1 + x_2 + 1)$
such that $d(-1,-1) = -1 =$ <i>false</i> and $d(-1,1) = d(1,-1) = d(1,1) = 1 =$ <i>true</i>
to verify this.<br>
<br>

<img src="ffnn_logical_or.svg" width="320"><br>
<br>

TODO show how AND, OR, NAND, NOR, and any other classification that includes 3 and excludes 1 or vice versa is all just the same problem, with different signs on the weights.<br>
TODO show how for 3 inputs, any combo of AND's, OR's, and NOT's that produces a single true and 7 falses (or vice versa), is just a single-layer 3x1 weight network, thanks to a single plane dividing all trues vs falses.<br>
TODO show for n inputs, and only a single ouptut that is T and all $2^n-1$ others F (or inverse), we just need a single-layer nx1 weight network.<br>
<br>

<h3>Logical XOR</h3>

Unlike Logical-AND or Logical-OR, Logical-XOR cannot divide its space of <i>true vs </i>false.  
Assuming our previous convention of <i>false</i> = -1 and <i>true</i> = 1,
we now have to classify a problem $d(-1, -1) = d(1,1) = -1$ and $d(1,-1) = d(-1,1) = 1$.
One simple mathematical solution to this is $d(x_1, x_2) = -x_1 x_2$, so we can use this as our desired/training function, but once again, neural networks cannot reproduce input multiplication, only linear functions of the inputs.<br>
<br>

Let's try with just 1 layer.<br>
$l = 1, \{(m,n)\} = \{(1,2)\}$<br>
<br>

Let's also keep using identity activation:<br>
$\sigma(x) = x$<br>
...however we're going to find out soon that, if we use a 2-layer network with identity activation, it degenerates into a 1-layer network, and back to our original statement that XOR cannot be reproduced with a 1-layer network, this also means XOR cannot be reproduced with a 2-layer identity-activation network:<br>
$Y = \sigma_2 ( W_2 \sigma_1 (W_1 X + B_1) + B_2 )$<br>
... let $\sigma_i = $ identity...<br>
$Y = ( W_2 (W_1 X + B_1) + B_2 )$<br>
$Y = W_2 W_1 X + W_2 B_1 + B_2$<br>
... let $W' = W_2 W_1$ and $B' = W_2 B_1 + B_2$:<br>
$Y = W' X + B'$<br>
So here we see a two-layer network with no activation is really just a 1-layer network.<br>
But TODO keep running with the identity-activation and see if the XOR network does converge or not.<br>
<br>

We are going to be reproducing the logic of $A \oplus B = (\lnot A \land B) \lor (A \land \lnot B)$,
so we're going to be reproducing 3 different AND/OR problems.  Each takes 3 weights, so we are going to need 9 weights total.<br>
<br>

So we will need a nonlinear activation on the first later at least:<br>
$\sigma_1(x) = tanh(x)$<br>
<img src="ffnn_tanh.svg" width="320"><br>
<br>
...with derivative...<br>
$\sigma'_1(x) = (cosh(x))^{-2}$<br>
<img src="ffnn_tanhderiv.svg" width="320"><br>
...but we will keep the 2nd layer identity.<br>
$\sigma_2(x) = x$<br>
<br>

Topology:<br>
$l = 2, \{(m,n)\} = \{(2,2), (1,2)\}$<br>
<br>

$X = X_1$, i.e. the first-layer input is the network input.<br>
$Y_1 = \sigma_1(B_1 + W_1 X_1)$, describes the 1st layer propagation, i.e.:<br>
$y_{1,1} = \sigma_1(b_{1,1} + w_{1,1,1} x_{1,1} + w_{1,1,2} x_{1,2})$<br>
$y_{1,2} = \sigma_1(b_{1,2} + w_{1,2,1} x_{1,1} + w_{1,2,2} x_{1,2})$<br>
$X_2 = Y_1$, i.e. the second-layer input is the first-layer output.<br>
$Y = Y_2 = \sigma_2(B_2 + W_2 X_2) = \sigma_2(B_2 + W_2 \sigma_1(B_1 + W_1 X_1))$, is the 2nd layer propagation, i.e.:<br>
$y_{2,1} = \sigma_2(b_{2,1} + w_{2,1,1} x_{2,1} + w_{2,1,2} x_{2,2})$<br>
$y_{2,1} = b_{2,1} + w_{2,1,1} x_{2,1} + w_{2,1,2} x_{2,2}$<br>
$y_{2,1} = b_{2,1} + w_{2,1,1} y_{1,1} + w_{2,1,2} y_{1,2}$<br>
$y_{2,1} = b_{2,1} + w_{2,1,1} \sigma_1(b_{1,1} + w_{1,1,1} x_{1,1} + w_{1,1,2} x_{1,2}) + w_{2,1,2} \sigma_1(b_{1,2} + w_{1,2,1} x_{1,1} + w_{1,2,2} x_{1,2})$<br>
<br>

Now comes the updates...<br>
$\frac{\partial b_{2,1}}{\partial t} = -x_{1,1} x_{1,2} - y_{2,1}$<br>
$\frac{\partial w_{2,1,1}}{\partial t} = (-x_{1,1} x_{1,2} - y_{2,1}) y_{1,1}$<br>
$\frac{\partial w_{2,1,2}}{\partial t} = (-x_{1,1} x_{1,2} - y_{2,1}) y_{1,2}$<br>
$\frac{\partial b_{1,1}}{\partial t} = (-x_{1,1} x_{1,2} - y_{2,1}) w_{2,1,1} \sigma'_1(\xi_{1,1})$<br>
$\frac{\partial w_{1,1,1}}{\partial t} = (-x_{1,1} x_{1,2} - y_{2,1}) w_{2,1,1} \sigma'_1(\xi_{1,1}) x_{1,1}$<br>
$\frac{\partial w_{1,1,2}}{\partial t} = (-x_{1,1} x_{1,2} - y_{2,1}) w_{2,1,1} \sigma'_1(\xi_{1,1}) x_{1,2}$<br>
$\frac{\partial b_{1,2}}{\partial t} = (-x_{1,1} x_{1,2} - y_{2,1}) w_{2,1,2} \sigma'_1(\xi_{1,2})$<br>
$\frac{\partial w_{1,2,1}}{\partial t} = (-x_{1,1} x_{1,2} - y_{2,1}) w_{2,1,2} \sigma'_1(\xi_{1,2}) x_{1,1}$<br>
$\frac{\partial w_{1,2,2}}{\partial t} = (-x_{1,1} x_{1,2} - y_{2,1}) w_{2,1,2} \sigma'_1(\xi_{1,2}) x_{1,2}$<br>
<br>

The equation is at a critical point where This is zero, which is ...<br>
$\frac{\partial b_{2,1}}{\partial t} = 0$, for $y_{2,1} = -x_{1,1} x_{1,2}$<br>
$\frac{\partial w_{2,1,1}}{\partial t} = (-x_{1,1} x_{1,2} - y_{2,1}) y_{1,1} = 0$, for $y_{2,1} = -x_{1,1} x_{1,2}$ or $y_{1,1} = 0$<br>
$\frac{\partial w_{2,1,2}}{\partial t} = (-x_{1,1} x_{1,2} - y_{2,1}) y_{1,2} = 0$, for $y_{2,1} = -x_{1,1} x_{1,2}$ or $y_{1,2} = 0$<br>
$\frac{\partial b_{1,1}}{\partial t} = (-x_{1,1} x_{1,2} - y_{2,1}) w_{2,1,1} \sigma'_1(\xi_{1,1})$<br>
$\frac{\partial w_{1,1,1}}{\partial t} = (-x_{1,1} x_{1,2} - y_{2,1}) w_{2,1,1} \sigma'_1(\xi_{1,1}) x_{1,1}$<br>
$\frac{\partial w_{1,1,2}}{\partial t} = (-x_{1,1} x_{1,2} - y_{2,1}) w_{2,1,1} \sigma'_1(\xi_{1,1}) x_{1,2}$<br>
$\frac{\partial b_{1,2}}{\partial t} = (-x_{1,1} x_{1,2} - y_{2,1}) w_{2,1,2} \sigma'_1(\xi_{1,2})$<br>
$\frac{\partial w_{1,2,1}}{\partial t} = (-x_{1,1} x_{1,2} - y_{2,1}) w_{2,1,2} \sigma'_1(\xi_{1,2}) x_{1,1}$<br>
$\frac{\partial w_{1,2,2}}{\partial t} = (-x_{1,1} x_{1,2} - y_{2,1}) w_{2,1,2} \sigma'_1(\xi_{1,2}) x_{1,2}$<br>
	</body>
</html>
